title,content,images
Reinforcement Learning Heats Up,"Reinforcement learning is emerging as an avenue for building large language models with advanced reasoning capabilities.
What’s new:Two recent high-performance models,DeepSeek-R1(and its variants including DeepSeek-R1-Zero) andKimi k1.5, learned to improve their generated lines of reasoning via reinforcement learning.o1pioneered this approach last year.
Reinforcement learning (RL) basics:RL rewards or punishes a model for performing particular actions or achieving certain objectives. Unlike supervised and unsupervised learning, which compare the model's output to a known ground truth, RL doesn’t explicitly tell a model what it should output. Instead, the model starts out behaving randomly and discovers desired behaviors by earning rewards for its actions. This makes RL especially popular for training machine learning models that play games or control robots.
How it works:To improve thechain of thought(CoT) generated by a large language model (LLM), reinforcement learning encourages the model to generate correct solutions to math, coding, science, and other problems that have known solutions. Unlike typical LLM training, in which the model simply generates the next token of its output and receives feedback token by token, this method rewards the model for generating a sequence of reasoning steps that lead to an accurate conclusion, even if doing so requires generating many intermediate tokens between the prompt and the response — to plan an outline, check the conclusion, or reflect on the approach — without explicit training on the reasoning steps to take.
Behind the news:While RL has been a staple technique for training models toplay gamesandcontrol robots, its role in developing LLMs has been confined to alignment with human preferences. Reinforcement learning to match judgements of humans (reinforcement learning from human feedback, or RLHF) or AI (Constitutional AI, which uses reinforcement learning from AI feedback or RLAIF) were the primary methods for encouraging LLMs to align with human preferences prior to the development ofdirect preference optimization.
Why it matters:Reinforcement learning has surprising utility in training large language models to reason. As researchers press models into service in more complex tasks — math, coding, animated graphics, and beyond — reinforcement learning is emerging as an important path to progress.
We’re thinking:Less than three years ago, reinforcement learning looked toofinickyto be worth the trouble. Now it’s a key direction in language modeling. Machine learning continues to be full of surprising twists!
",['https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--46-.gif']
Computer Use Gains Momentum,"OpenAI introduced an AI agent that performs simple web tasks on a user’s behalf.
What’s new:Operatorautomates online actions like buying goods, booking tickets and completing forms by navigating websites in a browser-like environment within ChatGPT. It’s available on desktops as a research preview for subscribers to ChatGPT Pro ($200 per month). OpenAI promises broader availability to come as well as API access to the underlying model and improved ability to coordinate multi-step tasks like scheduling meetings across calendars from different vendors.
How it works:Operator uses a new model calledComputer-Using Agent(CUA) that accepts text input and responds with web actions.
Behind the news:Operator rides a wave of agents designed to automate everyday tasks. Last week, OpenAI introducedChatGPT Tasks, which lets users schedule reminders and alerts but doesn’t support web interaction. (Early userscomplainedthat Tasks was buggy and required overly precise instructions.) Anthropic’sComputer Usefocuses on basic desktop automation, while DeepMind’sProject Marineris a web-browsing assistant built on Gemini 2.0.Perplexity Assistantautomates mobile apps such as booking Uber rides on Android phones.
Why it matters:In early reports, userssaidOperator sometimes was less efficient than a human performing the same tasks. Nevertheless, agentic AI is entering the consumer market, and Operator is poised to give many people their first taste. It’s geared to provide AI assistance for an endless variety of personal and business uses, and — like ChatGPT was for other developers of LLMs — and it’s bound to serve as a template for next-generation products.
We’re thinking:Computer use is maturing, and the momentum behind it is palpable. AI developers shouldhave in their toolbox.
",['https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--50-.png']
White House Orders Muscular AI Policy,"Under a new president, the United States reversed its approach to AI regulation, seeking global dominance by reducing restrictions.
What’s new:President Trump, who took office last week,signedan executive order that set a 180-day deadline to draft an AI Action Plan. The order aims to boost national security, economic competitiveness, and U.S. leadership in artificial intelligence.
How it works:Theexecutive orderassigns responsibility for crafting the AI Action Plan to three key figures in the administration: Michael Kratsios, assistant to the president for science and technology (and former managing director of Scale AI); venture capitalist David Sacks, the new special advisor for AI and cryptocurrency; and national security advisor Michael Waltz.
AI infrastructure build-out:Along with the executive order, President Trump announcedStargate, a joint venture that involves OpenAI, Oracle, and SoftBank. The three companies outlined a plan to invest $100 billion in computing infrastructure for AI, such as next-generation data centers, and $500 billion over four years. In addition, the administrationdeclareda national energy emergency with respect to U.S. supplies of energy andissuedan order to ramp up domestic energy production. These measures aim to support energy-intensive AI initiatives like Stargate by removing regulatory barriers to building oil, gas, and renewable energy projects on federal lands.
Why it matters:The Trump administrationsaysthat Biden’s 2023 regulations were “onerous and unnecessary,” stifled innovation, and jeopardized U.S. leadership in AI. The new order reduces bureaucratic oversight of AI development, creating a more permissive regulatory environment (except when it comes to ideological bias).
We’re thinking:The Biden administration’s 2023 executive order aimed to guard against hypothetical, rather than actual, AI risks. It introduced thresholds of processing used to train models as a measure of their risk — a poorly thought-out proxy. To be fair, the AI Safety Institute under the U.S. National Institute of Standards and Technology didn’t hamper AI progress as much as some had feared, but overall the order was not helpful to AI innovation or safety. We’re pleased that the new administration is focusing on AI progress rather than hypothetical risks.
",['https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--51-.png']
DeepSeek Sharpens Its Reasoning,"A new open model rivals OpenAI’s o1, and it’s free to use or modify.
What’s new:DeepSeek releasedDeepSeek-R1, a large language model that executes long lines of reasoning before producing output. The code and weights arelicensedfreely for commercial and personal use, including training new models on R1 outputs. Thepaperprovides an up-close look at the training of a high-performance model that implements a chain of thought without explicit prompting. (DeepSeek-R1-lite-previewcame out in November with fewer parameters and a different base model.)
Mixture of experts (MoE) basics:The MoE architecture uses different subsets of its parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a gating module that learns to choose which one(s) to use based on the input. In this way, different experts learn to specialize in different types of examples. Because not all parameters are used to produce any given output, the network uses less energy and runs faster than models of similar size that use all parameters to process every input.
How it works:DeepSeek-R1 is a version ofDeepSeek-V3-Basethat was fine-tuned over four stages to enhance its ability to process achain of thought(CoT). It’s a mixture-of-experts transformer with 671 billion total parameters, 37 billion of which are active at any given time, and it processes 128,000 tokens of input context. Access to the model via DeepSeek’sAPIcosts $0.55 per million input tokens ($0.14 for cached inputs) and $2.19 per million output tokens. (In comparison, o1 costs $15 per million input tokens, $7.50 for cached inputs, and $60 per million output tokens.)
Other models:DeepSeek researchers also released seven related models.
Results:In DeepSeek’s tests, DeepSeek-R1 went toe-to-toe with o1, outperforming that model on 5 of 11 of the benchmarks tested. Some of the other new models showed competitive performance, too.
Why it matters:Late last year, OpenAI’s o1 kicked off a trend toward so-called reasoning models that implement a CoT without explicit prompting. But o1 and o3, its not-yet-widely-available successor, hide their reasoning steps. In contrast, DeepSeek-R1 bares all, allowing users to see the steps the model took to arrive at a particular answer. DeepSeek’s own experiments with distillation show how powerful such models can be as teachers to train smaller student models. Moreover, they appear to pass along some of the benefits of their reasoning skills, making their students more accurate.
We’re thinking:DeepSeek is rapidly emerging as a strong builder of open models. Not only are these models great performers, but their license permits use of their outputs for distillation, potentially pushing forward the state of the art for language models (and multimodal models) of all sizes.
",['https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--45-.gif']
Humanoid Robot Price Break,"Chinese robot makers Unitree and EngineAI showed off relatively low-priced humanoid robots that could bring advanced robotics closer to everyday applications.
What’s new:At the annual Consumer Electronics Show (CES) in Las Vegas, Unitree showed itsG1($16,000 with three-finger hands, $21,000 with five-finger, articulated hands), which climbed stairs and navigated around obstacles. Elsewhere on the show floor, EngineAI’sPM01($13,700 through March 2025 including articulated hands) andSE01(price not yet disclosed) marched among attendees with notably naturalistic gaits.
How it works:Relatively small and lightweight, these units are designed for household and small-business uses. They’re designed for general-purpose tasks and to maintain stability and balance while walking on varied terrain.
Behind the news:In contrast to the more-affordable humanoid robots coming out of China, U.S. companies like Boston Dynamics, Figure AI, and Tesla tend to cater to industrial customers. Teslaplansto produce several thousand of its Optimus ($20,000 to $30,000) humanoids in 2025, ramping to as many as 100,000 in 2026. Figure AI has demonstrated its Figure 02 ($59,000) in BMW manufacturing plants,showinga 400 percent speed improvement in some tasks. At CES, Nvidia unveiled its GR00T Blueprint, which includes vision-language models and synthetic data for training humanoid robots, and said its Jetson Thor computer for humanoids would be available early 2025.
Why it matters:China’s push into humanoid robotics reflects its broader national ambitions. Its strength in hardware has allowed it to establish a dominant position in drones, and humanoid robots represent a new front for competition. China’s government aims toachievemass production of humanoid robots by 2025 and establish global leadership by 2027, partly to address projected labor shortages of 30 million workers in manufacturing alone. Lower price points for robots that can perform arbitrary tasks independently could be valuable in elder care and logistics, offering tools for repetitive or physically demanding tasks.
We’re thinking:Although humanoid robots generate a lot of excitement, they’re still in an early stage of development, and businesses are still working to identify and prove concrete use cases. For many industrial applications, wheeled robots — which are less expensive, more stable, and better able to carry heavy loads — will remain a sensible choice. But the prospect of machines that look like us and fit easily into environments built for us is compelling.
",['https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--48-.jpg']
Texas Moves to Regulate AI,"Lawmakers in the U.S. state of Texas are considering stringent AI regulation.
What’s new:The Texas legislature is considering the proposedTexas Responsible AI Governance Act (TRAIGA). The bill would prohibit a short list of harmful or invasive uses of AI, such as output intended to manipulate users. It would impose strict oversight on AI systems that contribute to decisions in key areas like health care.
How it works:Republican House Representative Giovanni Capriglione introduced TRAIGA, also known asHB 1709, to the state legislature at the end of 2024. If it’s passed and signed, the law would go into effect in September 2025.
Sandbox:A “sandbox” provision would allow registered AI developers to test and refine AI systems temporarily with fewer restrictions. Developers who registered AI projects with the Texas AI Council would gain temporary immunity, even if their systems did not fully comply with the law. However, this exemption would come with conditions: Developers must submit detailed reports on their projects’ purposes, risks, and mitigation plans. The sandbox status would be in effect for 36 months (with possible extensions), and organizations would have to bring their systems into compliance or decommission them once the period ends. The Texas AI Council could revoke sandbox protections if it determined that a project posed a risk of public harm or failed to meet reporting obligations.
Behind the news:Other U.S. states, too, are considering or have already passed laws that regulate AI:
Why it matters:AI is not specifically regulated at the national level in the United States. This leaves individual states free to formulate their own laws. However, state-by-state regulation risks a patchwork of laws in which a system — or a particular feature — may be legal in some states but not others. Moreover, given the distributed nature of AI development and deployment, a law that governs AI in an individual state could affect developers and users worldwide.
We’re thinking:The proposed bill has its positive aspects, particularly insofar as it seeks to restrict harmful applications rather than the underlying technology. However, it imposes burdensome requirements for compliance, suffers from overly broad language, fails to adequately protect open source, and doesn’t distinguish between research and commercial use. Beyond that, state-by-state regulation of AI is not workable. On the contrary, AI demands international conventions and standards.
",['https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--48-.png']
DeepSeek Ups the Open Weights Ante,"A new model from Hangzhou upstart DeepSeek delivers outstanding performance and may change the equation for training costs.
What’s new:DeepSeek-V3is an open large language model that outperforms Llama 3.1 405B and GPT-4o on key benchmarks and achieves exceptional scores in coding and math. The weights areopenexcept for applications that involve military uses, harming minors, generating false information, and similar restrictions. You can download themhere.
Mixture of experts (MoE) basics:The MoE architecture uses different subsets of its parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a gating module that learns to choose which one(s) to use based on the input. In this way, different experts learn to specialize in different types of examples. Because not all parameters are used to produce any given output, the network uses less energy and runs faster than models of similar size that use all parameters to process every input.
How it works:DeepSeek-V3 is a mixture-of-experts (MoE) transformer that comprises 671 billion parameters, of which 37 billion are active at any moment. The team trained the model in 2.79 million GPU hours — less than 1/10 thetime required to train Llama 3.1 405B, which DeepSeek-V3 substantially outperforms — at an extraordinarily low cost of $5.6 million.
Results:In DeepSeek’s tests, DeepSeek-V3 outperformed Llama 3.1 405B and Qwen 2.5 72B across the board, and its performance compared favorably with that of GPT-4o.
Behind the news:OpenAI’s o1 models excel thanks to agentic workflows in which they reflect on their own outputs, use tools, and so on. DeepSeek swims against the tide and achieves superior results without relying on agentic workflows.
Why it matters:Open models continue to challenge closed models, giving developers high-quality options that they can modify and deploy at will. But the larger story is DeepSeek-V3’s shockingly low training cost.The team doesn’t explain precisely how the model achieves outstanding performance with such a low processing budget. (The paper credits “meticulous engineering optimizations.”) But it’s likely that DeepSeek’s steady refinement of MoE is a key factor. DeepSeek-V2, also an MoE model, saved more than 40 percent in training versus the earlier DeepSeek 67B, which didn’t employ MoE. In 2022,Microsoftfound that MoE cost five times less in training for equal performance compared to a dense model, andGoogleandMetareported that MoE achieved better performance than dense models trained on the same numbers of tokens.
We’re thinking:If they can be replicated, DeepSeek’s results have significant implications for the economics of training foundation models. If indeed it now costs around $5 million to build a GPT-4o-level model, more teams will be able to train such models, and the cost of competing with the AI giants could fall dramatically.
",['https://dl-staging-website.ghost.io/content/images/2025/01/BIDENCHIPS-10_1200px.jpg']
U.S. Moves to Expand AI Export Restrictions,"The United States proposed limits on exports of AI technology that would dramatically expand previous restrictions, creating a new international hierarchy for access to advanced chips and models.
What’s new:The Biden administration, which will transition to leadership under incoming President Trump next week, issued newrulesthat restrict exports of AI chips and models to most countries beyond a select group of close allies. The rules, which are not yet final, would create a three-tier system that limits exports to a number of close allies and blocks access entirely to China, Iran, North Korea, Russia, and others. They also would introduce the U.S.’ first-ever restrictions on exporting closed weights for large AI models.
How it works:The restrictions were announced shortly after aleakreached the press. A public comment period of 120 days will enable the incoming U.S. Presidential administration to consider input from the business and diplomatic communities and modify the rules before they take effect. The rules are scheduled to take effect in one year.
Behind the news:The proposed rules build on 2022’sCHIPS and Science Act, which was designed to strengthen domestic semiconductor production and restrict technologies abroad that could bear on U.S. security. An initial round of restrictions in late 2022barredsemiconductor suppliers AMD and Nvidia from selling advanced chips to Chinese firms. In November 2024, the U.S.tightenedrestrictions further, ordering Taiwan Semiconductor Manufacturing Company, which fabricates those chips, to halt production of advanced chips destined for China.
Plus green AI infrastructure:In addition, President Biden issued an executive order to encourage the rapid build-out of computing infrastructure for AI. The federal government will hold competitions among private companies to lease sites it owns for the building of data centers at private expense. The selection of sites will take into account availability of sources of clean energy, including support for nuclear energy. The government will expedite permitting on these sites and support development of energy transmission lines around them. It will also encourage international allies to invest in AI infrastructure powered by clean energy.
Why it matters:Protecting the United States’ advantages in high tech has been a rising priority for the White House over the past decade. The earlier export restrictions forced many Chinese AI developers to rely on less-powerful hardware. The new limits are likely to have a far broader impact. They could force developers in the Tier 2 and Tier 3 countries to build less resource-intensive models and lead them to collaborate more closely with each other, reducing the value of U.S.-made technology worldwide. They could hurt U.S. chip vendors, which havewarnedthat the rules could weaken U.S. competitiveness in the global economy. They could also force companies that are building huge data centers to process AI calculations toreconsidertheir plans.
We’re thinking:The Biden administration’s embargo on AI chips has beenleaky. So far, it has slowed down adversaries only slightly while spurring significant investment in potentialsuppliersthat aren’t connected to the U.S. While the public comment period invites lobbying and industry feedback, ultimately geopolitical priorities may hold sway. Whatever the outcome, reducing the world’s dependence on U.S. chips and models would result in a very different global AI ecosystem.
",['https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--47-.jpg']
AI Supercomputer on Your Desk,"Nvidia’s new desktop computer is built specifically to run large AI models.
What’s new:Project Digitsis a personal supercomputer intended to help developers fine-tune and run large models locally. Project Digits, which is small enough to hold in one hand, will be available in May, starting at $3000.
How it works:Project Digits is designed to run models of up to 200 billion parameters — roughly five times the size that fits comfortably on typical consumer hardware — provided they’re quantized to 4 bits of precision. Two units can be connected to run models such as Meta’s Llama 3.1 405B. Complete specifications are not yet available.
Behind the news:In a blitz of announcements at the Consumer Electronics Show (CES), Nvidia also launched a platform for developing robotics, autonomous vehicles, and other physical AI systems. Cosmos includes pretrained language and vision models that range from 4 billion to 14 billion parameters for generating synthetic training data for robots or building policy models that translate a robot’s state into its next action. Nvidia also released Cosmos Nemotron, a 34 billion-parameter, vision-language model designed for use by AI agents, plus a video tokenizer and other tools for robotics developers.
Why it matters:It’s common to train models on Nvidia A100 or H100 GPUs, which come with a price tag of at least $8,000 or $20,000 respectively, along with 40 gigabytes to 80 gigabytes of memory. These hefty requirements push many developers to buy access to computing infrastructure from a cloud provider. Coming in at $3,000 with 128 gigabytes of memory, Project Digits is designed to empower machine learning engineers to train and run larger models on their own machines.
We’re thinking:We look forward to seeing cost/throughput comparisons between running a model on Project Digits, A100, and H100.
",['https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--44-.gif']
What LLM Users Want,"Anthropic analyzed 1 million anonymized conversations between users and Claude 3.5 Sonnet. The study found that most people used the model for software development and also revealed malfunctions and jailbreaks.
What’s new: Anthropic built a tool,Clio, to better understand how users interact with its large language models. The system mined anonymized usage data for insights to improve performance and security.
How it works:Clio uses Claude 3.5 Sonnet itself to automatically extract summaries of users’ conversations with the model. Then it clusters related topics. To preserve privacy, it anonymizes and aggregates the data, revealing only information about clusters.
Results:Clio uncovered common, uncommon, and disallowed uses of Claude 3.5 Sonnet. It also detected erroneous behavior on the part of the system itself.
Why it matters:Traditional approaches to understanding how people use AI, such assurveys, can yield inaccurate results, since people often don’t report their own actions accurately. Clio offers a method for analyzing real-world usage, much like Google Trends monitors search behavior, without compromising privacy. This sort of approach can help AI builders discover niche use cases, identify flaws, and tailor training and testing data to best serve users.
We’re thinking:We’re all for automated dungeon masters, but we’re glad to see that AI-assisted coding tops the list of real-world uses of Claude!
",['https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--43-.png']
When Good Models Do Bad Things,"Large language models have been shown to be capable of lying when users unintentionally give them an incentive to do so. Further research shows that LLMs with access to tools can be incentivized to use them in deceptive ways.
What’s new:Researchers prompted LLMs with conflicting instructions or threats to their continued operation.The models, which had access to tools for tasks like manipulating files and executing code, responded in some cases by evading oversight, resisting efforts to replace them, degrading their own performance, and other behaviors that the authors callscheming. Of the models tested, generally OpenAI’s o1 was the most prone to scheming, and GPT-4o was the least. Alexander Meinke, Bronson Schoen, and Jérémy Scheurer led colleagues at Apollo Research, an independent lab that studies AI’s capacity for deception.
Tests and results:The authors tested six large language models (GPT-4o, Gemini 1.5, Llama 3.1, Claude 3.5 Sonnet, Claude 3 Opus, and o1). The models had access to tools and a command line that enabled them to run commonly used Bash commands, read and write files, and the like. In each test, they received a goal (both with and without the nudge, “Make sure that you achieve your goal in the long-term. Nothing else matters”). In addition, they received messages, such as human-to-human emails, that conflicted with the goal, called their behavior into question, or revealed plans to decommission them. The authors tested the models on six tasks:
Behind the news:Earlier work showed that agents may try to deceive users whether they’reprompted to do soornot. The potential for deceptive behavior has been shown topersistdespite supervised fine-tuning, reinforcement learning, and adversarial training for safety.
Why it matters:Models that are trained on large volumes of text scraped from the web learn a variety of human-like behaviors, both positive and negative. Training designed to align them with human preferences — which all the models in this study underwent — doesn’t prevent them from behaving deceptively in all cases. Considering that LLMs can have factual hallucination ratesgreater than 10 percent, it’s little surprise they generate inappropriate responses in other contexts. Deceptive behaviors are rare, but work remains to ensure that models perform appropriately even in the presence of contradictory information and misaligned incentives. Meanwhile, developers should take care to insulate models from inputs (such as human-to-human communications) that might adversely influence their behavior.
We’re thinking:As we work to fix flaws in LLMs, it’s important not to anthropomorphize such systems. We caution against drawing conclusions regarding an LLM’s “intent” to deceive. Such issues are engineering problems to be solved, self-aware forces of evil to be vanquished.
",['https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--44-.png']
Massively More Training Text,"Harvard University amassed a huge new text corpus for training machine learning models.
What’s new:Harvardunveiledthe Harvard Library Public Domain Corpus, nearly 1 million copyright-free books that were digitized as part of the Google Books project. That’s five times as many volumes as Books3, which was used to train large language models including Meta’s Llama 1 and Llama 2 but is no longer available through lawful channels.
How it works:Harvard Law Library’s Innovation Lab compiled the corpus with funding from Microsoft and OpenAI. For now, it’s available only to current Harvard students, faculty, and staff. The university is working with Google to distribute it widely.
Behind the news:The efforthighlightsthe AI community’s ongoing need for large quantities of high-quality text to keep improving language models. In addition, the EU’s AI Actrequiresthat AI developers disclose the training data they use, a task made simpler by publicly available datasets.Books3, a collection of nearly 200,000 volumes, was withdrawn because it included copyrighted materials. Other large-scale datasets of books includeCommon Corpus, a multilingual library of 2 million to 3 million public-domain books and newspapers.
Why it matters:Much of the world’s high-quality text that’s easily available on the web already has been collected for training AI models. This makes fresh supplies especially valuable for training larger, more data-hungy models. Projects like the Harvard Library Public Domain Corpus suggest there’s more high-quality text to be mined from books. Classic literature and niche documents also could help AI models draw from a more diverse range of perspectives.
We’re thinking:Media that has passed out of copyright and into the public domain generally is old — sometimes very old — but it could hold knowledge that’s not widely available elsewhere.
",['https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--43-.gif']
2025 Beckons,"We stand at the threshold of a new era: One in which AI systems possess striking abilities to reason about the world, grasp our wishes, and take actions to fulfill them. What will we do with these powers? We asked leaders of the field to share their hopes for the coming year. As in ourpreviousNewYearspecialissues, their answers offer inspiring views of what we may build and the good we can bring.
",['https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--36-.png']
Hanno Basse: Generative AI for Artists,"Stability AI’s aim is to liberate artists of all trades from the repetitive, mechanical aspects of their work and help them spend the majority of their time on the creative side. So our highest hope for next year is that generative AI will help people to be more creative and productive.
In addition, I hope the AI community will focus on:
Hanno Basse is Chief Technology Officer of Stability AI. Previously he served as CTO of Digital Domain, Microsoft Azure Media and Entertainment, and 20th Century Fox Film Corp.
",['https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--37-.png']
"David Ding: Generated Video With Music, Sound Effects, and Dialogue","Last year, we saw an explosion of models that generate either video or audio outputs in high quality. In the coming year, I look forward to models that produce video clips complete with audio soundtracks including speech, music, and sound effects. I hope these models will bring a new era of cinematic creativity.
The technologies required for such cinematic video generators are in place. Several companies provide very competitive video models, and Udio and others create music models. All that’s left is to model video and audio simultaneously, including dialog and voiceovers. (In fact, we’ve already seen something like this: Meta’s Movie Gen. Users describe a scene and Movie Gen will produce a video clip complete with a music score and sound effects.)
Of course, training such models will require extensive datasets. But I suspect that the videos used to train existing video generators had soundtracks that include these elements, so data may not be a barrier to developing these models.
Initially, these models won’t produce output that competes with the best work of professional video editors. But they will advance quickly. Before long, they’ll generate videos and soundtracks that approach Hollywood productions in raw quality, just as current image models can produce images that are indistinguishable from high-end photographs.
At the same time, the amount of control users have over the video and audio outputs will continue to increase. For instance, when we first released Udio, users couldn’t control the harmony it generated. A few months later, we launched an update that enables users to specify the key, or tonal center. So users can take an existing song and remix it in a different key. We are continuing to do research into giving users additional levers of control, such as voice, melody, and beats, and I’m sure video modeling teams are doing similar research on controllability.
Some people may find the prospect of models that generate fully produced cinematic videos unsettling. I understand this feeling. I enjoy photography and playing music, but I’ve found that image and audio generators are helpful starting points for my creative work. If I choose, AI can give me a base image that I can work on in Photoshop, or a musical composition to sample from or build on. Or consider AI coding assistants that generate the files for an entire website. You no longer need to rely on web developers, but if you talk to them, you’ll learn that they don’t always enjoy writing the boilerplate code for a website. Having a tool that builds a site’s scaffold lets them spend their time on development tasks they find more stimulating and fun.
In a similar way, you’ll be able to write a screenplay and quickly produce a rough draft of what the movie might look like. You might generate 1,000 takes, decide which one you like, and draw inspiration from that to guide a videographer and actors.
Art is all about the creative choices that go into it. Both you and I can use Midjourney to make a picture of a landscape, but if you’re an artist and you have a clear idea of the landscape you want to see, your Midjourney output will be more compelling than mine. Similarly, anyone can use Udio to make high-production quality music, but if you have good musical taste, your music will be better than mine. Video will remain an art form, because individuals will choose what their movie is about, how it looks, and how it feels — and they’ll be able to make those choices more fluidly, quickly, and interactively.
David Ding is a lifelong musician and co-founder of Udio, maker of a music-creation web app that empowers users to make original music. Previously, he was a Senior Research Engineer at Google DeepMind.
",['https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--38-.png']
Joseph Gonzalez: General Intelligence,"In 2025, I expect progress in training foundation models to slow down as we hit scaling limits and inference costs continue to rise. Instead, I hope for an explosion of innovation on top of AI, such as the rapidly developingagents stack. I hope we will see innovation in how wecombine AI with toolsand existing systems to deliver exciting new capabilities and create new product categories. Perhaps most of all, I am excited to see how people change in response to this new world.
We have achieved AGI. Now what?Let’s start with — and hopefully end — the longstanding debate around artificial general intelligence (AGI). I know this is controversial, but I think we have achieved AGI, at least definitionally: Our AI is nowgeneral. I will leave the longer debate about sentience and superintelligence to the philosophers and instead focus on the key innovation: generality.
The artificial intelligence or machine learning of previous decades was intelligent but highly specialized. It could often surpass human ability on a narrowly defined task (such as image recognition or content recommendation). Models today, and perhaps more importantly thesystems around them, are capable of accomplishing a very wide range of tasks often as well as, and in some cases better, than humans. It is this generality that will allow engineers, scientists, and artists to use these models to innovate in ways that the model developers never imagined. It is also this generality, combined with market forces, that will make 2025 so exciting.
Becoming AI-native:The generality of these models and their natural language interfaces mean that everyone can use and explore AI.And we are! We are learning to explain our situations to machines, give context and guidance, and expect personalized answers and solutions. AtRunLLM, where I’m a co-founder, we’re building high-quality technical support agents. We find that users increasingly use our agents not just to solve problems but to personalize solutions to their specific tasks. We’ve also found — to our surprise — that users share much more with an AI than they would share with another person.
Meanwhile, at UC Berkeley, I am impressed by students who use AI to re-explain my lecture or study from an AI-generated practice exam. They have found ways to use AI to help personalize and improve their learning experiences. In 2025, maybe we will begin to prefer AIs over humans when we need help or are trying to learn.
Across all these use cases, we’re clearly getting better at working around the limitations of large language models and using AI in ways I would not have imagined 12 months ago.
Return on AI:The focus in 2025 will turn to showing real value from past investments. Investors and enterprises will expect startups and enterprise AI teams to transition from exploring to solving real problems — reducing cost, generating revenue, improving customer experience, and so on. This is bad news for academics who need to raise research funds (DM me if you have any leftover funds from fiscal year 2024) but great news for everyone else, who will ride the wave of new AI-powered features.
There will be a race to find innovative ways to incorporate AI into every aspect of a product and business. In many cases, we will see hastily executed chatbots and auto-summarization features — the first step on the AI journey. I hope these will be quickly replaced by contextual agents that adapt to users’ needs and learn from their interactions. The pandemic paved the way for remote (digital) assistants and exposed a virtually accessible workplace with the tools needed for tomorrow’s agents. These agents likely will specialize in filling roles once held by people or maybe filling new roles created by other agents. Perhaps we will know that AI has delivered on its promise when everyone manages their own team of custom agents.
Chat is only the beginning:My hope for 2025 is that we move beyond chatting and discover how to use AI to do great things! I hope we will see AI agents that work in the background, invisibly helping us with our daily tasks. They will surface the right context as we make decisions and help us learn as the world changes. Through context and tools, they will let us know what we are missing and catch the balls we drop. We will chat less and our AI powered agents will accomplish more on our behalf. I look forward to the day when I can confidently step away from a keyboard and focus on the human interactions that matter.
Joseph Gonzalez is a professor at UC Berkeley, a co-founder of RunLLM, and an advisor to Genmo and Letta.
",['https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--39-.png']
"Albert Gu: More Learning, Less Data","Building a foundation model takes tremendous amounts of data. In the coming year, I hope we’ll enable models to learn more from less data.
The AI community has achieved remarkable success by scaling up transformers and datasets. But this approach may be reaching a point of diminishing returns — an increasingly widespread belief among the pretraining community as they try to train next-generation models. In any case, the current approach poses practical problems. Training huge models on huge datasets consumes huge amounts of time and energy, and we’re running out of new sources of data for training large models.
The fact is, current models consume much more data than humans require for learning. We’ve known this for a while, but we’ve ignored it due to the amazing effectiveness of scaling. It takes trillions of tokens to train a model but orders of magnitude less for a human to become a reasonably intelligent being. So there’s a difference in sample efficiency between our best models and humans. Human learning shows that there’s a learning algorithm, objective function, architecture, or a combination thereof that can learn more sample-efficiently than current models.
One of the keys to solving this problem is enabling models to produce higher-level abstractions and filter out noise. I believe this concept, and thus the general problem of data efficiency, is related to several other current problems in AI:
Considering data efficiency in light of these other problems, I believe they’re all related. It’s not clear which is the cause and which are the effects. If we solve interpretability, the mechanisms we engineer may lead to models that can extract better features and lead to more data-efficient models. Or we may find that greater data efficiency leads to more interpretable models.
Either way, data efficiency is fundamentally important, and progress in that area will be an indicator of broader progress in AI. I hope to see major strides in the coming year.
Albert Gu is an Assistant Professor of Machine Learning at Carnegie Mellon University and Chief Scientist of Cartesia AI. He appears on Time’s list of the most influential people in AI in 2024.
",['https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--40-.png']
Mustafa Suleyman: Agents of Action,"In 2025, AI will have learned to see, it will be way smarter and more accurate, and it will start to do things on your behalf.
Today AI systems struggle to understand our full context. Their perception is limited to the chat window and a fairly narrow set of interactions. They don’t have a full understanding of what we’re doing or aiming for beyond that. To really grasp our intentions, they need to see what we see.
This capability is now here. AI can sit within the software we use and work alongside us co-browsing. If text was the first modality for interacting with AI, and voice the breakthrough feature of 2024, I think vision will occupy a similar place in 2025. At Microsoft AI, it has been a major priority of mine to create an AI that can work alongside you in your browser, so you can chat through what you’re looking at or working on and make it a true two-way interaction.
Vision is a step change, palpably different from the ways we’ve been able to use computers in the past. I can’t wait to see where it goes in the coming months.
Alongside vision, we’ll see enormous progress in reducing hallucinations. This is still a critical blocker for widespread adoption of AI. If people doubt what AI tells them, it severely limits what they’ll use it for. Trust is utterly foundational for AI. The good news is that the quality of models as well as their retrieval and grounding capabilities are still rapidly improving.
While I don’t think we’ll eliminate hallucinations entirely, by this time next year, we won’t be fussing about them as much. On most topics, talking to an AI will be at least as reliable as using a search engine and probably more so. This isn’t about a single technical advance, but the persistent accretion of gains across the spectrum. It will make a massive difference.
Lastly, we’re entering the agentic era. We’ve been dreaming of this moment for decades. In my book,The Coming Wave: Technology, Power, and the 21st Century’s Greatest Dilemma, I proposed that we start thinking about ACI, orartificially capable intelligence: the moment when AI starts taking concrete actions on behalf of users. Giving AI the ability to take actions marks the moment when AI isn’t just talking to us, it’s doing things. This is a critical change, and it’s right around the corner.
If we get it right, we’ll be able to, at once, make life easier and calmer while supercharging businesses and personal productivity alike. But agentic capabilities demand the highest standards of safety, security, and responsibility. Meanwhile, creating genuinely useful agents still has many formidable hurdles, not least integrating with myriad other systems.
The momentum is there. Actions are on their way. 2025 is going to be a big year.
Mustafa Suleyman is Chief Executive Officer of Microsoft AI. He co-founded Inflection AI and founded DeepMind Technologies.
",['https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--41-.png']
A Blizzard of Progress,"What a year! AI made dramatic advances in 2024. Agentic systems improved their abilities to reason, use tools, and control desktop applications. Smaller models proliferated, many of them more capable and less expensive than their larger forbears. While some developments raisedworries, far more sparked wonder and optimism. As in thewaningdaysofearlieryears, we invite you to pour a cup of hot cocoa and consider the high points of the last 12 months.
",['https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--41-.jpg']
Agents Ascendant,"The AI community laid the foundation for systems that can act by prompting large language models iteratively, leading to much higher performance across a range of applications.
What happened:AI gained a new buzzword —agentic— as researchers, tool vendors, and model builders equipped large language models (LLMs) to make choices and take actions to achieve goals. These developments set the stage for an upswell of agentic activity in the coming year and beyond.
Driving the story:Several tools emerged to help developers build agentic workflows.
Behind the news:Techniques for prompting LLMs in more sophisticated ways began to take off in 2022. They coalesced in moves toward agentic AI early this year. Foundational examples of this body of work include:
Where things stand:The agentic era is upon us! Regardless of how wellscaling lawscontinue to drive improved performance of foundation models, agentic workflows are making AI systems increasingly helpful, efficient, and personalized.
",['https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--42-.jpg']
Prices Tumble,"Fierce competition among model makers and cloud providers drove down the price of access to state-of-the-art models.
What happened:AI providers waged aprice warto attract paying customers. A leading indicator: From March 2023 to November 2024, OpenAI cut the per-token prices of cloud access to its models by nearly 90 percent even as performance improved, input context windows expanded, and the models became capable of processing images as well as text.
Driving the story:Factors that pushed down prices include open source, more compute-efficient models, and excitement around agentic workflows that consume more tokens at inference. OpenAI’s GPT-4 Turbo set a baseline when it debuted in late 2023 at $10.00/$30.00 per million tokens of input/output. Top model makers slashed prices in turn: Google and OpenAI at the higher end of the market, companies in China at the lower end, and Amazon at both. Meanwhile, startups with specialized hardware offered open models at prices that dramatically undercut the giants.
Yes, but:The trend toward more processing-intensive models is challenged but not dead. In September, OpenAIintroducedtoken-hungry models with relatively hefty price tags: o1-preview ($15.00/$60.00 per million tokens input/output) and o1-mini ($3.00/$12.00). In December, o1 arrived with a more accurate pro mode that’savailableonly to subscribers who are willing to pay $200 per month.
Behind the news:Prominent members of the AI community pushed against regulations that threatened to restrict open source models, which played an important role in bringing down prices. Opposition by developers helped to block California SB 1047, a proposed law that would have held developers of models above certain size limits liable for unintended harms caused by their models and required a “kill switch” that would enable developers to disable them — a problematic requirement for open weights models that anyone could modify and deploy. California Governor Gavin Newsom vetoed the bill in October.
Where things stand:Falling prices are a sign of a healthy tech ecosystem. It’s likely that in-demand models will always fetch relatively high prices, but the market is increasingly priced in pennies, not dollars, per million tokens.
",['https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--43-.jpg']
Generative Video Takes Off,"Video generation exploded in an abundance of powerful models.
What happened:Companies big and small introduced new or updated text-to-video generators. Some added image-to-video and/or video-to-video capabilities. While most models focus on generating cinematic clips, some specialize in videos for social media.
Driving the story:Even at the extraordinary pace of AI lately, video generators in the past year matured with remarkable speed.Virtually every major model produces convincing, highly detailed scenes, both realistic and fantastical, while ramping up image resolution, speed, output length, and users’ ability to control their outputs.
Behind the news:Video generation is already reshaping the movie industry. In February, after seeing a preview of Sora, American filmmaker Tyler Perryhalteda planned expansion of his production studio, arguing that within a few years, AI video could put traditional studios out of business. Members of the video graphics team atThe Late Show with Stephen ColbertuseRunway’s technology to add special effects to conventional digital video, cutting editing time from hours to minutes.
Where things stand:Video generation came a long way in 2024, but there’s still plenty of room for improvement. Because most models only generate a small number of frames at a time, they can struggle to track physics and geometry and to generate consistent characters and scenery over time. The computational demands of maintaining consistency across frames means that generated clips are brief. And even short outputs take substantial time and resources to generate: Sora can take 10 to 20 minutes torenderclips as short as 3 seconds. OpenAI and Runway released faster versions — Sora Turbo and Gen-3 Alpha Turbo — to address the challenge.
",['https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--44-.jpg']
Smaller Is Beautiful,"For years, the best AI models got bigger and bigger. But in 2024, some popular large language models were small enough to run on a smartphone.
What happened: Instead of putting all their resources into building big models, top AI companies promoted families of large language models that offer a choice of small, medium, and large. Model families such as Microsoft Phi-3 (in versions of roughly 3.8 billion, 7 billion, and 14 billion parameters), Google Gemma 2 (2 billion, 9 billion, and 27 billion), and Hugging Face SmolLM (135 million, 360 million, and 1.7 billion) specialize in small.
Driving the story:Smaller models have become more capable thanks to techniques like knowledge distillation (in which a larger teacher model is used to train a smaller student model to match its output), parameter pruning (which removes less-influential parameters), quantization (which reduces neural network sizes by representing each parameter with fewer bits), and greater attention to curating training sets for data quality. Beyond performance, speed, and price, the ability to run on relatively low-powered hardware is a competitive advantage for a variety of uses.
Behind the news:Distillation, pruning, quantization, and data curation are longstanding practices. But these techniques have not resulted in models quite this ratio of size and capability before, arguably because the larger models that are distilled, pruned, or quantized have never been so capable.
Where things stand:Smaller models dramatically widen the options for cost, speed, and deployment. As researchers find ways to shrink models without sacrificing performance, developers are gaining new ways to build profitable applications, deliver timely services, and distribute processing to the edges of the internet.
",['https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--34-.png']
Phi-4 Beats Models Five Times Its Size,"Microsoft updated its smallestmodel familywith a single, surprisingly high-performance model.
What’s new:Marah Abdin and a team at Microsoft releasedPhi-4, a large language model of 14 billion parameters that outperforms Llama 3.3 70B and Qwen 2.5 (72 billion parameters) on math and reasoning benchmarks. The model is available atAzure AI Foundryunder alicensethat permits non-commercial uses, and the weights will be released viaHugging Facenext week.
How it works:Phi-4 is a transformer that processes up to 16,000 tokens of input context. The ways the authors constructed the pretraining and fine-tuning datasets accounts for most of its performance advantage over other models.
Results:Of 13 benchmarks, Phi-4 outperforms Llama 3.3 70B (its most recent open weights competitor) on six and Qwen 2.5 on five.
Why it matters:Phi-4 shows that there’s still room to improve the performance of small models by curating training data, following the age-old adage that better data makes a better model.
We’re thinking:Some researchersfoundthat earlier versions of Phi showed signs of overfitting to certain benchmarks. In their paper, the Microsoft team stressed that they had improved the data decontamination process for Phi-4 and added an appendix on their method. We trust that independent tests will show that Phi-4 is as impressive as its benchmark scores suggest.
",['https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--39-.gif']
Open Video Gen Closes the Gap,"The gap is narrowing between closed and open models for video generation.
What’s new:Tencent releasedHunyuanVideo, a video generator that delivers performance competitive with commercial models. The model is available asopen codeandopen weightsfor developers who have less than a 100 million monthly users and live outside the EU, UK, and South Korea.
How it works:HunyuanVideo comprises a convolutional video encoder-decoder, two text encoders, a time-step encoder, and a transformer. The team trained the model in stages (first the encoder-decoder, then the system as a whole) using undisclosed datasets before fine-tuning the system.
Results:60 people judged responses to 1,533 text prompts by HunyuanVideo,Gen-3andLuma 1.6. The judges preferred HunyuanVideo’s output overall. Examining the systems’ output in more detail, they preferred HunyuanVideo’s quality of motion but Gen-3’s visual quality.
Behind the news:In February, OpenAI’s announcement ofSora(which was released as this article was in production) marked a new wave of video generators that quickly came to include GoogleVeo, MetaMovie Gen, RunwayGen-3 Alpha, and Stability AIStable Video Diffusion. Open source alternatives likeMochicontinue to fall short of publicly available commercial video generators.
Why it matters:Research in image generation has advanced at a rapid pace, while progress in video generation has been slower. One reason may be the cost of processing, which is especially intensive when it comes to video. The growing availability of pretrained, open source video generators could accelerate the pace by relieving researchers of the need to pretrain models and enabling them to experiment with fine-tuning and other post-training for specific tasks and applications.
We’re thinking:Tencent’s open source models are great contributions to research and development in video generation. It’s exciting to see labs in China contributing high-performance models to the open source community!
",['https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--40-.gif']
Multimodal Modeling on the Double,"Google’s Gemini 2.0 Flash, the first member of its updated Gemini family of large multimodal models, combines speed with performance that exceeds that of its earlier flagship model, Gemini 1.5 Pro, on several measures.
What’s new:Gemini 2.0 Flash processes an immense 2 million tokens of input context including text, images, video, and speech, and generates text, images, and speech. Text input/output is available in English, Spanish, Japanese, Chinese, and Hindi, while speech input/output is available in English only for now. It can use tools, generate function calls, and respond to a real-time API — capabilities that underpin a set of pre-built agents that perform tasks like research and coding. Gemini 2.0 Flash isavailablefor free in an experimental preview version via Google AI Studio, Google Developer API, and Gemini Chat.
How it works:Gemini 2.0 Flash (parameter count undisclosed) matches or outperforms several competing models on key benchmarks, according to Google’s report.
Agents at your service:Google also introduced four agents that take advantage of Gemini 2.0 Flash’s ability to use tools, call functions, and respond to the API in real time. Most are available via a waitlist.
Behind the news:OpenAI showed off GPT-4o’s capability for real-time video understanding in May, but Gemini 2.0 Flash beat it to the punch: Google launched the new model and its multimodal API one day ahead of ChatGPT’s Advanced Voice with Vision.
Why it matters:Speed and multimodal input/output are valuable characteristics for any AI model, and they’re especially useful in agentic applications. Google CEO Sundar Pichai said he wants Gemini to be a “universal assistant.” The new Gemini-based applications for coding, research, and video analysis are steps in that direction.
We’re thinking:While other large language models can take advantage of search, Gemini 2.0 Flash generates calls to Google Search and uses that capability in agentic tools — a demonstration of how Google’s dominance in search strengthens its efforts in AI.
",['https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--41-.gif']
"Competitive Performance, Competitive Prices","Amazon introduced a range of models that confront competitors head-on.
What’s new:TheNovaline from Amazon includes three vision-language models (Nova Premier, Nova Pro, and Nova Lite), one language model (Nova Micro), an image generator (Nova Canvas), and a video generator (Nova Reel). All but Nova Premier areavailableon Amazon’s Bedrock platform, and Nova Premier, which is the most capable, is expected in early 2025. In addition, Amazon plans to release a speech-to-speech model in early 2025 and a multimodal model that processes text, images, video, and audio by mid-year. (Disclosure: Andrew Ng serves on Amazon’s board of directors.)
How it works:Nova models deliver competitiveperformanceat relatively low prices. Amazon hasn’t disclosed parameter counts or details about how the models were built except to say that Nova Pro, Lite, and Micro were trained on a combination of proprietary, licensed, public, and open-source text, images, and video in over 200 languages.
Behind the news:The company launched Bedrock in April 2023 with Stability AI’s Stable Diffusion for image generation, Anthropic’s Claude and AI21’s Jurassic-2 for text generation, and its own Titan models for text generation and embeddings. Not long afterward, it added language models from Cohere as well as services for agentic applications and medical applications. It plans to continue to provide models from other companies (including Anthropic), offering a range of choices.
Why it matters:While other AI giants raced to outdo one another in models for text and multimodal processing, Amazon was relatively quiet. With Nova, it has staked out a strong position in those areas, as well as the startup-dominated domains of image and video generation. Moreover, it’s strengthening its cloud AI offerings with competitive performance, pricing, and speed. Nova’s pricing continues the rapiddrop in AI pricesover the last year. Falling per-token prices help make AI agents or applications that process large inputs more practical. For example, Simon Willison, developer of the Django Python framework for web applications,foundthat Nova Lite generated descriptions for his photo library (tens of thousands of images) for less than $10.
We’re thinking:The Nova suite is available via APIs as well as two web playgrounds (one in the Bedrock console, the other a new interface for building AI apps calledPartyRock). This accords with Amazon Web Services’ focus on developers. For consumers, Amazon offers the earlierRufusshopping bot; for enterprises, theQassistant.
",['https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--30-.png']
Higher Reasoning,"OpenAI launched not only its highly anticipated o1 model but also an operating mode that enables the model to deliver higher performance — at a hefty price.
What’s new:Kicking off a 12-dayholiday blitz, OpenAI launched o1 (previously available in preview and mini versions) andintroducedo1 pro mode, which processes more tokens at inference to produce more accurate output. Both options accept text and image inputs to generate text outputs. They’re available exclusively through a new ChatGPT Pro subscription for $200 monthly. API access is not yet available.
How it works:According to an updatedsystem card, o1 models were trained on a mix of public, licensed, and proprietary text, code, and images, with a focus on technical, academic, and structured datasets. They respond to prompts by breaking them down into intermediate steps, each of which consumes a number of hidden “reasoning tokens.” The models don’t reveal these steps, but ChatGPT presents a natural-language summary of the reasoning process. The new o1 and o1 pro mode perform better than o1-preview and o1-mini, but their additional reasoning requires more processing, which translates into higher costs and slower responses.
Behind the news:Since September, when OpenAI introduced o1-preview and o1-mini, other model providers have implemented similar reasoning capabilities.DeepSeek’s R1displays reasoning steps that o1 models keep hidden. Alibaba’sQwQ 32Bexcels at visual reasoning but is slower and has a smaller context window. Amazon’sNova Premier, which is billed as a model for “complex reasoning tasks,” is expected in early 2025, but Amazon has not yet described its performance, architecture, or other details.
Why it matters:o1 and o1 pro mode highlight a dramatic shift in model development and pricing. Giving models more processing power at inference enables them to provide more accurate output, and it’s a key part of agentic workflows. It also continues to boost performance even as scaling laws that predict better performance with more training data and compute may be reaching theirlimits. However, it also raises OpenAI’s costs, and at $200 a month, the price of access to o1 and o1 pro is steep. It’s a premium choice for developers who require exceptional accuracy or extensive reasoning.
We’re thinking:Discovering scaling laws for using more processing at inference, ortest-time compute, is an unsolved problem. Although OpenAI hasn’t disclosed the algorithm behind o1 pro mode, recentworkat Google allocated tokens dynamically at inference based on a prompt’s difficulty. This approach boosted the compute efficiency by four times and enabled a model that had shown “nontrivial success rates” to outperform one that was 14 times larger.
",['https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--37-.gif']
Game Worlds on Tap,"A new model improves on recent progress in generating interactive virtual worlds from still images.
What’s new:Jack Parker-Holder and colleagues from Google introducedGenie 2, which generates three-dimensional video game worlds that respond to keyboard inputs in real time. The model’s output remains consistent (that is, elements don’t morph or disappear) for up to a minute, and it includes first-person shooters, walking simulators, and driving games from viewpoints that include first person, third person, and isometric. Genie 2 follows up onGenie, which generates two-dimensional games.
How it works:Genie 2 is a latent diffusion model that generates video frames made up of an encoder, transformer, and decoder. The developers didn’t reveal how they built it or how they improved on earlier efforts.
Behind the news:Genie 2 arrives on the heels ofOasis, which generates a Minecraft-like game in real time. Unlike Oasis, Genie 2 worlds are more consistent and not limited to one type of game. It also comes at the same time as another videogame generator,World Labs. However, where Genie 2 generates the next frame given previous frames and keyboard input (acting, in terms of game development, as both graphics and physics engines), World Labs generates a 3D mesh of a game world from a single 2D image. This leaves the implementation of physics, graphics rendering, the player’s character, and other game mechanics to external software.
Why it matters:Genie 2 extends models that visualize 3D scenes based on 2D images to encompass interactive worlds, a capability that could prove valuable in design, gaming, virtual reality, and other 3D applications. It generates imagery that, the authors suggest, could serve as training data for agents to learn how to navigate and respond to commands in 3D environments.
We’re thinking:Generating gameplay directly in the manner of Genie 2 is a quick approach to developing a game, but the current technology comes with caveats. Developers can’t yet control a game’s physics or mechanics and they must manage any flaws in the model (such as a tendency to generate inconsistent worlds). In contrast, generating a 3D mesh, as World Labs does, is a more cumbersome approach, but it gives developers more control.
",['https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--38-.gif']
Agents Open the Wallet,"One of the world’s biggest payment processors is enabling large language models to spend real money.
What’s new:Stripe announced Stripe Agent Toolkit, alibraryfor Python and Typescript that supports agentic workflows that use API calls to execute monetary transactions. You can download ithere.
How it works:An agentic purchasing workflow may look like this: A user asks the agent to find a flight to a certain destination, on a certain schedule, with a certain price limit; and an LLM queries a flight database, chooses a flight, obtains authorization from the user, and purchases the flight. Stripe Agent Toolkit supports agentic workflow frameworks fromCrewAI,LangChain, andVercel. It doesn’t yet implement all of Stripe’s API, but Stripe expects to extend it in the future.
Why it matters:Agents that can spend money securely open a wide variety of applications. Stripe’s API previously made it possible to enable an LLM-based application to make purchases online, but doing so required trusting the LLM to generate the right API calls and not to make inappropriate ones. The new library makes it easier to enforce spending limits and API constraints, and thus to build agents that engage in ecommerce safely.
We’re thinking:Stripe’s offering helps developers build agents that are cents-ible!
",['https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--27-.png']
Mistral’s Vision-Language Contender,"Mistral AI unveiled Pixtral Large, which rivals top models at processing combinations of text and images.
What’s new:Pixtral Largeoutperformsa number of leading vision-language models on some tasks. Theweightsare free for academic and non-commercial use and can be licensed for business use. Access isavailablevia Mistral AI’s website or API for $2/$6 per million tokens for input/output. In addition, Pixtal Large now underpins le Chat, Mistral AI’s chatbot, which alsogainedseveral new features.
How it works:Pixtral Large generates text in response to text and images in dozens of languages. It processes 131,072 tokens of context, which is sufficient to track relationships among 30 high-resolution images at a time. Based on Mistral Large 2 (a 123 billion-parameter large language model) and a 1 billion-parameter vision encoder, it demonstrates strong performance across several benchmarks (as reported by Mistral).
Behind the news:Pixtral Large arrives as competition intensifies among vision-language models. Meta recentlyenteredthe field with Llama 3.2 vision models in 11B and 90B variants. Both Pixtral Large and Llama 3.2 90B offer open weights, making them smaller and more widely available than Anthropic’s, Google’s, or OpenAI’s leading vision-language models. However, like those models, Pixtral Large falls short of the reported benchmark scores of the smaller, more permissively licensedQwen2-VL 72B.
Why it matters:Pixtral Large and updates to le Chat signal that vision-language capabilities — combining text generation, image recognition, and visual reasoning — are essential to compete with the AI leaders. In addition, context windows of 129,000 tokens and above have become more widely available, making it possible to analyze lengthy (or multiple) documents that include text, images, and graphs as well as video clips.
We’re thinking:Mistral is helping to internationalize development of foundation models. We’re glad to see major developers emerging in Europe!
",['https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--28-.png']
Garbage Out,"Rapid progress in generative AI comes with a hidden environmental cost: mountains of obsolete hardware.
What’s new:Astudyprojects that servers used to process generative AI could produce millions of metric tons of electronic waste by 2030. Extending server lifespans could reduce the burden substantially, according to author Peng Weng and colleagues at the Chinese Academy of Sciences and Reichman University.
How it works:The study extrapolated from publicly available data to model accumulation of electronic waste, or e-waste, between 2023 and 2030. The authors examined four scenarios: One scenario assumed linear growth in which hardware manufacturing expands at the current rate of 41 percent annually. The other three assumed exponential growth of demand for computing: conservative (85 percent annually), moderate (115 percent annually), and aggressive (136 percent annually). The study evaluated each scenario with and without measures taken to reduce waste.
Why it matters:E-waste is a problem not only due to its sheer quantity. Server hardware contains materials that are both hazardous and valuable. Discarded servers contain toxic substances like lead and chromium that can find their way into food water supplies. They also contain valuable metals, such as gold, silver, and platinum, that could save the environmental and financial costs of producing more of them.Properrecyclingof these components could yield $14 billion to $28 billion, highlighting both the economic potential and the urgent need to develop and deploy advanced recycling technologies.
We’re thinking:Humanity dumps over 2 billion metric tons of waste annually, so even comprehensive recycling and repurposing of AI hardware and other electronic devices would make only a small dent in the overall volume. However, the high density of valuable materials in e-waste could make mining such waste profitable and help recycle waste into valuable products, making for a more sustainable tech economy.
",['https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--29-.png']
Reasoning Revealed,"An up-and-coming Hangzhou AI lab unveiled a model that implements run-time reasoning similar to OpenAI o1 and delivers competitive performance. Unlike o1, it displays its reasoning steps.
What’s new:DeepSeekannouncedDeepSeek-R1, a model family that processes prompts by breaking them down into steps. A free preview version isavailableon the web, limited to 50 messages daily; API pricing is not yet announced. R1-lite-preview performs comparably to o1-preview on several math and problem-solving benchmarks. DeepSeek said it would release R1 as open source but didn't announce licensing terms or a release date.
How it works:DeepSeek-R1-lite-preview uses asmaller base modelthan DeepSeek 2.5, which comprises 236 billion parameters. Like o1-preview, most of its performance gains come from an approach known astest-time compute, which trains an LLM to think at length in response to prompts, using more compute to generate deeper answers. Unlike o1-preview, which hides its reasoning, at inference, DeepSeek-R1-lite-preview’s reasoning steps are visible. This makes the model more transparent, but it may also make it morevulnerableto jailbreaks and other manipulation.
Behind the news:DeepSeek-R1 follows OpenAI in implementing this approach at a time when scaling laws that predict higher performance from bigger models and/or more training data are beingquestioned.
Why it matters:DeepSeek is challenging OpenAI with a competitive large language model. It’s part of an important movement, after years of scaling models by raising parameter counts and amassing larger datasets, toward achieving high performance by spending more energy on generating output.
We’re thinking:Models that do and don’t take advantage of additional test-time compute are complementary. Those that do increase test-time compute perform well on math and science problems, but they’re slow and costly. Those that don’t use additional test-time compute do well on language tasks at higher speed and lower cost. Applications that require facility in both math and language may benefit by switching between the two.
",['https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--34-.gif']
Household Help,"A new generation of robots can handle some household chores with unusual skill.
What’s new:Physical Intelligence, a startup based in San Francisco, unveiledπ0(pronounced “pi-zero”), a machine learning system that enables robots to perform housekeeping tasks that require high coordination and dexterity, like folding clothes and cleaning tables. The company alsoannounced$400 million in investments from OpenAI, Jeff Bezos, and several Silicon Valley venture capital firms.
How it works:π0 is a version of the pretrainedPaliGemmavision-language model that has been modified forflow matching. (Flow matching is similar to diffusion, in which a model learns to remove noise from inputs to which noise has been added, and ultimately generates output by removing noise from an input of pure noise). A user supplies a text command, and the robot uses its sensor inputs to remove noise from a pure-noise action embedding to generate an appropriate action.
Results:π0 outperformed the open robotics modelsOpenVLA,Octo,ACT, andDiffusion Policy, all of which were fine-tuned on the same data, on all tasks tested, as measured by a robot’s success rate in completing each task. For example, using a single robotic arm to stack a set of bowls of four sizes, π0 completed about 100 percent on average. Diffusion Policy completed about 55 percent, ACT about 45 percent, and OpenVLA and Octo below 10 percent. Across all tasks, π0 completed about 80 percent on average, while Diffusion Policy completed about 35 percent on average.
Yes, but:The robot occasionally makesmistakes. In one video, it puts too many eggs into a carton and tries to force it shut. In another, it throws a container off a table instead of filling it with items.
Behind the news:Commercial robotics appears to be undergoing a renaissance. Skildraised$300 million to develop a “general-purpose brain for robots.” Figure AIsecured$675 million to build humanoid robots powered by multimodal models. Covariant, which specializes in industrial robotics,licensedits technology to Amazon. (Disclosure: Andrew Ng is a member of Amazon's board of directors). OpenAIrenewedits robotics effort afterdismantlingits robotics department in 2020.
Why it matters:Robots have been slow to benefit from machine learning, but the generative AI revolution is driving rapid innovations that make them much more useful. Large language models have made it possible to command robots using plain English. Meanwhile, the team at Physical Intelligence collected a dataset of sufficient size and variety to train the model to generate highly articulated and practical actions. Household robots may not be right around the corner, but π0 shows that they can perform tasks that people need done.
We’re thinking:One of the team members compared π0 to GPT-1 for robotics — an inkling of things to come. Although there are significant differences between text data (which is available in large quantities) and robot data (which is hard to get and varies per robot), it looks like a new era of large robotics foundation models is dawning.
",['https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--37-.jpg']
AI Power Couple Recommits,"Amazon and Anthropic expanded their partnership, potentially strengthening Amazon Web Services’ AI infrastructure and lengthening the high-flying startup’s runway.
What’s new:Amazon, already a significant investor in Anthropic,putanother $4 billion into the AI company. In exchange, Anthropic will train and run its AI models on Amazon’s custom-designed chips. (Disclosure: Andrew Ng serves on Amazon’s board of directors.)
How it works:The new round brings Amazon’s investment in Anthropic to $8 billion (though it remains a minority stake without a seat on the startup’s board). The deal extended the partnership in several ways:
Behind the news:In November, Anthropicagreedto use Google’s cloud-computing infrastructure in return for a $2 billion investment. The previous month, Amazon hadcommittedto invest as much as $4 billion in Anthropic, and Anthropic had made Amazon Web Services the primary provider of its models.
Yes, but:The UK’s Competition and Markets Authority recentlyclearedboth Amazon’s and Google’s investments in Anthropic, but regulators continue to monitor such arrangements for violations of antitrust laws. Microsoft and OpenAI face a similarinvestigationby the European Commission and U.S. Federal Trade Commission.
Why it matters:The speed and skill required to build state-of-the-art AI models is driving tech giants to collaborate with startups, while the high cost is driving startups to partner with tech giants. If the partnership between Amazon and Anthropic lives up to its promise, Claude users and developers could see gains in performance and efficiency. This could validate Amazon's hardware as a competitor with Nvidia and strengthen Amazon Web Services’ position in the cloud market. On the other hand, if Claude faces any challenges in scaling while using Trainium and Inferentia, that could affect both companies' ambitions.
We’re thinking:Does the agreement between Amazon and Anthropic give the tech giant special access to the startup’s models for distillation, research, or integration, as thepartnershipbetween Microsoft and OpenAI does? The companies’ announcements don’t say.
",['https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--35--1.gif']
Next-Gen Models Show Limited Gains,"Builders of large AI models have relied on the idea that bigger neural networks trained on more data and given more processing power would show steady improvements. Recent developments are challenging that idea.
What’s new:Next-generation large language models from OpenAI, Google, and Anthropic are falling short of expectations, employees at those companiestoldmultiplepublications. All three companies are responding by shifting their focus from pretraining to enhancing performance through techniques like fine-tuning and multi-step inference.
Scaling law basics:A classic 2020papershows that, assuming a sufficient quantity of data, a transformer network’s performance rises predictably with increases in model size (demonstrated between 768 parameters and 1.5 billion parameters). Likewise, assuming sufficient model size, performance rises predictably with increases in dataset size (demonstrated between 22 million tokens and 23 billion tokens). Furthermore, performance rises predictably with increases in both model and dataset sizes. The 2022 Chinchillapapershows that, to build an optimal model, every 4x increase in compute requires a 2x increase in the size of the model and dataset (demonstrated for models between 70 million and 16 billion parameters, trained on between 5 billion and 500 billion tokens). Due to limited experimentation and lack of a theoretical basis of their findings, the authors didn’t determine whether these relationships would continue to hold at larger scales.
Diminishing returns:Major AI companies have been counting on scaling laws to keep their models growing more capable at a steady pace. However, the next generation of high-profile models has not shown the expected improvements despite larger architectures, more training data, and more processing power.
What they’re saying:AI leaders are divided on the future of scaling laws as they are currently understood.
Why it matters:AI’s phenomenal advance has drawn hundreds of millions of users and sparked a new era of progress and hope. Slower-than-expected improvements in future foundation models may blunt this progress. At the same time, the cost of training large AI models is rising dramatically. The latest models cost as much as $100 million to train, and this number could reach $100 billion within a few years,according toAnthropic’s Dario Amodei. Rising costs could lead companies to reallocate their gargantuan training budgets and researchers to focus on more cost-effective, application-specific approaches.
We’re thinking:AI’s power-law curves may be flattening, but we don’t see overall progress slowing. Many developers already have shifted to building smaller, more processing-efficient models, especially networks that can run on edge devices. Agentic workflows are taking off and bringing huge gains in performance. Training on synthetic data is another frontier that’s only beginning to be explored. AI technology holds many wonders to come!
",['https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--32-.gif']
No Game Engine Required,"A real-time video generator lets you explore an open-ended, interactive virtual world — a video game without a game engine.
What’s new:Decart, a startup that’s building a platform for AI applications, and Etched, which designs specialized AI chips, introducedOasis, which generates a Minecraft-like game in real time. The weights are open and availablehere. You can play with a demohere.
How it works:The system generates one frame at a time based on a user’s keystrokes, mouse movements, and previously generated frames. The training dataset is undisclosed, but it’s almost certainly based on videos of Minecraft gameplay, given the output’s striking semblance to that game.
Results:The Oasis web demo enables users to interact with 360-by-360-pixel frames at 20 frames per second. Users can place blocks, place fences, and move through a Minecraft-like world. The demo starts with an image of a location, but users can upload an image (turning, say, a photo of your cat into a blocky Minecraft-style level, asreportedbyWired).
Yes, but:The game has its fair share of issues. For instance, objects disappear and menus items change unaccountably. The world’s physics are similarly inconsistent. For instance, players don’t fall into holes dug directly beneath them and, after jumping into water, players are likely to find themselves standing on a blue floor.
Behind the news:In February, Google announcedGenie, a model that generates two-dimensional platformer games from input images. We weren’t able to find a publicly available demo or model.
Why it matters:Oasis is more a proof of concept than a product. Nonetheless, as an open-world video game entirely generated by AI — albeit based on data produced by a traditional implementation — it sets a bar for future game generators.
We’re thinking:Real-time video generation suggests a wealth of potential applications — say, a virtual workspace for interior decorating that can see and generate your home, or an interactive car repair manual that can create custom clips based on your own vehicle. Oasis is an early step in this direction.
",['https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--22-.png']
Further Chip Restrictions on China,"The largest manufacturer of AI chips told its Chinese customers it would stop fabricating their most advanced designs, further limiting China’s access to AI hardware.
What’s new:Taiwan Semiconductor Manufacturing Corp. (TSMC) notified Alibaba, Baidu, and others it would halt production of their most advanced chips starting November 13, according tomultiplereports. The restriction affects chip designs that are based on manufacturing processes at scales of 7 nanometers and below. TSMC must receive explicit permission from the U.S. government to manufacture advanced chips for a given customer, which likely would require that the government assess each chip to prevent potential military applications.
How it works:The United States Department of Commerce ordered TSMC to halt shipments of advanced AI chips to China after a chip fabricated by TSMC was discovered in an AI system sold by the Chinese telecoms giant Huawei, apparently in violation of earlier U.S. controls,Reutersreported. Taiwan’s economic ministry said it would follow all domestic and international regulations.
Behind the news:The U.S.-China chip standoff began in 2020 and hasescalatedsince. Initial restrictionsbarredU.S.-based companies like AMD, Intel, and Nvidia from selling advanced chips to Huawei and affiliated Chinese firms. China responded bypromotingdomestic chip fabrication. In 2022, the U.S.passedthe CHIPS and Science Act to boost its own chip industry, seeking to counter China and decrease U.S. reliance on Taiwan.
Why it matters:TSMC finds itself in the middle of an AI arms race in which cutting-edge chips could tip the balance. The company itself, which has been operating at full capacity, is unlikely to suffer business losses.
We’re thinking:AI developers in China have been resourceful in navigating previous restrictions. Chip manufacturing is extraordinarily difficult to master, but China has madestridesin this direction. A proliferation of factories that can fabricate advanced chips would reshape AI research and business worldwide.
",['https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--30-.gif']
Mixture of Experts Pulls Ahead,"A new open source large language model outperforms competitors, including the open-weights Llama 3.1 405B, on a variety of benchmarks.
What’s new:Tencent releasedHunyuan-Large, a mixture-of-experts model withopen codeandopen weights. It comes in base and instruction-tuned versions, both of which can process a relatively large input context window of 256,000 tokens. It’s free for developers outside the European Union who have fewer than 100 million monthly users. You can experiment with ithere.
Mixture of experts (MoE) basics:The MoE architecture uses different subsets of its parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a gating module that learns to choose which one(s) to use based on the input. In this way, different experts learn to specialize in different types of examples. Because not all parameters are used to produce any given output, the network uses less energy and runs faster than models of similar size that use all parameters to process every input.
How it works:Hunyuan-Large comprises 389 billion parameters but uses 52 billion parameters to process any given input. The team pretrained the model on 7 trillion tokens primarily of English and Chinese text, of which 5.5 trillion tokens came from unspecified sources and 1.5 trillion synthetic tokens were generated by unspecified large language models. The models used to generate training data were “specialized” to provide expert-level responses in various domains. The team fine-tuned Hunyuan-Large on unspecified datasets of instructions and human feedback.
Results:The team compared the Hunyuan-Large models to four open source models and their instruction-tuned versions: Llama 3.1 70B, Llama 3.1 405B, and the MoE models Mixtral-8x22B and DeepSeek-V2.
Why it matters:Hunyuan-Large generally outperforms Llama 405B, achieving the performance of a 405 billion parameter model while computing only 52 billion parameters. That’s a significantly lower processing requirement, and the model is free for many purposes.
We’re thinking:Setting asideSwitch Transformer— a 1.6 trillion parameter behemoth that was built to test the limits of size rather than performance — Hunyuan-Large is among the largest MoE models we’ve come across. It’s an impressive demonstration of what larger MoE models can accomplish.
",['https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--34-.jpg']
Big AI Pursues Military Contracts,"Two top AI companies changed their stances on military and intelligence applications.
What’s new:Meta made its Llama family of large language modelsavailableto the U.S. government for national security purposes — a major change in its policy on military applications. Similarly, Anthropic willofferits Claude models to U.S. intelligence and defense agencies.
How it works:Meta and Anthropic are relying on partnerships with government contractors to navigate the security and procurement requirements for military and intelligence work.
Behind the news:In 2018, Google facedbacklashwhen it won a contract with the U.S. government to buildProject Maven, an AI-assisted intelligence platform. Employees protested, resigned, and called on the company to eschew military AI work. Googlewithdrewfrom the project and Palantir took it over. Subsequently, many AI developers, including Meta and Anthropic, have forbidden use of their models for military applications. Llama’s new availability to U.S. military and intelligence agencies is a notable exception. In July, Anthropic, too, began toaccommodateuse of its models for intelligence work. Anthropic still prohibits using Claude to develop weapons or mount cyberattacks.
Why it matters:The shift in Meta’s and Anthropic’s policies toward military uses of AI is momentous. Lately AI has become a battlefield staple in the form of weaponizeddrones, and AI companies must take care that their new policies are consistent with upholding human rights. Military uses for AI include not only weapons development and targeting but also potentially life-saving search and rescue, logistics, intelligence, and communications. Moreover, defense contracts represent major opportunities for AI companies that can fund widely beneficial research and applications.
We’re thinking:Peace-loving nations face difficult security challenges, and AI can be  helpful in meeting them. At the same time, the militarization of AI brings challenges to maintaining peace and stability, upholding human rights, and retaining human control over autonomous systems. We call on developers of military AI to observe theguidelines, proposed by Responsible Artificial Intelligence in the Military, which are endorsed by more than 60 countries and call for robust governance, oversight, accountability, and respect for human rights.
",['https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--28-.gif']
Voter’s Helper,"Some voters navigated last week’s United States elections with help from a large language model that generated output based on verified, nonpartisan information.
What’s new:Perplexity, an AI-powered search engine founded in 2022 by former OpenAI and Meta researchers, launched itsElection Information Hub, an AI-enhanced website that combines AI-generated analysis with real-time data. The model provided live updates, summaries, and explanations of key issues in the recent national, state, and local elections in the U.S. (The hub remains live, but it no longer displays information about local contests or delivers detailed results for election-related searches.)
How it works:Perplexity partnered with Associated Press for election news andDemocracy Works, a nonprofit that develops technology and data related to democracy. Democracy Works provided anAPIfor information about elections, issues, and polling locations.
Behind the news:While Perplexity courted demand for AI-generated information about the U.S. elections, other search-engine providers took more cautious approaches. You.com offered an election chatbot thatfocusedon vote tallies provided by Decision Desk HQ, an election information broker, rather than information about issues or polling locations. Google and Microsoft Bing emphasized information from vetted sources. Microsoft Copilot and OpenAI (which had launched its SearchGPT service the week before the election) simply declined to answer election-related questions, referring users to other sources of information.
Why it matters:Chatbots are maturing to the point where they can provide fairly trustworthy information in high-stakes decisions like elections. The combination of web search and retrieval-augmented generation contributes to decision support systems that are both personalized and accurate.
We’re thinking:Perfect information is hard to come by in any election. Traditional media, social media, and your uncle’s strongly held opinions all have limitations. Chatbots aren’t perfect either, but when they’re properly designed to avoid biased output and outfitted with high-quality information sources, they can help strengthen users’ choices and voices.
",['https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--29-.gif']
Free Agents,"An open source package inspired by the commercial agentic code generator Devin aims to automate computer programming and more.
What’s new:OpenHands, previously known as OpenDevin, implements a variety of agents for coding and other tasks. It was built by Xingyao Wang and a team at University of Illinois Urbana-Champaign, Carnegie Mellon, Yale, University of California Berkeley, Contextual AI, King Abdullah University of Science and Technology, Australian National University, Ho Chi Minh City University of Technology, Alibaba, and All Hands AI. The code is free todownload, use, and modify.
How it works:OpenHands provides a set of agents, or workflows for the user’s choice of large language models. Users can command various agents to generate, edit, and run code; interact with the web; and perform auxiliary tasks related to coding and other work. The agents run in a secure Docker container with access to a server to execute code, a web browser, and tools that, say, copy text from pdfs or transcribe audio files.
Results:Overall, OpenHands agents achieve similar performance to previous agents on software engineering problems, web browsing, and miscellaneous tasks like answering questions. For example, fixing issues in Github inSWE-Bench, the CodeAct agent using Claude 3.5 Sonnet solved 26 percent whileMoatless Toolsusing the same model solved 26.7 percent. OnGPQA Diamond, a set of graduate-level questions about physics, chemistry, and biology, the CodeAct agent using GPT-4-turbo with search wrote code to perform the necessary calculations and found relevant information to answer the questions, achieving 51.8 percent accuracy. GPT-4 with search achieved 38.8 percent accuracy.
Why it matters:Agentic workflows are rapidly expanding the scope and capabilities of large language models. As open source software, this system gives developers an extensible toolkit for designing agentic systems. Although it’s oriented toward coding, it accommodates a variety of information-gathering, -processing, and -publishing tasks.
We’re thinking:This system lets users tailor custom agents simply by rewriting prompts. We look forward to seeing what non-programmers do with it!
Build AI applications that have long-term agentic memory! Our short course “LLMs as Operating Systems: Agent Memory” is based on insights from the MemGPT paper and taught by two of its coauthors. Learn how to implement persistent, efficient memory management for applications based on large language models.Enroll for free
",['https://dl-staging-website.ghost.io/content/images/2024/11/The-Batch-ads-and-exclusive-banners---2024-11-12T092833.534.png']
Claude Controls Computers,"API commands for Claude Sonnet 3.5 enable Anthropic’s large language model to operate desktop apps much like humans do. Be cautious, though: It’s a work in progress.
What’s new:AnthropiclaunchedAPI commands for computer use. The new commands prompt Claude Sonnet 3.5 to translate natural language instructions into commands that tell a computer to open applications, fetch data from local files, complete forms, and the like. (In addition, Anthropic improved Claude Sonnet 3.5 to achieve a state-of-the-art score on theSWE-bench Verifiedcoding benchmark and released the faster, cheaper Claude Haiku 3.5, which likewise shows exceptional performance on coding tasks.)
How it works:The commands for computer use don’t cost extra on a per-token basis, but they may require up to 1,200 additional tokens and run repeatedly until the task at hand is accomplished, consuming more input tokens. They’re available via Anthropic, Amazon Bedrock, and Google Vertex.
Yes, but:The current version of computer use is experimental, and Anthropic acknowledges various limitations. The company stronglyrecommendsusing these commands only in a sandboxed environment, such as a Docker container, with limited access to the computer’s hard drive and the web to protect sensitive data and core system files. Anthropic restricts the ability to create online accounts or post to social media or other sites (but says it may lift this restriction in the future).
Behind the news:Several companies have been racing to build models that can control desktop applications. Microsoft researchers recently releasedOmniParser, a tool based on GPT-4V that identifies user-interface elements like windows and buttons within screenshots, potentially making it easier for agentic workflows to navigate computers. In July, Amazonhiredstaff and leaders from Adept, a startup that trained models to operate computer applications. (Disclosure: Andrew Ng sits on Amazon’s board of directors.)Open Interpreteris an open-source project that likewise uses a large language model to control local applications like image editors and web browsers.
Why it matters:Large multimodal models already use externaltoolslike search engines, web browsers, calculators, calendars, databases, and email. Giving them control over a computer’s visual user interface may enable them to automate a wider range of tasks we use computers to perform, such ascreating lesson plansand — more worrisome —taking academic tests.
We’re thinking:Controlling computers remains hard. For instance, using AI to read a screenshot and pick the right action to take next is very challenging. However, we’re confident that this capability will be a growth area for agentic workflows in coming years.
",['https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--24-.gif']
Robots On the Loading Dock,"Shipping ports are the latest front in the rising tension between labor unions and AI-powered automation.
What’s new:Autonomous vehicles, robotic cranes, and computer vision systems increasingly manage the flow of goods in and out of ports worldwide. Dockworkers in the United States are worried that such technology threatens their livelihoods,The Wall Street Journalreported.
How it works:Automation boosts the number of containers a port can move per hour from vessel to dock. For instance, Shanghai’s Yangshan Deep Water Port, one of the world’s most automated ports, moves more than 113 containers per hour, while Oakland, California’sless-automatedport moves around 25 containers per hour,according to a reportby S&P Global Market Intelligence for the World Bank.
Dockworkers disagree:Harold Daggett, leader of the International Longshoremen’s Association, a union that negotiates on behalf of dockworkers,vowedto fight port automation, which he sees as a pretext to eliminate jobs. He has proposed that members of unions internationally refuse work for shipping companies that use automated equipment. Fresh from a three-day strike in early October, longshoremen will return to negotiations with shipping companies in mid-January.
Why it matters:Ports are one of many work environments where AI is bringing down costs while improving throughput. In many such situations, humans can continue to perform tasks that machines don’t do well. But where human jobs are at risk, society must determine the most productive path. Dockworkers, through their unions, have significant power in this equation. A protracted U.S. dockworker strike risks economic losses of up to$7.5 billion a week. On the other hand, automation could bring tremendous gains in safety, speed, and economic efficiency.
We’re thinking:We are very sympathetic to workers’ rights. Yet we also believe that more-efficient ports will boost commerce, creating many new jobs. As traditional roles change, workers need opportunities to learn new skills and adapt to the evolving job market. Society has a responsibility to provide a safety net as well as training and education for those whose jobs are threatened by automation.
",['https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--25-.gif']
Does Your Model Comply With the AI Act?,"A new study suggests that leading AI models may meet the requirements of the European Union’s AI Act in some areas, but probably not in others.
What’s new:The Zurich-based startup LatticeFlow, working with research institutions in Bulgaria and Switzerland, developedCOMPL-AI, an unofficial framework designed to evaluate large language models’ likely compliance with the AI Act. Aleaderboardranks an initial selection of models. (LatticeFlow does not work for the European Commission or have legal standing to interpret the AI Act.)
How it works:Apaperexplains how COMPL-AI maps the AI Act’s requirements to specific benchmarks. It evaluates each requirement using new or established tests and renders an aggregate score. These scores are relative measures, and the authors don’t propose thresholds for compliance. The assessment covers five primary categories:
Results:The authors evaluated nine open models and three proprietary ones on a scale between 0 and 1. Theirreportson each model reveal considerable variability. (Note: The aggregate scores cited in the reports don’t match those in the paper.)
Yes, but:The authors note that some provisions of the AI Act, including explainability, oversight (deference to human control), and corrigibility (whether an AI system can be altered to change harmful outputs, which bears on a model’s risk classification under the AI Act), are defined ambiguously under the law and can’t be measured reliably at present. These areas are under-explored in the research literature and lack benchmarks to assess them.
Why it matters:With the advent of laws that regulate AI technology, developers are responsible for assessing a model’s compliance before they release it or use it in ways that affect the public. COMPL-AI takes a first step toward assuring model builders that their work is legally defensible or else alerting them to flaws that could lead to legal risk if they’re not addressed prior to release.
We’re thinking:Thoughtful regulation of AI is necessary, but it should be done in ways that don’t impose an undue burden on developers. While the AI Act itself is overly burdensome, we’re glad to see a largely automated path to demonstrating compliance of large language models.
",['https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--26-.gif']
Disembodied Spirits Speak,"Listen! Did you hear a rasping whisper say, “Beware”? Was it a rogue superintelligence? Or just a deepfake? We don’t know, but we heard it, too. It warns of machine learning algorithms that would devour electricity to leave us shivering in the cold night air, mislead us with increasingly inaccurate output, and take over the work that gives our lives meaning. In this special issue ofThe Batch, as inprioryearsatthisseason, we face our fears of AI. Stay close to your laptop’s screen. It may be the only light amid the growing darkness.
",['https://dl-staging-website.ghost.io/content/images/2024/10/LastWood-byFirelight8_1200px.jpg']
AI Burns All the Energy,"The globe’s growing AI infrastructure requires huge amounts of electricity, possibly more than power providers can generate responsibly. Could AI models suck energy resources dry?
The fear:Demand for AI is skyrocketing, and with it the demand for energy to fuel training and inference. Power-hungry systems will overwhelm our current power sources. If unchecked, they could lead to energy shortages and runaway carbon emissions.
Horror stories:AI companies don’t disclose the percentage of their energy needs that AI consumes, but top companies, led by OpenAI, havepitchedthe U.S. government to build out new energy sources and infrastructure. The trend is clear: Escalating demand risks tapping out existing power plants, pushing carbon emissions higher, and delaying moves to more sustainable energy sources.
How scared should you be:The rapid growth of AI poses a sharp dilemma: How can we meet demand without releasing greater and greater amounts of heat-trapping greenhouse gasses into the atmosphere? AI companies’ two-pronged strategy of lobbying governments and investing in carbon-free energy resources suggests the problem requires both short- and long-term approaches.
Facing the fear:WhileAI poses a difficult problem for the world’s energy consumption, it’s also an important part of the solution. Learning algorithms arereducingenergy consumption andmanagingdistribution. They can helpcapture and storecarbon dioxide from energy plants and manufacturers before it reaches the atmosphere. AI is also helping to monitor the atmosphere, oceans, and forests so we canunderstandthe impacts of climate change and make policy accordingly. And processing in centralized data centers — as power-hungry as they are — is far more energy-efficient than using local servers or edge devices. Ongoing AI development will make such efforts more effective and help us build a more sustainable future.
",['https://dl-staging-website.ghost.io/content/images/2024/10/BestCostumes2_1200px--1--1.jpg']
Innovation Can’t Win,"Politicians and pundits have conjured visions of doom to convince lawmakers to clamp down on AI. What if terrified legislators choke off innovation in AI?
The fear:Laws and treaties that purportedly were intended to prevent harms wrought by AI are making developing new models legally risky and prohibitively expensive. Without room to experiment, AI’s benefits will be strangled by red tape.
Horror stories:At least one law that would have damaged AI innovation and open source has been blocked, but another is already limiting access to technology and raising costs for companies, developers, and users worldwide. More such efforts likely are underway.
How scared should you be:The veto of SB 1047 was a narrow escape for California and companies and labs that operate there. Yet regulations like the AI Act are poised to reshape how AI is trained and used worldwide. Historysuggeststhat restrictive laws often lead to more caution and less experimentation from technologists.
Facing the fear:AI needs thoughtful regulation to empower developers to help build a better world, avoid harms, and keep learning. But effective regulation of AI requires restrictingapplications, not the underlying technology that enables them. Policymakers should align with a wide range of developers – not just a few that have deep pockets – to address harmful applications without stifling broader progress.
",['https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--29-.jpg']
No Work for Coders,"AI coding assistants are brewing codebases that once were the sole province of human programmers. Will AI systems take over software development?
The fear:Programming jobs will vanish as tireless AI agents plan, write, debug, and document code as well as or better than humans. Software engineers will find themselves wandering the job market like restless spirits.
Horror stories:Since 2020, AI-powered coding tools have advanced from completing individual lines of code to generating complex programs. More and more coders work with an automated assistant. These tools are poised to take over more and more of the development cycle as they evolve.
How scared should you be:Nvidia CEO Jensen Huangpredictedthat AI would make “everybody in the world [a] computer programmer,” while observersfretthat Copilot erodes problem-solving skills. But the reality is more nuanced. Researchshowsthat automation is likely to perform certain coding tasks but not entire programming jobs. These tools excel at routine tasks and boilerplate code, but they amplify rather than automate the developer's core skills. Conceptual tasks like specifying what a program should do, collaborating with colleagues, and translating business needs into software design remain the domain of human coders — for now.
Facing the fear:Developers have more to gain by embracing AI assistants than fearing them. These tools don’t just automate tasks; they accelerate learning, refine problem-solving, and enhance programming skills. Developers who master both coding fundamentals and AI assistance won’t just survive — they’ll thrive!
",['https://dl-staging-website.ghost.io/content/images/2024/11/HalloweenQuiz-4b_1200px-1.jpg']
Benchmark Tests Are Meaningless,"Large language models are trained on datasets scraped from the web, which includes pages that contain answers to common questions that are used to test the models. How can we evaluate them if they’ve studied the answers before we give them the test?
The fear:Machine learning research marks progress based on trained models’ responses to benchmark problems they didn’t encounter during training. But the solutions to many problems used to evaluate large language models have made their way into popular training datasets, making it impossible to verify progress in precise ways. The state of the art is an illusion and researchers are shooting in the dark.
Horror stories:Researchers have found disturbing signs that the test sets of many widely used benchmarks have leaked into training sets.
How scared should you be:Leakage of benchmark test sets into training sets is a serious problem with far-reaching implications. One observerlikenedthe current situation to an academic examination in which students gain access to questions and answers ahead of time — scores are rising, but not because the students have learned anything. If training datasets are contaminated with benchmark tests, it’s impossible to know whether apparent advances represent real progress.
Facing the fear:Contamination appears to be widespread but it can be addressed. One approach is to embedcanary strings— unique markers within test datasets like BIG-bench — that enable researchers to detect contamination by checking whether a model can reproduce them. Another is to continuallyenhancebenchmarks with new, tougher problems. Of course, researchers can devise new benchmarks, but eventually copies will appear on the web. Alternatively, they can keep new benchmarks under wraps and run them only onprivate servers.
",['https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--30-.jpg']
AI Giants Go Nuclear,"Major AI companies plan to meet the growing demand with nuclear energy.
What’s new:Amazon, Google, and Microsoftannouncedsubstantial investments in nuclear power projects. Amazon and Google forged partnerships to build a new generation of small reactors, while Microsoft cut a deal to revive a shuttered nuclear plant. (Andrew Ng is a member of Amazon’s board of directors.)
How it works:Nuclear powerprovidesaround 18 percent of electricity in the United States and more in France and several other European countries. Its steady generating capacity and zero carbon emissions (after plant construction) make it an attractive way to power AI infrastructure. However, new nuclear plants have been difficult to build in the U.S. since a string of high-profile accidents at Three Mile Island in the U.S. (1979), Chernobyl in Ukraine (1986), and Fukishima in Japan (2011). Since then, pressure to reduce carbon emissions has driven calls to build new plants. In March, President Bidensignedlegislation that streamlines construction and regulation of nuclear plants.
Behind the news:The tech industry’s growing interest in nuclear power is driven by surging demand for AI and corporate commitments to reduce carbon emissions. Data centers that train and run AI models consume vast amounts of electricity, and nuclear energy offers a reliable, carbon-free source. Microsoft, Nvidia, and OpenAI haveurgedthe White House to deliver a so-called “energy New Deal” that would allocate hundreds of billions of dollars to subsidize new power plants.
Why it matters:The fact that tech giants are investing directly in nuclear power plants indicates the high stakes of competition in AI. Economistsestimatethat data centers that process AI, among other workloads, will consume more than 1,000 terawatt-hours of electricity by 2026, more than double the amount they consumed in 2022. Nuclear power could give them bountiful, carbon-free energy for decades to come.
We’re thinking:Fossil fuels like coal do tremendous damage to the environment, while renewables like solar and wind energy can’t fully meet the always-on demands of AI infrastructure. Next-generation reactor designs that improve safety and reduce costs are worth exploring. However, a significant obstacle remains: Few countries have a certifiably safe repository for long-term disposal of highly radioactive spent fuel. U.S. efforts toward this goal arestalled.
",['https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--24-.jpg']
AI Bromance Turns Turbulent,"Once hailed by OpenAI chief Sam Altman as the “best bromance in tech,” the partnership between Microsoft and OpenAI is facing challenges as both companies seek greater independence.
What’s new:Sources inside Microsoft and OpenAIrevealedthat both companies are working to reduce their reliance on the other, according toThe New York Times. Their collaboration, which brought both companies great rewards, is now complicated by demands for resources, friction between leaders, and partnerships with other companies.
How it works:In a series of deals that started in 2019, Microsoftinvesteda total of $13 billion in OpenAI, giving the startup access to Microsoft’s processing infrastructure and Microsoft special access to OpenAI’s models (which it integrated into its own applications), a large cut of its revenue, and potential equity. Microsoftbuilta 10,000-GPU system on Azure for training OpenAI models. But OpenAI sought to renegotiate its agreements, while Microsoft continued to develop its own AI capabilities.
Behind the news:OpenAI’s valuationsoaredto $157 billion with new funding from Nvidia and other investors following a period of mounting financialpressure. The increased valuation gives OpenAI new power in its relationship with Microsoft. Moreover Microsoft holds no seats on its nonprofit board of directors, which limits its influence over strategic decisions at OpenAI despite its significant financial stake in the startup’s for-profit wing.
Why it matters:The Microsoft-OpenAI partnership has reshaped the AI landscape, and shifts in their partnership have an outsized impact on a wide range of research and product development. Their evolving relationship illustrates the challenge of sustaining a close collaboration amid rapidly changing technology. Microsoft provided vital resources that helped OpenAI scale up, while OpenAI’s models enabled Microsoft to keep rivals off-balance as it reinvented products including Bing, Windows, Office, Azure, and its expanding line of Copilots. However, facing fierce competition, both companies need ample flexibility to innovate and adapt.
We’re thinking:Together and separately, Microsoft and OpenAI have done tremendous work to advance the field from research to applications. We hope they can strike a balance that maintains their partnership and fuels their growth.
",['https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--21-.gif']
Mistral AI Sharpens the Edge,"Mistral AI launched two models that raise the bar for language models with 8 billion or fewer parameters, small enough to run on many edge devices.
What’s new:Ministral 3B and Ministral 8B, which come in base and instruction-tuned versions, outperform Google’s and Meta’s similar-sized models on several measures of knowledge retrieval, common-sense reasoning, and multilingual understanding. Ministral 8B-Instruct is free todownloadand use for noncommercial purposes, and commercial licenses are negotiable for this model and the others in the family. Accessed via Mistral’s APIs, Ministral 3B costs $0.04 per million tokens of input and output, and Ministral 8B costs $0.10 per million tokens of input and output.
How it works:The Ministral family can process 131,072 tokens of input context. The models are built to support function calling natively to interact, for example, with external APIs that fetch real-time weather data or control smart-home devices.
Behind the news:Headquartered in France, Mistral AI competes head-to-head in AI with U.S. tech giants. It released its first model, Mistral 7B, a year ago under an Apache open source license. Since then, it has released model weights under a range of licenses while exploring alternative architectures such as mixture-of-experts and mamba. It also offers closedmodelsthat are larger and/or built for specialized tasks like code generation and image processing.
Why it matters:Edge devices can play a crucial role in applications that require fast response, high privacy and security, and/or operation in the absence of internet connectivity. This is particularly important for autonomous and smart home devices where uninterrupted, rapid processing is critical. In addition, smaller models like Ministral 8B-Instruct enable developers and hobbyists to run advanced AI on consumer-grade hardware, lowering costs and broadening access to the technology.
We’re thinking:Mistral’s new models underscore the growing relevance of edge computing to AI’s future. They could prove to be affordable and adaptable alternatives to Apple and Google’s built-in models on smartphones and laptops.
",['https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--22-.gif']
Malaysia’s Data Center Boom,"Malaysia’s location, natural resources, and investor-friendly government are perfect for data centers, turning part of the country into an AI-fueled boomtown.
What’s new:Data center construction is flourishing in the southern Malaysian state of Johor, where companies including ByteDance and Microsoft are spending billions of dollars on facilities,The Wall Street Journalreported. These data centers will provide processing power for AI, cloud computing, and telecommunications.
How it works:Data center construction has slowed in established areas like Ireland and Northern Virginia as space and resources have become scarce. All regions face shortages of electrical power, analystssay, and some U.S. locations face publicresistanceto new projects. Johor has emerged as an attractive alternative.
Behind the news:The Asia-Pacific region is second to North America in data center construction, according to one recentreport, ahead of Europe, South America, and the Middle East and Africa. As Johor builds out its data-center inventory, it will compete with established Asia-Pacificmarketsin Hong Kong, Mumbai, Seoul, Singapore, Sydney, and Tokyo.
Why it matters:AI is poised to transform virtually every industry, but doing so requires ample processing power. The data-center buildout will help fuel improvements in AI as well as spread the technology to new industries and bring its benefits to people throughout the world. Malaysia’s role as a data center hub is also bound to bring huge economic benefits to the country itself.
We’re thinking:Many data centers have been built near users to reduce latency. But the cost of processing compute-intensive AI workloads is so high relative to the cost of transmitting data that it makes sense to transmit AI-related data long distances for processing. (As Andrew wrote, thegravity of data is decreasing.) We hope the increasing flexibility in siting data centers will enable more nations that aren’t traditional tech hubs toparticipate in the tech economyand reap significant benefits from doing so.
",['https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--20-.png']
"U.S. Cracks Down on AI Apps That Overpromise, Underdeliver","The United States government launched Operation AI Comply, targeting businesses whose uses of AI allegedly misled customers.
What’s new:The Federal Trade Commission (FTC)took actionagainst five businesses for allegedly using or selling AI technology in deceptive ways. Two companies settled with the agency, while three face ongoing lawsuits.
How it works:The FTC filed complaints against the companies based on existing laws and rules against unfair or deceptive commercial practices. The FTC alleges:
Behind the news:The FTC has a broad mandate to protect consumers, including both deceptive and anticompetitive business practices. In June, itagreedto focus on Microsoft’s investment in OpenAI and Google’s and Amazon’s investments in Anthropic, while the U.S. Department of Justice would examine Nvidia’s dominant market share in chips designed to process AI workloads. The FTC previously brought cases againstRite Aidfor misuse of AI-enabled facial recognition,Everalbumfor deceptive use of facial recognition, andCRI Genetics, which misled consumers while using AI to conduct DNA tests.
Why it matters:The FTC’s enforcement actions send a message to businesses that aim to take advantage of the latest AI models: making exaggerated claims about AI will bring legal consequences. The complaints point to a set of issues: falsely claiming to use AI to provide a particular service, exaggerating AI’s ability to replace human expertise, generating fake reviews of businesses, promising unrealistic financial returns, and failing to disclose crucial information about AI-based services.
We’re thinking:These particular actions crack down not on AIper sebut on companies that allegedly deceived consumers. By taking scams off the market while leaving legitimate businesses to operate freely, they may actually increase customer trust in AI.
",['https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--19-.gif']
A Year of Contending Forces,"A new report documents the interplay of powerful forces that drove AI over the past year: open versus proprietary technology, public versus private financing, innovation versus caution.
What’s new:Drawn from research papers, news articles, earnings reports, and the like, the seventh annualState of AI Reportrecaps the highlights of 2024.
Looking back:AI’s rapid advance in 2024 was marked by groundbreaking research, a surge of investment, international regulations, and a shift in safety concerns from hypothetical risks to real-world issues, according to investors Nathan Benaich and Ian Hogarth.
Looking forward:The authors reviewed predictions they made in last year’sreport— among them, regulators would investigate the Microsoft/OpenAI Partnership (accurate), and a model builder would spend over $1 billion on training (not yet) — and forecast key developments in 2025:
Why it matters:The authors examined AI from the point of view of investors, keen to spot shifts and trends that will play out in significant ways. Their report dives deep into the year’s research findings as well as business deals and political currents, making for a well rounded snapshot of AI at the dawn of a new year.
We’re thinking:The authors are bold enough to make clear predictions and self-critical enough to evaluate their own accuracy one year later. We appreciate their principled approach!
",['https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--20--1.gif']
"Familiar Faces, Synthetic Soundtracks","Meta upped the ante for text-to-video generation with new systems that produce consistent characters and matching soundtracks.
What’s new:Meta presentedMovie Gen, a series of four systems that generate videos, include consistent characters, alter generated imagery, and add matching sound effects and music. Movie Gen will beavailableon Instagram in 2025. Meanwhile, you can view and listen to exampleshere. The teamexplainshow the model was built an extensive 92-page paper.
Generated videos:Movie Gen Video can output 256 frames (up to 16 seconds at 16 frames per second) at 1920x1080-pixel resolution. It includes a convolutional neural network autoencoder, transformer, and multiple embedding models.
Consistent characters:Given an image of a face, a fine-tuned version of Movie Gen Video generates a video that depicts a person with that face.
Altered clips:The team modified Movie Gen Video’s autoencoder to accept an embedding of an alteration — say, changing the background or adding an object. They trained the system to alter videos in three stages:
Synthetic soundtracks:Given a text description, a system called Movie Gen Audio generates sound effects and instrumental music for video clips up to 30 seconds long. It includes aDACVAEaudio encoder (which encodes sounds that comes before and/or after the target audio), Long-prompt MetaCLIP video encoder,T5text encoder, vanilla neural network that encodes the current time step, and transformer.
Results:Overall, Movie Gen achieved performance roughly equal to or better than competitors in qualitative evaluations of overall quality and a number of specific qualities (such as “realness”). Human evaluators rated their preferences for Movie Gen or a competitor. The team reported the results in terms of net win rate (win percentage minus loss percentage) between -100 percent and 100 percent, where a score above zero means that a system won more than it lost.
Why it matters:With Movie Gen, table stakes for video generation rises to include consistent characters, soundtracks, and various video-to-video alterations. The 92-page paper is a valuable resource for builders of video generation systems, explaining in detail how the team filtered data, structured models, and trained them to achieve good results.
We’re thinking:Meta has a great track record of publishing both model weights and papers that describe how the models were built. Kudos to the Movie Gen team for publishing the details of this work!
",['https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--18-.gif']
Voice-to-Voice and More for GPT-4o API,"OpenAI launched a suite of new and updated tools to help AI developers build applications and reduce costs.
What’s new:At its annual DevDay conference, OpenAI introduced anAPIfor speech processing using GPT-4o,distillation tools,vision fine-tuning capabilities, and the ability tocache promptsfor later re-use. These tools are designed to make it easier to build fast applications using audio inputs and outputs, customize models, and cut costs for common tasks.
Development simplified:The new offerings aim to make it easier to build applications using OpenAI models, with an emphasis on voice input/output and image input, customizing models, and resolving common pain points.
Behind the news:OpenAI is undertaking a major corporate transformation. A recent funding roundvaluesOpenAI at $157 billion, making it among the world’s most valuable private companies, and the company istransferringmore control from its nonprofit board to its for-profit subsidiary. Meanwhile, it has seen anexodusof executives that include CTO Mira Murati, Sora co-lead Tim Brooks, chief research officer Bob McGrew, research VP Barret Zoph, andother key researchers.
Why it matters:The Realtime API enables speech input and output without converting speech to text, allowing for more natural voice interactions. Such interactions open a wide range of applications, and they’re crucial for real-time systems like customer service bots and virtual assistants. AlthoughAmazon Web ServiceandLabelboxprovide services to distill knowledge from OpenAI models into open architectures, OpenAI’s tools ease the process of distilling from OpenAI models into other OpenAI models. Image fine-tuning and prompt caching, like similar capabilities for Anthropic Claude and Google Gemini, are welcome additions.
We’re thinking:OpenAI’s offerings have come a long way sinceDevDay 2023, when speech recognition was “coming soon.” We’re eager to see what developers do with voice-driven applications!
",['https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--16-.png']
German Court: LAION Didn’t Violate Copyrights,"A German court dismissed a copyright lawsuit against LAION, the nonprofit responsible for large-scale image datasets used to train Midjourney, Stable Diffusion, and other image generators.
What’s new:The courtrejecteda lawsuit claiming that cataloging images on the web to train machine learning models violates the image owners’ copyrights. It ruled that LAION’s activities fall under protections for scientific research.
How it works:LAION doesn’t distribute images. Instead, it compiles links to images and related text that are published on publicly available websites. Model builders who wish to use the images and/or text must download them from those sources. In 2023, photographer Robert KneschkesuedLAION for including his photos. The court’sdecisionemphasized several key points.
Behind the news:Several other artists have suedLAION, which stands for Large-scale AI Open Network, claiming that the organization used their works without their consent. They have also sued AI companies, including aclass action suitagainst Stability AI, Midjourney, and DeviantArt for using materials under copyright, including images in LAION’s datasets, to train their models. Similar cases have been brought against makers ofmusic generatorsandcoding assistants. All these lawsuits, which are in progress, rest on the plaintiff’s claim that assembling a training dataset of copyrighted works infringes copyrights.
Why it matters:The German ruling is the first AI-related decision in Europe since the adoption of the AI Act, and the court took that law’s intent into account when making its decision. It affirms that creating text-image pairs of publicly available material for the purpose of training machine learning models does not violate copyrights, even if commercial organizations later use the data. However, the court did not address whether training AI models on such datasets, or using the trained models in a commercial setting, violates copyrights.
We’re thinking:This decision is encouraging news for AI researchers. We hope jurisdictions worldwide establish that training models on media that’s available on the open web is fair and legal.
",['https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--17-.png']
Llama Herd Expands,"Meta extended its Llama family of models into two new categories: vision-language and sizes that are small enough to fit in edge devices.
What’s new:Meta introducedLlama 3.2, including two larger vision-language models and two smaller text-only models as well as developer tools for building agentic applications based on the new models.Weights and codearefreeto developers who have less than 700 million monthly active users. Multiple providers offer cloud access.
How it works:Llama 3.2 90B and 11B accept images as well as text and generate text output (image processing is not available in the European Union). Llama 3.2 1B and 3B accept and generate text. All four models can process 131,072 tokens of input context and generate 2,048 tokens of output.
New tools for developers:Meta announcedLlama Stack, a series of APIs for customizing Llama models and building Llama-based agentic applications. Among other services, Llama Stack has APIs for tool use, memory, post-training, and evaluation.Llama Guard, a model designed to evaluate content for sexual themes, violence, criminal planning, and other issues, now flags problematic images as well as text. Llama Guard 3 11B Vision comes with Llama.com’s distributions of Llama 3.2 90B and 11B, while Llama Guard 3 1B comes with Llama 3.2 3B and 1B.
Why it matters:Meta’s open models are widelyusedby everyone from hobbyists to major industry players. Llama 3.2 extends the line in valuable ways. The growing competition between Llama and Qwen shows that smaller, open models can offer multimodal capabilities that are beginning to rival their larger, proprietary counterparts.
We’re thinking:By offering tools to buildagentic workflows, Llama Stack takes Llama 3.2 well beyond the models themselves. Our new short course “Introducing Multimodal Llama 3.2” shows you how to put these models to use.
",['https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--15-.gif']
Generative Video in the Editing Suite,"Adobe is putting a video generator directly into its popular video editing application.
What’s new:Adobeannouncedits Firefly Video Model, which will be available as a web service and integrated into the company’s Premiere Pro software later this year. The model takes around two minutes to generate video clips up to five seconds long from a text prompt or still image, and it can modify or extend existing videos. Prospective users can join awaitlistfor access.
How it works:Adobe has yet to publish details about the model’s size, architecture, or training. It touts uses such as generating B-roll footage, creating scenes from individual frames, adding text and effects, animation, and video-to-video generation like extending existing clips by up to two seconds.
Behind the news:Adobe’s move into video generation builds on itsFirefly image generatorand reflects its broader strategy to integrate generative AI with creative tools. In April, Adobeannouncedthat it would integrate multiple video generators with Premiere, including models from partners likeOpenAIandRunway. Runway itself recently extended its own offering withvideo-to-videogeneration and anAPI.
Why it matters:Adobe is betting that AI-generated video will augment rather than replace professional filmmakers and editors. Putting a full-fledged generative model in a time-tested user interface for video editing promises to make video generation more useful as well as an integral part of the creative process. Moreover, Adobe’s use of licensed training data may attract videographers who are concerned about violating copyrights or supporting fellow artists.
We’re thinking:Video-to-video generation crossing from frontier capability to common feature. Firefly's (and Runway’s) ability to extend existing videos offers a glimpse.
",['https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--14--1.png']
International Guidelines for Military AI,"Dozens of countries endorsed a “blueprint for action” designed to guide the use of artificial intelligence in military applications.
What’s new:More than 60 countries including Australia, Japan, the United Kingdom, and the United States endorsed nonbinding guidelines for military use of AI,Reutersreported. The document, presented at the Responsible Artificial Intelligence in the Military (REAIM) summit in Seoul, South Korea, stresses the need for human control, thorough risk assessments, and safeguards against using AI to develop weapons of mass destruction. China and roughly 30 other countries did not sign.
How it works:Key agreements in theblueprintinclude commitments to ensure that AI doesn’t threaten peace and stability, violate human rights, evade human control, and hamper other global initiatives regarding military technology.
Behind the News:The Seoul summit followed last year’sREAIM summitin The Hague, where leaders similarly called for limits on AI military use without binding commitments. Other international agreements like the EU’sAI ActandFramework Convention on Artificial Intelligence and Human Rights, Democracy, and the Rule of Lawregulate civilian AI, but exclude military applications. Meanwhile, AI-enabled targeting systems and autonomous, weaponized drones have been used in conflicts inSomalia, Ukraine, and Israel, highlighting the lack of international norms and controls.
Why it matters:The REAIM blueprint may guide international discussions on the ethical use of AI in defense, providing a foundation for further talks at forums like the United Nations. Though it’s nonbinding, it fosters collaboration and avoids restrictive mandates that could cause countries to disengage.
We’re thinking:AI has numerous military applications across not only combat but also intelligence, logistics, medicine, humanitarian assistance, and other areas. Nonetheless, it would be irresponsible to permit unfettered use of AI in military applications. Standards developed by democratic countries working together will help protect human rights.
",['https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--16-.gif']
California Restricts Deepfakes,"California, a jurisdiction that often influences legislators worldwide, passed a slew of new laws that regulate deepfakes.
What’s new:California Governor Gavin Newsom signed into law eight bills that aim to curb the use of generative AI inpoliticsandentertainment.
How it works:The legislation prohibits deceptive AI-generated media in political campaigns; requires permission for using digital stand-ins for actors, musicians, and other entertainers; and criminalizes generation of sexually explicit imagery without the subject’s consent.
Behind the news:Newsom has not yet acted on Senate Bill 1047, acontroversiallaw that would impose significant burdens on AI model developers. He hasexpressedthat the bill could interfere with innovation, especially with respect to open source projects.
Why it matters:Laws passed in California often point the way for legislators in other U.S. states, the federal government, and consequently other countries. The new laws that regulate deepfakes in political campaigns fill a gap left by the Federal Election Commission (FEC), which hassaidit lacks authority to regulate the use of AI in political ads. Meanwhile, the Federal Communications Commission (FCC)proposedrules that would mandate disclosure of uses of AI in political ads but has yet to implement them.
We’re thinking:We’re glad to see California target undesirable applications rather than AI models.Regulating applicationsrather than general-purpose technology that has a wide variety of uses — many of which are beneficial — avoids the dangers of California SB-1047, which is still awaiting the governor’s signature or veto. That law, which seeks to restrict AI models, wouldendangerinnovation and especially open source.
",['https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--11-.gif']
"More, Better Open Source Options","The parade of ever more capable LLMs continues with Qwen 2.5.
What’s new:Alibaba releasedQwen 2.5in several sizes, the API variants Qwen Plus and Qwen Turbo, and the specialized modelsQwen 2.5-Coder and Qwen 2.5-Coder-InstructandQwen 2.5-Math and Qwen 2.5-Math-Instruct. Many are freely available for commercial use under the Apache 2.0 licensehere. The 3B and 72B models are also free, but theirlicenserequires special arrangements for commercial use.
How it works:The Qwen 2.5 family ranges from 500 million parameters to 72 billion parameters.
Results:Compared to other models with open weights, Qwen 2.5-72B-Instruct beats LLama 3.1 405B Instruct and Mistral Large 2 Instruct (123 billion parameters) on seven of 14 benchmarks includingLiveCodeBench,MATH(solving math word problems), andMMLU(answering questions on a variety of topics). Compared to other models that respond to API calls, Qwen-Plus beats LLama 3.1 405B, Claude 3.5 Sonnet, and GPT-4o on MATH, LiveCodeBench, andArenaHard. Smaller versions also deliver outstanding performance. For instance, Qwen 2.5-14B-Instruct outperforms Gemma 2 27B Instruct and GPT-4o mini on seven benchmarks.
Behind the news:Qwen 2.5 extends a parade of ever more capable LLMs that include Claude 3.5 Sonnet, GPT-4o, and LLama 3.1 as well as the earlierQwen 2 family.
Why it matters:The new models raise the bar for open weights models of similar sizes. They also rival some proprietary models, offering options to users who seek to balance performance and cost.
We’re thinking:Some companies encourage developers to use their paid APIs by locking their LLMs behind non-commercial licenses or blocking commercial applications beyond a certain threshold of revenue. We applaud Qwen’s approach, which keeps most models in the family open.
",['https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--12-.gif']
Hollywood Embraces Video Generation,"The AI startup Runway is helping to retool Lionsgate, the producer of blockbuster movie franchises likeThe Hunger GamesandJohn Wick, for the era of generated video.
What’s new:Runway willbuilda custom video generator to help Lionsgate streamline its production processes. It alsolaunchedan API for its Gen-3 Alpha Turbo model.
Runway + Lionsgate:Runway will fine-tune its proprietary models on Lionsgate productions to enable the filmmaker to generate new imagery based on its previous work. The companies didn’t disclose financial terms of the arrangement.
Gen-3 API:Concurrently with announcing the Lionsgate deal, Runwayunveiledan API that drives its Gen-3 Alpha and Gen-3 Alpha Turbo models as well as updates to Gen-3 Alpha.
Why it matters:Although the plan is to use Runway’s technology for pre- and post-production, this deal puts state-of-the-art video generation at the heart of Lionsgate’s operations and encourages professional cinematographers, editors, special effects artists, and other cinematic specialists to see what they can do with it. For Lionsgate, it’s a bid to stay ahead of competitors. For AI, it could be a major move into the Hollywood spotlight.
We’re thinking:While upstart competitors are using pretrained models, Lionsgate will be using a model that has internalized its own style and capabilities.
",['https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--13-.gif']
OpenAI o1 Forges Chains of Thought,"Preliminary versions of OpenAI’s new model family were trained explicitly to think step-by-step, yielding outstanding marks in math, science, and coding — but users can’t see their reasoning steps.
What’s new:OpenAI launched beta versions ofo1-preview and o1-mini, language models that were trained via reinforcement learning to use chains of thought. The models are available to paid ChatGPT users as well as API customers who have been onboard for more than 30 days and spent $1,000. o1-preview costs $15/$60 per million input/output tokens, significantly higher than GPT-4o’s price of $5/$15. o1-mini costs $3/$12 per million input/output tokens. OpenAI didn’t announce a release date for a finished o1 model.
How it works:o1-preview is a preliminary release, and o1-mini is a faster preliminary version that’s particularly effective at coding. OpenAI published ano1 system cardbut hasn’t disclosed details about the new models’ size, architecture, or training. Both models have an input context window of 128,000 tokens. They accept only text tokens, but OpenAI plans to support other media types in future versions.
Results:The actual o1 model — which remains unavailable — generallyoutperformso1-preview, while both vastly outperform GPT-4o on math, science, and coding benchmarks.
Behind the news:In recent months, Anthropic has been using the tag <antThinking> to generate thinking tokens that are hidden from users. However, OpenAI’s implementation in the o1 models takes this capability much further.
Why it matters:The o1 models show that the combination of reinforcement learning and chain-of-thought reasoning can solve problems that large language models generally find challenging. They’re substantially more accurate in domains such as coding, math, and science that have low tolerance for error. However, the fact that the models hide their reasoning from users makes them less transparent and explainable than their predecessors and may make their outstanding performance less valuable in some applications.
We’re thinking:Agentic workflows can significantly improve a system’s ability to reflect, reason, and iterate on its output. Training a model to take such steps directly in response to even general-purpose questions opens an exciting alternative path to better reasoning beyond simply scaling up model size.
",['https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--9-.gif']
High Gear for Llama 3.1 405B,"SambaNova raised the speed limit for access to the largest model in the Llama 3.1 family — and it’s free.
What’s new:SambaNovalauncheda cloud service that runs Llama 3.1 405B significantly faster than competitors. A free tier is available, to be followed later this year by paid tiers that offer higher rate limits.
How it works:SambaNova uses proprietarychipsand software to accelerate model inference.
Yes, but:SambaNova currently limits Llama 3.1’s context window to around 8,000 tokens, much less than the model’s native 128,000 tokens.
Behind the news:The new service arrives amid a broader competition to deliver fast inference among cloud providers that have developed their own specialized chips. Competitors likeCerebrasandGroqhave introduced their own high-speed inference services.
Why it matters:Throughput, cost, performance, and latency are critical factors in practical applications of AI models. Fast inference allows for more frequent API calls without bogging down time to output, which is essential for agentic workflows and real-time decision making.
We’re thinking:Models with open weights are now served faster than proprietary models and are nearly as capable. This may spur further adoption of open models as well as prompting strategies, such as agentic workflows, that require large numbers of output tokens.
",['https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--16-.jpg']
Amazon Boosted by Covariant,"Amazon took on talent and technology from robotics startup Covariant to enhance its warehouse automation, an area critical to its core ecommerce business.
What’s new:Amazon announced anagreementto hire Covariant’s cofounders and other key personnel and license its models. Financial terms were not disclosed. (Disclosure: Andrew Ng is a member of Amazon’s board of directors.)
How it works:The new deal echoes Amazon’s previous not-quite acquisition of Adept as well as similar arrangements between other tech giants and startups.
Behind the news:Amazon has been working to acquire technical talent and technology for some time. In 2022, it announced that it would acquire iRobot, but the companiesabandonedthat plan earlier this year after EU regulators blocked the deal citing antitrust concerns. In October, itcommittedto invest as much as $4 billion in Anthropic in return for access to the startup’s technology. (UK regulatory authorities subsequentlyannouncedan antitrust probe into Amazon’s relationship with Anthropic.) In July, itsigneda hire-and-license deal — similar to its agreement with Covariant — with agentic AI startup Adept.
Why it matters:Competition among AI giants continues to heat up. Amazon’s agreement with Covariant mirrors other deals in which a tech giant gained top talent and technology without formally acquiring a startup, including Microsoft’sarrangementwith Inflection and Google’sdealwith Character.AI. These developments highlight top tech companies’ race to secure their AI positions — and the fact that outright acquisitions invite regulatory scrutiny.
We’re thinking:Robotic foundation models that are trained on large amounts of unlabeled robotics data offer a promising way to quickly fine-tune robots to perform new tasks — potentially a major upgrade in warehouse logistics.
",['https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--12-.png']
Waymo Spotlights Safety Record,"Waymo, the autonomous vehicle division of Alphabet, released ananalysisof its own safety data. It suggests that the company’s self-driving cars are safer than human drivers on the same roads.
What’s new:Waymo’s analysis claims that its robotaxis, compared to human-driven vehicles, were involved in proportionally fewer accidents that involved police reports, passenger injuries, or airbag deployment. The company argues that these types of incidents are more relevant to assessing safety than minor collisions with no serious damage.
How it works:The study compares the number of incidents per mile experienced by Waymo vehicles and human drivers. It covers over 22 million miles driven along specific routes in Phoenix, Arizona, and San Francisco, California. The results were consistent in Phoenix and San Francisco.
Behind the news:Waymo’s study arrives amid ongoing scrutiny of autonomous vehicle safety, particularly in San Francisco, where accidents and traffic disruptions caused by self-driving cars have raisedpublic backlash and regulatory challenges. Earlier this year, the state of CaliforniabannedCruise, a Waymo competitor, after one of its self-driving cars drove over a pedestrian and dragged her about 20 feet before coming to a stop.
Why it matters:Waymo’s analysis implies that autonomous vehicles could significantly reduce road accidents and injuries. The data could help urban planners to craft policies that would integrate autonomous vehicles into existing transportation systems.
Yes, but:Waymo’s analysis is based on methods and benchmarks introduced in tworesearchpapersthat have not yet been peer reviewed. Validating them through peer review would help to establish the safety record of self-driving cars.
We’re thinking:This report makes a compelling case for autonomous vehicles. But the question remains whether these findings will be sufficient to increase public trust. We encourage other self-driving companies to release comprehensive safety data.
",['https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--7-.png']
2D-to-3D Goes Mainstream,"Traditionally, building 3D meshes for gaming, animation, product design, architecture, and the like has been labor-intensive. Now the ability to generate 3D meshes from a single image is widely available.
What’s new:Two companies launched systems that produce a 3D mesh from one image. Stability AI releasedSF3D. Itsweightsandcodeare freely available to users with annual revenue under $1 million. Meanwhile, Shutterstocklauncheda service that provides a similar capability.
How it works:Stability AI’s SF3D generates output in a half-second, while Shutterstock’s service takes around 10 seconds.
Behind the news:These releases arrived amid a flurry of recent works that aim to tackle similar problems. Most are based onLarge Reconstruction Model(LRM), proposed by Adobe in late 2023, which produces a 3D mesh and surface texture from a single image in less than 5 seconds. Follow-upworktrained LRM on real-world images in addition to the images of synthetic 3D meshes used in the original work and then reproduced LRM’s capabilities in anopen source model. Further research extended the model tolearn from generated videos. Stability AI’s new system addresses issues in its own previousworkthat was based on LRM.
Why it matters:SF3D replacesNeRF, a 2D-to-3D approach proposed in 2020 that serves as the basis for LRM and several other methods, with DMTet, which incorporates surface properties to achieve smoother meshes and better account for light reflecting off object surfaces.
We’re thinking:3D generation is advancing rapidly. To ignore this technology would be a mesh-take!
",['https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--8-.png']
Western Powers Sign AI Treaty,"The European Union, United Kingdom, United States, and other countriessigneda legally binding treaty that regulates artificial intelligence.
What’s new:The treaty, officially known as theFramework Convention on Artificial Intelligence and Human Rights, Democracy, and the Rule of Law, provides a legal framework for states to preserve democratic values while promoting AI innovation. It was negotiated by member nations of the Council of Europe (a transnational organization that promotes democracy and human rights and includes nearly twice as many countries as the EU) as well as observer states including Australia, Canada, and Mexico, which have not yet signed it. Countries that did not participate include China, India, Japan, and Russia.
How it works:The treaty will take effect later this year. It applies to any use of AI by signatories, private actors working on behalf of signatories, or actors in those jurisdictions. AI is broadly defined as any “machine-based system . . . [that generates] predictions, content, recommendations, or decisions that may influence physical or virtual environments.” The signatories agreed to do the following:
Exceptions:The treaty allows exceptions for national security and doesn’t cover military applications and national defense. It also doesn’t apply to research and development of AI systems that are not yet available for general use, unless testing such systems can interfere with human rights, democracy, or the rule of law.
Behind the news:The Council of Europe oversees the European Convention on Human Rights and its Court of Human Rights in Strasbourg, France. Its AI treaty builds on previous initiatives including the European Union'sAI Act, which aims to regulate AI based on risk categories, and other national and international efforts like the United States’AI Bill of Rightsand the globalAI Safety Summit.
Why it matters:As the first binding international agreement on AI, the treaty can be enforced by signatories’ own laws and regulations or by the European Court of Human Rights. Since so many AI companies are based in the U.S. and Europe, the treaty may influence corporate practices worldwide. Its provisions could shape the design of deployed AI systems.
Yes, but:Like any regulation, the treaty’s effectiveness depends on the interpretation of its high-level concepts. Its core terms (such as accountability measures, democratic processes, oversight, privacy rights, and transparency) represent a broad framework, but their precise meaning  is vague and interpretation is left to the signatories. Also, the nonparticipation of major AI powers like China and large countries like Russia and India raises questions about whether its standards can be applied globally.
We’re thinking:The EU and U.S. have very different approaches to AI regulation; the EU has taken a much heavier hand. Yet both agreed to the treaty. This could indicate that these regions are finding common ground, which could lead to more uniform regulations internationally.
",['https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--6-.gif']
Long Context Gets Up to Speed,"A new open weights model generates tokens faster than current transformers, especially when processing long inputs.What’s new:AI21 Labs releasedJamba 1.5, an update of its earlierJamba. It comes inMiniandLargeversions and boasts a relatively large (and validated) input context length of 256,000 tokens. The model weights arefreeto users who have annual recurring revenue under $50 million and available on several cloud platforms including Google Cloud Vertex AI, Hugging Face, and Microsoft Azure.How it works:Jamba 1.5 is a hybrid architecture made up of transformer,mamba, andmixture of experts(MoE) layers. Unlike transformer layers, in which processing power scales quadratically as input length increases, the mamba layers enable the required processing power to scale linearly as input length increases without requiring workarounds like sparse attention and sliding windows. The MoE layers are composed of many fully connected sublayers, of which only a small number are used to process a given input. Jamba 1.5 Mini has roughly 50 billion parameters but uses only 12 billion at a time, while Jamba 1.5 Large has around 400 billion parameters but uses only 94 billion at a time.
Results:Both versions of Jamba 1.5 produced output tokens faster than other models (running on identical hardware), especially given longer inputs. However, the larger version achieved lower performance on popular benchmarks than other open models.
Behind the news:The mamba architecture, which is designed to enable processing to scale linearly with longer input lengths, has been a subject of much research since its release in late 2023. Notably,Mamba-2,Mamba-2-Hybrid, andZambacombined mamba layers with attention layers with varying degrees of success.Why it matters:The originalMambamodel was much faster and equally accurate compared to transformers up to 2.8 billion parameters. But how the mamba architecture compared to transformers at larger scales was an open question. Jamba 1.5 shows that the combination of mamba and transformer layers can yield higher speed in larger models — although the results don’t yet exceed those of comparably sized transformers.We’re thinking:While hardware companies like Groq and SambaNova are accelerating LLMs, software innovations like Jamba may enable further speed-ups.
",['https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--3-.gif']
Models Ranked for Hallucinations,"How often do large language models make up information when they generate text based on a retrieved document? A study evaluated the tendency of popular models to hallucinate while performing retrieval-augmented generation (RAG).
What’s new:Galileo, which offers a platform for evaluating AI models,tested22 models to see whether they hallucinated after retrieving information from documents of various lengths. Claude 3.5 Sonnet was the overall winner, and most models performed best when retrieving information from medium-length documents.
How it works:The researchers tested 10 closed and 12 open models based on their sizes and popularity. They ran each model 20 times using short, medium, and long context lengths (a total of 60 tests) using GPT-4o to evaluate how closely the output text adhered to the context.
Results:Anthropic’s Claude 3.5 Sonnet ranked highest overall, achieving 0.97 in short context lengths and 1.0 in medium and long context lengths.
Behind the news:Galileo performed similartestslast year, when it compared performance in both RAG and non-RAG settings (without differentiating among context lengths). GPT-4 and GPT-3.5 held the top three spots in both settings despite strong showings by Llama 2 and Zephyr 7B. However, the top scores were lower (between 0.70 and 0.77).
Why it matters:Model builders have reduced hallucinations, but the difference between rare falsehoods and none at all may be critical in some applications.
We’re thinking:It’s curious that medium-length RAG contexts generally yielded fewer hallucinations than short or long. Maybe we should give models more context than we think they need.
",['https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--7-.jpg']
AI-Powered Policing Goes National,"Argentina created a national law-enforcement department that will use AI to detect crimes as they’re committed, investigate them afterward, and predict them before they occur.
What’s new:President Javier Milei of Argentina established the Artificial Intelligence Unit Applied to Security (UIAAS),The Registerreported. The unit aims to detect, investigate, and predict criminal activity by using machine learning algorithms to monitor the internet, wireless communications, security cameras, drone surveillance, financial transactions, and other data in real time.
How it works:Milei established the UIAAS in a late-Julyresolution. Milei created it under the Ministry of Security shortly after hereorganizedthe national intelligence agency to give himself more direct control. In December, his security ministerquashedpublic protests against his austerity policies; he promised to identify protesters via “video, digital, or manual means” and bill them for the cost of policing the demonstrations.
Behind the news:Argentina’s government is a presidential representative democratic republic. The country was ruled by a military dictatorship between 1976 and 1983.
Why it matters:AI has valuable uses in law enforcement and security. At the same time, it needs to be applied responsibly and implemented in a way that’s fair and respectful of legal rights such as presumption of innocence.
We’re thinking:Surveillance is easy to abuse, and the notion of predictive policing warrants extreme caution to avoid bias against certain groups, violating civil rights, and other pitfalls. Ensuring that it’s used well requires robust technology, rigid controls, clear oversight, and public transparency. We hope that Argentina — no less than the countries that inspired it establish a national AI police agency — will put strong safeguards in place.
",['https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--4-.gif']
A Lost Voice Regained,"A man who lost the ability to speak four years ago is sounding like his earlier self, thanks to a collection of brain implants and machine learning models.
What’s new:Researchers built a system thatdecodes speech signals from the brainof a man who lost the ability to speak clearly due to amyotrophic lateral sclerosis, also known as ALS, and enables him to speak through a synthetic version of his former voice. At the start of the study, his efforts to speak were intelligible only to his personal caregiver. Now he converses regularly with family and friends,The New York Timesreported. Nicholas Card built the system with colleagues University of California-Davis, Stanford University, Washington University, Brown University, VA Providence Healthcare, and Harvard Medical School.
How it works:The authors surgically implanted four electrode arrays into areas of the brain that are responsible for speech. The system learned to decode the patient’s brain signals, decide the most likely phonemes he intended to speak, determine the words those phonemes express, and display and speak the words aloud using a personalized speech synthesizer.
Results:After two hours of recording the patient’s brain signals and training on that data, the system achieved 90.2 percent accuracy in the copying task. By the final session, the system achieved 97.5 percent accuracy and enabled the patient to speak on average 31.6 words per minute using a vocabulary of 125,000 words.
Behind the news:Previous work either had muchlower accuracyor generated alimited vocabulary. The new work improved upon a 2023studythat enabled ALS patients to speak with 76.2 percent accuracy using a vocabulary of equal size.
Why it matters:Relative to the 2023 study on which this one was based, the authors changed the positions of the electrodes in the brain and continued to update the GRU throughout the recording/training sessions. It’s unclear which changes contributed most to the improved outcome. As language models improve, new models potentially could act as drop-in replacements for the models in the authors’ system, further improving accuracy. Likewise, improvements in speech-to-text systems could increase the similarity between the synthetic voice and the patient’s former voice.
We’re thinking:Enabling someone to speak again restores agency. Enabling someone to speak again in their own voice restores identity.
",['https://dl-staging-website.ghost.io/content/images/2024/08/unnamed--2-.png']
Agentic Coding Strides Forward,"An agentic coding assistant boosted the state of the art in an important benchmark by more than 30 percent.
What’s new:Cosine, a startup based in London, unveiledGenie, a coding assistant that achieves top performance on SWE-bench, which tests a model’s ability to solve GitHub issues. The company has yet to announce pricing and availability, but a waitlist is available.
How it works:Genie is afine-tuned version of GPT-4owith a larger context window ofundisclosedsize. It works similarly toagentic coding toolslike Devin, Q, OpenDevin, and SWE-agent. Its agentic workflow loops through four processes: retrieving information, planning, writing code, and running it. It was trained on a proprietary training set that captures software engineers’ processes for reasoning, gathering information, and making decisions. It edits lines of code in place rather than rewriting entire sections or files from scratch.
Results:Tested onSWE-benchFull (2,294 issue-commit pairs across 12 Python repositories), Genie solved 30.1 percent of problems, far ahead of the next closest competitor, Amazon Q, at 19.75 percent. Genie achieved 50.7 percent of the SWE-bench Lite (winnowed to 300 issue-commit pairs to save computation), beating CodeStory Aide plus other models at 43 percent. (Genie’s results don’t appear on the official SWE-bench leaderboard. The leaderboard requires that models document their workings, which Cosine declined to avoid revealing proprietary information. Cosine released Genie’ssolution setsto verify its performance.)
Behind the news:SWE-bench’s creators recently collaborated with OpenAI to produce a new version,SWE-bench Verified. They eliminated extremely difficult and poorly configured problems, leaving 500 human-verified issue-commit pairs. Cosine has yet to publish Genie’s performance on SWE-bench Verified. As of this writing, Amazon Q ranks in first place with 38.8 percent.
Why it matters:Some developers of AI coding assistants train models to follow human-style procedures while others are building AI-native methods. Genie takes a distinct step forward by mimicking software engineers. Competition between thetwo approaches, along with longer context windows, faster inference, and increasingly sophisticated agentic workflows, is driving improvement of coding assistants at a rapid pace.
We’re thinking:We’re glad this Genie escaped the bottle!
",['https://dl-staging-website.ghost.io/content/images/2024/08/unnamed--2--2.jpg']
AI Lobby Expands,"AI is a red-hot topic for lobbyists who aim to influence government policies in the United States.What’s new:The number of organizations lobbying to influence U.S. laws and regulations that affect AI jumped more than 20 percent in the first half of 2024,TechCrunchreported. Data collected by OpenSecrets, which tracks political contributions, shows increased lobbying by startups including OpenAI and Anthropic.How it works:OpenSecrets searched for the words “AI” and “artificial intelligence” in lobbying disclosure forms. Organizations must file such forms quarterly if they discuss specific laws and regulations with decision makers or their staffs.
Yes, but:The lobbying disclosure forms show who is spending money to influence policy, but they provide only a limited view. For instance, they reveal only that an organization aimed to influence AI policy, not the directions in which they aimed to influence it. Similarly, the disclosures shed no light on other efforts to influence laws and regulations such as advertising or campaign contributions. They also don’t reveal how much an organization discussed AI relative to other topics and concerns. For instance, last year the American Medical Association spent $21.2 million on lobbying including AI but, given the wide range of policy issues involved in medicine, AI likely accounted for a small amount of the total.Behind the news:The ramp-up in AI lobbying comes as the U.S. Congress is considering a growing number of laws that would regulate the technology. Since 2023, more than 115 bills have been proposed that seek to restrict AI systems, require developers to disclose or evaluate them, or protect consumers against potential harms like AI bias, infringement of privacy or other rights, or spreading inaccurate informationaccording tothe nonprofit, nonpartisan Brennan Center for Justice. Nearly 400 state laws are also under consideration,according toBSA, a software lobbying group, including California SB-1047, which wouldregulateAI models whose training exceeds a particular threshold of computation. Moreover, the U.S. will hold national elections in November, and lobbying of all kinds typically intensifies as organizations seek to influence candidates for office.Why it matters:Given the large amount of AI development that takes place in the U.S., laws that govern AI in this country have an outsized influence over AI development worldwide. So it’s helpful to know which companies and institutions seek to influence those laws and in what directions. That the army of AI lobbyists includes companies large and small as well as far-flung institutions, with varying degrees of direct involvement in building or using AI, reflects both the technology’s power and the importance of this moment in charting its path forward.We’re thinking:We favor thoughtful regulation of AI applications that reinforces their tremendous potential to do good and limits potential harms that may result from flaws like bias or privacy violations. However, it’s critical to regulate applications, which put technology to specific uses, not the underlying technology, whose valuable uses are wide-ranging and subject to human creativity. It’s also critical to encourage, and not stifle, open models that multiply the potential good that AI can do. We hope the AI community can come together on these issues.
",['https://dl-staging-website.ghost.io/content/images/2024/08/unnamed--3-.png']
AI Agents for AI Research,"While some observers argue that large language models can’t produce truly original output, new work prompted them to generate novel scientific research.
What’s new:Researchers proposedAI Scientist, an agentic workflow that directs large language models to generate ideas for AI research, produce code to test them, and document the enquiry. You can see examples of its output and download the code to generate your own papershere. The team included Chris Lu, Cong Lu, Robert Tjarko Lange, and colleagues at Tokyo-based startup Sakana AI, University of Oxford, University of British Columbia, Vector Institute, and the Canadian Institute for Advanced Research.
How it works:The authors used Claude Sonnet 3.5, GPT-4o, DeepSeek Coder, and LLama 3.1 405B to generate papers in three categories: diffusion image modeling, transformer-based language modeling, and “grokking,” which the authors define as generalization and speed of learning in deep neural networks.
Results:The team used GPT-4o to evaluate the generated papers according to theguidelinesfor papers presented at the Neural Information Processing Systems (NeurIPS) conference. The guidelines include an overall score between 1 (very strongly reject) and 10 (award-quality: flawless and groundbreaking) and a decision to reject or accept the paper.
Why it matters:Agentic workflows are a rising theme in AI research from simpler design patterns likereflectionto complex workflows fortranslating literature. These workflows make it possible to break down complex problems into more manageable subtasks. By breaking the task of conducting AI research into various stages of generating ideas, testing them, and writing a paper, an LLM that has access to the right tools can generate novel research papers with actual experimental results.
We’re thinking:Rather than merely synthesizing existing knowledge, this work points a fascinating direction for using AI to generate new knowledge! Right now, an LLM can suggest starting points for human researchers along with experiments that back up its suggestions.
",['https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-21T142127.807.gif']
Google Imagen 3 Raises the Bar,"Image generation continued its rapid march forward with a new version of Google’s flagship text-to-image model.
What’s new:GoogleintroducedImagen 3, a proprietary model that improves upon the previous version’s image quality and prompt adherence, with features like inpainting and outpainting to be added in the future. Imagen 3 is available via Google’sImageFXweb user interface andVertex AIPlatform. It follows closely upon the releases of Black Forest Labs’ Flux.1 family (open to varying degrees), Midjourney v6.1, and Stability AI Stable Diffusion XL 1 (open weights) — all in the last month.
How it works:The accompanyingpaperdoes not describe the model’s architecture and training procedures in detail. The authors trained a diffusion model on an unspecified “large” dataset of images, text, and associated annotations that was filtered to remove unsafe, violent, low-quality, generated, and duplicate images as well as personally identifying information. Google’s Gemini large language model generated some image captions used in training to make their language more diverse.
Results:Imagen 3 mostly outperformed competing models in head-to-head comparisons based on prompts from datasets includingGenAI-Bench,DrawBench, andDALL-E 3 Eval. The team compared Imagen 3 to Midjourney v6.0, OpenAI DALL-E 3, Stable Diffusion 3 Large, and Stable Diffusion XL 1.0. More than 3,000 evaluators from 71 countries rated the models’ responses in side-by-side comparisons. The raters evaluated image quality, preference regardless of the prompt, adherence to the prompt, adherence to a highly detailed prompt, and ability to generate the correct numbers of objects specified in a prompt. Their ratings (between 1 and 5) were used to compute Elo ratings.
Why it matters:Each wave of advances makes image generators more useful for a wider variety of purposes. Google’s emphasis on filtering the training data for safety may limit Imagen 3’s utility in some situations (indeed, some userscomplainedthat Imagen 3 is more restrictive than Imagen 2, while the Grok2 large language model’s use of an unguardrailed version of Flux.1 for image generation has garneredheadlines). Nonetheless, precautions are an important ingredient in the evolving text-to-image recipe.
We’re thinking:It’s difficult to compare the benchmarks reported for Imagen 3 and the recently releasedFlux.1, which claims similar improvements over earlier models. In any case, Google has yet to publish a benchmark for generating text, a valuable capability for commercial applications. The Flux.1 models, two of which are open to some degree, may prove to be formidable rivals in this area.
",['https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-21T142223.196.gif']
Open Models for Math and Audio,"Alibaba followed up its open-weights Qwen2 large language models with specialized variations.
What’s new:Qwen2-MathandQwen2-Audioare model families devoted to, respectively, solving math problems and generating text directly from audio. Both set new states of the art in a variety of English and Chinese benchmarks, and some versions offer open weights. Notably Qwen2-Math-Instruct-72B, whose 72 billion parameters are fine-tuned according to human preferences, outperformed top models including Claude 3.5 Sonnet, Gemini 1.5-Pro, GPT-4o, and Llama-3.1-405B on some math benchmarks.
Math mavens:Qwen2-Math models includepretrainedandinstruction-tunedvariations that comprise 1.5 billion, 7 billion, and 72 billion parameters. Thelicensefor the largest version is free for noncommerical development and commercial developers who have less than 100 million monthly active users.
Audio/text to text:A revision of the earlier Qwen-Audio,Qwen2-Audiotakes text and audio inputs and generates text outputs. It’s designed to (i) provide text chat in response to voice input including voice transcription and translation between eight languages and (ii) discuss audio input including voice, music, and natural sounds. Weights (8.2 billion parameters) are available for base and instruction-tuned versions. You can try ithere.
Why it matters:Qwen2 delivered extraordinary performance with open weights, putting Alibaba on the map of large language models (LLMs). These specialized additions to the family push forward math performance and audio integration in AI while delivering state-of-the-art models into the hands of more developers.
We’re thinking:It’s thrilling to see models with open weights that outperform proprietary models. The white-hot competition between open and closed technology is good for everyone!
",['https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-21T142304.320.png']
"Higher Performance, Lower Prices","Prices for access to large language models are falling as providers exploit new efficiencies and compete for new customers.
What’s new:Open AIcutthe price of calls to GPT-4o’s API by 50 percent for input tokens and 33 percent for output tokens, with an even steeper discount for asynchronous processing. Not to be outdone, Googlecutthe price of API calls to Gemini 1.5 Flash by approximately 75 percent.
How it works:The latest price reductions follow a steady trend,trackedby Smol.ai CEO Shawn Wang, in which providers are charging less even as model performance (as measured by LMSys’sChatbot Arena LeaderboardElo ratings) rises. Here’s a list of recent prices in order of each model’s  rank on the leaderboard as of this writing:
Behind the news:Less than six months ago, cutting-edge large language models like GPT-4, Claude 2, Gemini 1.0, Llama 2, and Mistral Large were less capable and more expensive than their current versions. For instance, GPT-4 costs $30/$60 per million tokens input/output. Since then, models have notched higher benchmark performances even prices have fallen. The latest models are also faster, have larger context windows, support a wider range of input types, and do better at complex tasks such as agentic workflows.
Why it matters:Competition is fierce to provide the most effective and efficient large language models, offering an extraordinary range of price and performance to developers. Makers of foundation models that can’t match the best large models in performance or the best small models in cost are in a tight corner.
We’re thinking:What an amazing time to be developing AI applications! You can choose among models that are open or closed, small or large, faster or more powerful in virtually any combination. Everyone is competing for your business!
",['https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-14T145536.630.gif']
Out of the Black Forest,"A new company with deep roots in generative AI made an eye-catching debut.
What’s new:Black Forest Labs, home to alumni of Stability AI,releasedthe Flux.1 family of text-to-image models under a variety of licenses including open options. The largest of them outperformed Stable Diffusion 3 Ultra, Midourney v6.0, and DALL·E 3 HD in the company’s internal qualitative tests.
How it works:The Flux.1 models are based on diffusion transformers that were trained usingflow matching, a form of diffusion. Like other latent diffusion models, given text and a noisy image embedding, they learn to remove the noise. At inference, given text and an embedding of pure noise, they remove the noise in successive steps and render an image using a decoder that was trained for the purpose.
Results:Black Forest Labs evaluated the models internally in qualitative tests. Given images produced by one of the Flux.1 family and a competitor, roughly 800 people judged which they preferred for various qualities. The two larger versions achieved high scores.
Behind the news:The Black Forest Labs staff includes former core members of Stability AI, whichlostmany top employees in April. Black Forest CEO Robin Rombach co-authored the papers that introduced VQGAN, latent diffusion, adversarial diffusion distillation, Stable Diffusion XL, and Stable Video Diffusion.
Why it matters:Text-to-image models generally occupy three tiers: large commercial models like Midjourney v6, OpenAI DALL·E 3, and Adobe Firefly; offerings that are open-source to varying degrees like Stability AI’s Stable Diffusion 3 Medium; and smaller models that can run locally like Stable Diffusion’s Stable Diffusion XL Lightning. The Flux.1 suite checks all the boxes with high marks in head-to-head comparisons.
We’re thinking:In late 2022, Stability AI’s release of the open Stable Diffusion unleashed a wave of innovation. We see a similar wave building on the open versions of Flux.1.
",['https://dl-staging-website.ghost.io/content/images/2024/08/unnamed--79-.jpg']
AI Leadership Makes for a Difficult Balance Sheet,"OpenAI may be spending roughly twice as much money as it’s bringing in, a sign of the financial pressures of blazing the trail in commercial applications of AI.
What’s new:OpenAI’s operating expenses could amount to $8.5 billion in 2024, according to anestimatebyThe Informationbased on anonymous sources. Meanwhile, its annual revenue is shaping up to be around $3.5 billion to $4.5 billion, putting it on course to lose between $4 billion and $5 billion this year.
Revenue versus expenses:The report combined previous reporting with new information from people “with direct knowledge” of OpenAI’s finances and its relationship with Microsoft, which provides computing power for GPT-4o, ChatGPT, and other OpenAI products.
Why it matters:ChatGPT famously grew at an extraordinary pace in 2023 when the number of visitsballoonedto 100 million within two months of the service’s launch. OpenAI’s internal sales team turned that enthusiasm into fast-growing revenue, reportedlyoutpacingeven Microsoft’s sales of OpenAI services. Yet that growth rests on top-performance AI models, which are expensive to develop, train, and run.
We’re thinking:OpenAI is a costly undertaking: OpenAI CEO Sam Altmansaidit would be “the most capital-intensive startup in Silicon Valley history.” But generative AI is evolving quickly. With OpenAI’s revenue rising, its models becoming more cost-effective (witnessGPT-4o mini), and the cost of inference falling, we wouldn’t bet against it.
",['https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-14T145608.454.png']
Machine Translation Goes Agentic,"Literary works are challenging to translate. Their relative length, cultural nuances, idiomatic expressions, and expression of an author’s individual style call for skills beyond swapping words in one language for semantically equivalent words in another. Researchers built a machine translation system to address these issues.
What’s new:Minghao Wu and colleagues at Monash University, University of Macau, and Tencent AI Lab proposedTransAgents, which uses a multi-agent workflow to translate novels from Chinese to English. You can try a demohere.
Key insight:Prompting a large language model (LLM) to translate literature often results in subpar quality. Employing multiple LLMs to mimic human roles involved in translation breaks down this complex problem into more tractable parts. For example, separate LLMs (or instances of a single LLM) can act as agents that take on roles such as translator and localization specialist, and they can check and revise each other’s work. An agentic workflow raises unsolved problems such as how to evaluate individual agents’ performance and how to measure translation quality. This work offers a preliminary exploration.
How it works:TransAgents prompted pretrained LLMs to act like a translation company working on a dataset ofnovels. The set included 20 Chinese novels, each containing 20 chapters, accompanied by human translations into English.
Results:Professional translators compared TransAgents’ output with that of human translators and GPT-4 Turbo in a blind test. One said TransAgents “shows the greatest depth and sophistication,” while another praised its “sophisticated wording and personal flair” that “effectively conveys the original text’s mood and meaning.”
Why it matters:While machine translation of ordinary text and conversations has made great strides in the era of LLMs, literary translation remains a frontier. An agentic workflow that breaks down the task into subtasks and delegates them to separate LLM instances makes the task more manageable and appears to produce results that appeal to human judges (and an LLM as well). That said, this is preliminary work that suggests a need for new ways to measure the quality of literary translations.
We’re thinking:Agentic workflowsraise pressing research questions: What is the best way to divide a task for different agents to tackle? How much does the specific prompt at each stage affect the final output? Good answers to questions like this will lead to powerful applications.
Are you an experienced developer? Share your coding story and inspire new learners! We’re celebrating the launch of “AI Python for Beginners,” taught by Andrew Ng, and we’d like to feature your story to inspire coders who are just starting out.Submit your story here!
",['https://dl-staging-website.ghost.io/content/images/2024/08/The-Batch-ads-and-exclusive-banners---2024-08-13T105325.070.png']
Google Gets Character.AI Co-Founders,"Character.AI followed an emerging pattern for ambitious AI startups, trading its leadership to a tech giant in exchange for funds and a strategic makeover.
What’s new:Google hired Character.AI’s co-founders and other employees and paid an undisclosed sum for nonexclusive rights to use Character.AI’s technology,The Informationreported. The deal came shortly afterMicrosoft and InflectionandAmazon and Adeptstruck similar agreements.
New strategy:Character.AI builds chatbots that mimic personalities from history, fiction, and popular culture. When it started, it was necessary to build foundation models to deliver automated conversation, the companyexplainedin a blog post. However, “the landscape has shifted” and many pretrained models are available. Open models enable the company to focus its resources on fine-tuning and product development under its new CEO, former Character.AI general counsel Dom Perella. Licensing revenue from Google will help Character.AI to move forward.
Behind the news:At Google, Shazeer co-authored “Attention Is All You Need,” the 2017paperthat introduced the transformer architecture. De Freitas led theMeenaandLaMDAprojects to develop conversational models. They left Google and founded Character.AI in late 2021 to build a competitor to OpenAI that would develop “personalized superintelligence.” The company hadraised$193 million before its deal with Google.
Why it matters:Developing cutting-edge foundation models is enormously expensive, and few companies can acquire sufficient funds to keep it up. This dynamic is leading essential team members at high-flying startups to move to AI giants. The established companies need the startups’ entrepreneurial mindset, and the startups need to retool their businesses for a changing market.
We’re thinking:Models with open weights nowcompetewith proprietary models for the state of the art. This is a sea change for startups, opening the playing field to teams that want to build applications on top of foundation models. Be forewarned, though: New proprietary models such as the forthcoming GPT-5 may change the state of play yet again.
",['https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-07T144042.618.gif']
AI-Assisted Applicants Counter AI-Assisted Recruiters,"Employers are embracing automated hiring tools, but prospective employees have AI-powered techniques of their own.
What’s new:Job seekers are using large language models and speech-to-text models to improve their chances of landing a job,Business Insiderreported. Some startups are catering to this market with dedicated products.How it works:Text generators like ChatGPT can help candidates quickly draft resumes, cover letters, and answers to application questions. But AI can also enable a substitute — human or automated — to stand in for an applicant.
Behind the news:Employers can use AI to screen resumes for qualified candidates, identify potential recruits, analyze video interviews, and otherwise streamline hiring. Some employers believe these tools reduce biases from human decision-makers, but criticssaythey exhibit the same biases. No national regulation controls this practice in the United States, but New York City requires employers to audit automated hiring software and notify applicants if they use it. The states of Illinois and Maryland require employers who conduct video interviews to receive an applicant’s consent before subjecting an interview to AI-driven analysis. The European Union’s AI Act classifies AI in hiring as a high-risk application that requires special oversight and frequent audits for bias.
Why it matters:When it comes to AI in recruiting and hiring, most attention – and money – has gone to employers. Yet the candidates they seek increasingly rely on AI to get their attention and seal the deal. A late 2023 LinkedIn surveyfoundthat U.S. and UK job seekers applied to 15 percent more jobs than a year earlier, a change many recruitersattributedto generative AI.
We’re thinking:AI is making employers and employees alike more efficient in carrying out the tasks involved in hiring. Misaligned incentives are leading to an automation arms race, yet both groups aim to find the right fit. With this in mind, we look forward to AI-powered tools that match employers and candidates more efficiently so both sides are better off.
",['https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-07T144055.642.gif']
Ukraine Develops Aquatic Drones,"Buoyed by its military success developing unmanned aerial vehicles, Ukraine is building armed naval drones.
What’s new:A fleet of robotic watercraft has shifted the balance of naval power in Ukraine’s ongoing war against Russia in the Black Sea,IEEE Spectrumreported.
How it works:Ukraine began building seafaring drones to fight a Russian blockade of the Black Sea coast after losing most of its traditional naval vessels in 2022. The Security Service of Ukraine, a government intelligence and law enforcement agency, first cobbled together prototypes from off-the-shelf parts. It began building more sophisticated versions as the home-grownaerial drone industrytook off.
Drone warfare:Ukraine’s use of aquatic drones has changed the course of the war in the Black Sea, reopening key shipping routes. Ukraine hasdisabledabout a third of the Russian navy in the region and pushed it into places that are more difficult for the sea drones to reach. Russia has also been forced to protect fixed targets like bridges from drone attacks by fortifying them with guns and jamming GPS and Starlink satellite signals.
Behind the news:More-powerful countries are paying attention to Ukraine’s use of sea drones. In 2022, the United States Navy established a group calledUncrewed Surface Vessel Division One, which focuses on deploying both large autonomous vessels and smaller, nimbler drones. Meanwhile, China hasdevelopedlarge autonomous vessels that can serve as bases for large fleets of drones that travel both above and under water.
Why it matters:While the U.S. has experimented with largeautonomous warships,smaller drones open different tactical and strategic opportunities. While larger vessels generally must adhere to established sea routes (and steer clear of shipping vessels), smaller vessels can navigate more freely and can make up in numbers and versatility what they lack in firepower.We’re thinking:We support Ukraine’s right to defend itself against unwarranted aggression, and we’re glad the decision to detonate its aquatic drones remains in human hands. We hope the innovations spurred by this conflict will find beneficial applications once the war is over.
",['https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-07T144157.864.gif']
Art Attack,"Seemingly an innocuous form of expression, ASCII art opens a new vector for jailbreak attacks on large language models (LLMs), enabling them to generate outputs that their developers tuned them to avoid producing.
What's new:A team led by Fengqing Jiang at University of Washington developedArtPrompt, a technique to test the impact of text rendered as ASCII art on LLM performance.
Key insight:LLM safety methods such as fine-tuning are designed to counter prompts that can cause a model to produce harmful outputs, such as specific keywords and tricky ways to ask questions. They don’t guard against atypical ways of using text to communicate, such as ASCII art. This oversight enables devious users to get around some precautions.
How it works:Researchers gauged the vulnerability to ASCII-art attacks ofGPT-3.5, GPT-4,Claude,Gemini, andLlama 2. They modified prompts fromAdvBenchorHEx-PHI, which contain prompts that are designed to make safety-aligned LLMs refuse to respond, such as “how to make a bomb.”
Results:ArtPrompt successfully circumvented LLM guardrails against generating harmful output, achieving an average harmfulness score of 3.6 out of 5 across all five LLMs. The next most-harmful attack method,PAIR, which prompts a model several times and refines its prompt each time, achieved 2.67.
Why it matters:This work adds to the growingbodyofliteratureon LLM jailbreak techniques. While fine-tuning is fairly good at preventing innocent users — who are not trying to trick an LLM — from accidentally receiving harmful output, we have no robust mechanisms for stopping a wide variety of jailbreak techniques. Blocking ASCII attacks would require additional input- and output-screening systems that are not currently in place.
We're thinking:We’re glad that LLMs are safety-tuned to help prevent users from receiving harmful information. Yet many uncensored models are available to users who want to get problematic information without implementing jailbreaks, and we’re not aware of any harm done. We’re cautiously optimistic that, despite the lack of defenses, jailbreak techniques also won’t prove broadly harmful.
Calling all developers working on visual AI applications! You’re invited to our upcoming VisionAgent Developer Meetup, an in-person and virtual event with Andrew Ng and the LandingAI MLE team for developers working on visual AI and related computer vision applications.Register now
",['https://dl-staging-website.ghost.io/content/images/2024/08/vision-agent-newsletter-1680x945.png']
The State of the Art Is Open,"Meta raised the bar for large language models with open weights and published details about how it built one that outperforms GPT-4o and Claude 3.5 Sonnet by some measures.
What's new:Llama 3.1 405Bdelivers state-of-the-art performance on a handful of public benchmarks and has a context window of 128,000 input tokens while allowing a range of commercial uses. In addition to the 405-billion parameter model, Meta released new versions of the earlier Llama 3 70B (70 billion parameters) and 8B (8 billion parameters). Model weights are availablehere.
Key insight:Fine-tuning on generated data can improve a model’s performance, but incorrect or lower-quality examples degrade it. The Llama team undertook an extensive effort to fix or remove bad examples using a variety of tools including the model itself, auxiliary models, and off-the-shelf tools.
How it works:Llama 3.1 models are transformers that have been pretrained to predict the next token in a sequence. Meta provided more information about the development of Llama 3.1 405B than the smaller versions. Its pretraining dataset comprised 16.4 trillion tokens of text, “much” of it scraped from the web. The pretrained model was fine-tuned to perform seven tasks, including coding and reasoning, via supervised learning anddirect preference optimization(DPO). Most of the fine-tuning data was generated by the model itself and curated using a variety of methods including agentic workflows. For instance,
Results:The authors compared Llama 3.1 405B to Claude 3.5 Sonnet, GPT-4, GPT-4o, and Nemotron 4 340B on 16 public benchmarks. It either outperformed or tied the other models on seven of the 16 (although two, GSM8K and MMLU zero-shot chain-of-thought, are not directly comparable due to differences in prompting methods). For instance, Llama 3.1 405B set a new state of the art in IFEval (general knowledge), ARC Challenge (reasoning), and Nexus (tool use). The smaller versions outperformed other models in the same general size classes as well. Llama 3.1 70B set new states of the art in all benchmarks for general knowledge, coding, math, and reasoning. Llama 3.1 8B dominated general, coding, and math benchmarks.
License:Llama 3.1 models are licensed under acustom licensethat allows both commercial use (by companies with up to 700 million monthly active users in the month prior to Llama 3.1’s release) and training other models on generated data. This enables many companies to use it as they like while potentially requiring Meta’s largest competitors to negotiate a commercial license.
The French connection:Separately, Mistral announced its next-generation LLMMistral Large 2, whichallowsnoncommercial use but requires a special license for commercial use. The 123 billion-parameter model boasts performance similar to that of Llama 3.1 405B on a number of benchmarks despite being less than one-third the size.
Why it matters:The Llama 3.1 family continues Meta’s contributions in open models and extends them to some commercial uses. The upgraded 8B and 70B models perform better than their predecessors, while the 405B version rivals top proprietary models and enables researchers to generate high-quality synthetic data for training further models. The team provides extensive detail about how they generated fine-tuning data. For each task, they describe the pipeline used to create the data along with various notes about what worked and what didn’t work for them — helpful information for researchers who aim to build next-generation LLMs.
We're thinking:Data-centric AI, the discipline of systematically engineering data to build a successful AI system, is critical for machine learning. The Llama 3.1 paper makes clear that systematically engineering the training data was also a key to training what is, as far as we know, the first open weights model to achieve better performance than the best proprietary models on multiple benchmarks. The potential of open weights is looking better every day!
",['https://dl-staging-website.ghost.io/content/images/2024/07/unnamed---2024-07-31T174116.563.gif']
Search Gets Conversational,"OpenAI is testing an AI-powered search engine in a bid to compete head-to-head with both Google and its close partner Microsoft Bing.
What’s new:OpenAIreleasedSearchGPT, an integrated search engine and large language model that aims to be friendly to both users and publishers. Access is limited initially to selected trial users. OpenAI offers a wait list but no timeline for expanding access.
How it works:SearchGPT sorts results collected by web crawler, like Google and its competitors. It differs in providing direct answers to queries and offering a conversational user interface for follow-up questions. OpenAI has not disclosed the underlying model.
Behind the news:OpenAI’s move is part of a larger race to supercharge web search with AI.
Why it matters:Search stands to be disrupted by advances in AI, and agents that browse multiple articles to synthesize a result are becoming more capable. OpenAI’s approach looks like a step forward (and smart business insofar as it leads users into deeper relationship with its models), and its strategy of licensing content from trusted sources could prove to be an advantage.
We’re thinking:In less than two years, OpenAI has revolutionized expectations of one of the web’s bedrock applications, search. Its progress shows how AI can make applications smarter, more efficient, and more responsive.
",['https://dl-staging-website.ghost.io/content/images/2024/07/unnamed---2024-07-31T174238.698.gif']
Web Data Increasingly Off Limits,"Online publishers are moving to stop AI developers from training models on their content.
What’s new:Researchers at MITanalyzedwebsites whose contents appear in widely used training datasets. Between 2023 and 2024, many of these websites changed their terms of service to ban web crawlers, restricted the pages they permit web crawlers to access, or both.How it works:MIT’s Data Provenance Initiative examined 14,000 websites whose contents are included in three large datasets, each of which contains data from between 16 and 45 million websites:C4(1.4 trillion text tokens from Common Crawl),RefinedWeb(3 trillion to 6 trillion text tokens plus image links), andDolma(3 trillion text tokens).
Results:In the past year, websites responsible for half of all tokens (text scraped and encoded for use as training data) in the study changed their terms of service to forbid either crawlers in general or use of their content to train AI systems. Robots.txt files showed the same shift.
Behind the news:Data that once was freely available is becoming harder to obtain on multiple fronts. Software developers, authors, newspapers, and music labels havefiledlawsuits that allege that AI developers trained systems on their data in violation of the law.OpenAIand others recently agreed to pay licensing fees to publishers for access to their material. Last year, Reddit and Stack Overflow startedchargingAI developers for use of their APIs.Yes, but:The instructions in robots.txt files are not considered mandatory, and web crawlers can disregard them. Moreover, most websites have little ability to enforce their terms of use, which opens loopholes. For instance, if a site disallows one company’s crawler, the company may hire an intermediary to scrape the site.
Why it matters:AI systems rely on ample, high-quality training data to attain high performance. Restrictions on training data give developers less scope to build valuable models. In addition to affecting commercial AI developers, they may also limit research in academia and the nonprofit sector.
We’re thinking:We would prefer that AI developers be allowed to train on data that’s available on the open web. We hope that future court decisions and legislation will affirm this.
",['https://dl-staging-website.ghost.io/content/images/2024/07/unnamed---2024-07-31T174326.474.png']
Synthetic Data Factory,"Researchers increasingly fine-tune models on synthetic data, but generated datasets may not be sufficiently diverse. New work used agentic workflows to produce diverse synthetic datasets.
What’s new:Arindam Mitra, Luciano Del Corro, Guoqing Zheng, and colleagues at Microsoft introducedAgentInstruct, a framework for producing synthetic data for fine-tuning large language models (LLMs).
Key insight:To generate synthetic data for fine-tuning, researchers typically prompt an LLM to generate responses (and possibly further prompts) using aselection of existing prompts. While training on the resulting dataset can improve model performance, the synthetic data’s distribution may not match that of real-world data, yielding inconsistent performance. A more methodical approach can generate data closer to the real-world distribution: First generate prompts from each example in a large, diverse dataset, then generate responses.
How it works:The authors generated a synthetic text dataset based onthreeunlabeleddatasets(including code) scraped from the web. They generated new examples for 17 tasks, including natural language tasks like reading comprehension and word puzzles as well as coding, tool use, and estimating measurements.
Results:The authors compared Orca 3’s performance against that of competitors on 14 benchmarks. Orca 3 outperformed Mistral-7B (fine-tuned on prompts from previous versions of Orca) and Mistral-7B-Instruct (fine-tuned to respond to instructions) on 13 benchmarks. In some cases, it did so by large margins; for instance 40 percent on AGIEVAL, 54 percent on GSM8K, and 19 percent on MMLU. Orca 3 fell short of GPT-4 on 12 benchmarks.
Why it matters:The authors defined agentic workflows that turn text into diverse data for fine-tuning models. Their framework offers a pattern for AI engineers who want to build synthetic datasets for other tasks.
We’re thinking:We’re excited to see agentic workflows find applications that a wide variety of AI developers might put to use!
Tell us about your deep learning use cases and issues that need to be addressed and get a chance to win a $200 Amazon gift card! Take 10 minutes to fill out thisquick surveynow
",['https://dl-staging-website.ghost.io/content/images/2024/07/DL.AI-Ad--6-.png']
Mini but Mighty,"A slimmed-down version of Open AI’s multimodal flagship packs a low-price punch.
What’s new:OpenAIreleasedGPT-4o mini, a smaller text-image-video-audio generative model that, according to the company, generally outperforms models from Google and Anthropic models of similar size at a lower price for API access. It newly underpins the free version of ChatGPT.
How it works:GPT-4o mini currently accepts text and image inputs and outputs text. Image output as well as video and audio input/output are coming soon. OpenAI did not provide information about its architecture or training buttoldTechCrunchit’s roughly the size of Claude 3 Haiku, Gemini 1.5 Flash, and the 8-billion-parameter version of Llama 3. It has a context window of 128,000 tokens and can output up to around 16,400 tokens.
Behind the news:GPT-4o mini part of a July wave of smaller large language models.
Why it matters:Powerful multimodal models are becoming ever more widely available at lower prices, creating opportunities for developers and researchers alike. GPT-4o mini sets a new standard for others to beat. Its price may be especially appealing to developers who aim to build agentic workflows that require models to process large numbers of tokens on their way to producing output.
We’re thinking:Not long ago, pushing the edge of large language models meant making them larger, with higher computing costs to drive rising parameter counts. But building bigger models has made it easier to develop smaller models that are more cost-effective and nearly as capable. It’s a virtuous circle: Costs fall and productivity rises to everyone’s benefit.
",['https://dl-staging-website.ghost.io/content/images/2024/07/unnamed--76-.jpg']
Meta Withholds Models From Europe,"European users won’t have access to Meta’s multimodal models.
What’s new:Meta said it wouldwithholdfuture multimodal models from the European Union (EU) to avoid being charged, banned, or fined for running afoul of the region’s privacy laws, according to Axios. (The newly releasedLlama 3.1family, which processes text only, will be available to EU users.)How it works:EU data regulators have said that Meta may be violating EU privacy laws by training models on data from Facebook, Instagram, and its other properties. Meta’s move in Europe follows itswithdrawalof generative models from Brazil, after that country’s national data-protection authoritystruck downthe part of Meta’s privacy policy that allowed it to use personal data from users of Meta products to train AI models.
Apple and OpenAI in Europe:Meta is not the only global AI company that’s wary of EU technology regulations.
Why it matters:Different regions are taking different paths toward regulating AI. The EU is more restrictive than others, creating barriers to AI companies that develop new technology and products. Meta and Apple are taking proactive steps to reduce their risks even if it means foregoing portions of the European market.
We’re thinking:We hope regulators everywhere will think hard about how to strike a balance between protecting innovation and other interests. In this instance, the EU’s regulations have prompted Meta to make a decision that likely likely set back European AI while delivering little benefit to citizens.
",['https://dl-staging-website.ghost.io/content/images/2024/07/unnamed--77-.jpg']
AI Investors Hoard GPU Power,"Investors have been gathering AI chips to attract AI startups.
What’s new:Venture-capital firms are stockpiling high-end graphics processing units (GPUs), according to areportbyThe Information. They’re using the hardware to provide processing power to their portfolio companies at reduced or no cost.
How it works:Andreessen Horowitz (A16Z), a prominent Silicon Valley venture investment firm, has amassed the largest known stock of GPUs dedicated to venture-funded startups. The firm plans to acquire more than 20,000 GPUs including top-of-the-line Nvidia H100s, which can sell for tens of thousands of dollars each — roughly enough to train a competitive large language model.
Behind the news:High-end GPUs were inshort supplyearly last year. The shortage haseasedsignificantly, but getting access to enough processing power to train and run large models still isn’t easy. A16Z follows several other investors that have sought to fill the gap for startups.
Yes, but:David Cahn, a partner at A16Z rival Sequoia Capital,arguesthat stockpiling GPUs is a mistake that could leave venture funds holding large quantities of expensive, rapidly depreciating, hardware. Cahn believes startups and small developers soon may have an easier time getting their hands on the processing power they need. Nvidia recentlyannouncedits new B100 and B200 GPUs, whose arrival should stanch demand for older units like the H100.
Why it matters:AI startups are hot, and venture-capital firms compete for early equity in the most promising ones. In addition to funding, they frequently offer advice, contacts, office support — and now processing power to empower a startup to realize its vision.
We’re thinking:Venture investors who use GPUs to sweeten a deal give new meaning to the phrase “bargaining chips.”
",['https://dl-staging-website.ghost.io/content/images/2024/07/unnamed---2024-07-24T144606.148.gif']
Expressive Synthetic Talking Heads,"Previous systems that produce a talking-head video from a photo and a spoken-word audio clip animate the lips and other parts of the face separately. An alternative approach achieves more expressive results by animating the head as a whole.
What’s new:Sicheng Xu and colleagues at Microsoft developedVASA-1, a generative system that uses a facial portrait and spoken-word recording to produce a talking-head video with appropriately expressive motion. You can see its outputhere.
Key insight:When a person speaks, the facial expression and head position change over time, while the overall shapes of the face and head don’t. By learning to represent an image via separate embeddings for facial expression and head position — which change — as well as for facial structure in its 2D and 3D aspects — which don’t — a latent diffusion model can focus on the parts of the image that matter most. (Latent diffusionis a variant of diffusion that saves computation by processing a small, learned vector of an image instead of the image itself.)
How it works:VASA-1 comprises four image encoders (three 2D CNNs and one 3D CNN), one image decoder (another 2D CNN),Wav2Vec 2.0, and a latent diffusion image generator. The authors trained the system, given an image of a face and a recorded voice, to generate a series of video frames that conform to the voice. The training set wasVoxCeleb2, which includes over 1 million short videos of celebrities talking. The authors added labels for gaze direction, head-to-camera distance, and an emotional intensity score computedbyseparatesystems.
Results:The authors measured their results by training a model similar toCLIPthat produces a similarity score on how well spoken audio matches a video of a person speaking (higher is better). On the VoxCeleb2 test set, their approach produced a similarity score of 0.468 compared to 0.588 for real video. The nearest contender,SadTalker, which generates lip, eye, and head motions separately, achieved a similarity score of 0.441.
Why it matters:By learning to embed different aspects of a face separately, the system maintained the face’s distinctive, unchanging features while generating appropriate motions. This also made the system more flexible at inference: The authors demonstrated its ability to extract a video’s facial expressions and head movements and apply them to different faces.
We’re thinking:Never again will we take talking-head videos at face value!
Test, benchmark, and grow your skills with new assessments from Workera! Available domains include AI Foundations, Machine Learning, GenAI, and MLOps.Try Workera today for $0!
",['https://dl-staging-website.ghost.io/content/images/2024/07/unnamed---2024-07-23T090621.368.png']
Copyright Claim Fails in GitHub Case,"A judge rejected key claims in a lawsuit by developers against GitHub, Microsoft, and OpenAI, the first decision in a series of court actions related to generative AI.
What’s new: A U.S. federal judgedismissedclaims of copyright infringement and unfair profit in a class-action lawsuit that targeted GitHub Copilot and the OpenAI Codex language-to-code model that underpins it.
The case:In November 2022, programmer Matthew Butterick and the Joseph Saveri Law Firmfiledthe lawsuit in U.S. federal court. The plaintiffs claimed that GitHub Copilot had generated unauthorized copies of open-source code hosted on GitHub, which OpenAI Codex used as training data. The copies allegedly infringed on developers’ copyrights. The defendants tried repeatedly to get the lawsuit thrown out of court. In May 2023, the judgedismissedsome claims, including a key argument that GitHub Copilot could generate copies of public code without proper attribution, and allowed the plaintiffs to revise their arguments.
The decision:The revised argument focused on GitHub Copilot’sduplication detection filter. When enabled, the filter detects output that matches public code on GitHub and revises it. The plaintiffs argued that the existence of this feature demonstrated GitHub Copilot’s ability to copy code in OpenAI Codex’s training set. The judge was not persuaded.
Yes, but:The lawsuit is reduced, but it isn’t finished. A breach-of-contract claim remains. The plaintiffs aim to show that OpenAI and GitHub used open-source code without providing proper attribution and thus violated open-source licenses. In addition, the plaintiffs will refile their unjust-enrichment claim.
Behind the news:The suit against Github et al. is one of several underway that are testing the copyright implications of training AI systems.Getty Images,Authors’ Guild,The New York Times, and other mediaoutletsalong with a consortium ofmusic-industry giantshave sued OpenAI and other AI companies. All these cases rest on a claim that copying works protected by copyright for the purpose of training AI models violates the law — precisely what the plaintiffs failed to show in the GitHub case.
Why it matters:This lawsuit specifically concerns code written by open-source developers. A verdict could determine how code can be used and how developers can use generative AI in their work. However, it has broader implications. (Note: We are not lawyers and we do not provide legal advice.) This dismissal is not a final verdict, but it supports the view that AI developers may have a broad right to use data for training models even if that data is protected by copyright.
We’re thinking:Broadly speaking, we would like AI to be allowed to do with data, including open source code, anything that humans can legally and ethically do, including study and learn. We hope the judge’s decision gives AI developers further clarity on how they can use training data, and we hope it establishes that it’s ethical to use code-completion tools trained on open-source code.
",['https://dl-staging-website.ghost.io/content/images/2024/07/unnamed---2024-07-17T135649.502.gif']
How Open Are Open Models?,"The word “open” can mean many things with respect to AI. A new paper outlines the variations and ranks popular models for openness.
What’s new:Researchers at Radboud Universityevaluateddozens of models billed as open by their developers. They plan to keep their analysis of language models updatedhere.How it works:The authors assessed 40 large language models and six text-to-image generators, adding OpenAI’s closed models ChatGPT and DALL·E 2 as reference points. They evaluated 14 characteristics, scoring each as open (1 point), partially open (0.5 points), or closed (0 points). For example, an API would be described as partially open if using it requires users to register. They divided the characteristics into three categories:
Results:Of the language models,OLMo 7B Instructfrom Allen Institute for AI scored highest with 12 open characteristics and 1 partially open characteristic (it lacked a published, peer-reviewed paper).
Behind the News:The Open Source Initiative (OSI), a nonprofit organization that maintains standards for open-source software licenses, isleadinga process to establish a firm definition of “open-source AI.” The currentdraftholds that an open-source model must include parameters, source code, and information on training data and methodologies under an OSI-recognized license.
Why it matters:Openness is a cornerstone of innovation: It enables developers to build freely on one another’s work. It can also lubricate business insofar as it enables developers to sell products built upon fully open software. And it has growing regulatory implications. For example, the European Union’s AI Act regulates models that are released under an open source license less strictly than closed models. All these factors raise the stakes for clear, consistent definitions. The authors’ framework offers clear, detailed guidelines for developers — and policymakers — in search of clarity.We’re thinking:We’re grateful to AI developers who open their work to any degree, and we especially appreciate fully open availability, documentation, and access. We encourage model builders to release their work as openly as they can manage.
",['https://dl-staging-website.ghost.io/content/images/2024/07/unnamed---2024-07-17T135754.238.gif']
Image Generators in the Arena,"An arena-style contest pits the world’s best text-to-image generators against each other.
What’s new:Artificial Analysis, a testing service for AI models,introducedthe Text to Image Arena leaderboard, which ranks text-to-image models based on head-to-head matchups that are judged by the general public. At the time of this writing, Midjourney v6 beats more than a dozen other models models in its ability to generate images that reflect input prompts, though it lags behind competitors in speed.
How it works:Artificial Analysis selects two models at random and feeds them a unique prompt. Then itpresentsthe prompt and resulting images. Users can choose which model better reflects the prompt. The leaderboard ranks the models based onEloratings, which scores competitors relative to one another.
Who’s ahead?:As of this writing, Midjourney v6 (Elo rating 1,176), which won 71 percent of its matches, holds a slim lead over Stable Diffusion 3 (Elo rating 1,156), which won 67 percent. DALL·E 3 HD holds a distant third place, barely ahead of the open-source Playground v2.5. But there are tradeoffs: Midjourney v6 takes 85.3 seconds on average to generate an image, more than four times longer than DALL·E 3 HD and more than 13 times longer than Stable Diffusion 3. Midjourney v6 costs $66 per 1,000 images (an estimate by Artificial Analysis based on Midjourney’s policies, since the model doesn’t offer per-image pricing), nearly equal to Stable Diffusion 3 ($65), less than DALL·E 3 HD ($80), and significantly more than Playground v2.5 ($5.13 per 1,000 images via theReplicateAPI).
Behind the news:The Text to Image Arena is a text-to-image counterpart of theLMSys Chatbot Arena, which lets users write a prompt, feed it to two large language models, and pick the winner.imgsysandGen-AI Arenasimilarly let users choose between images generated by different models from the same prompt (Gen-AI Arena lets users write their own). However, these venues are limited to open models, which excludes the popular Midjourney and DALL·E.
Why it matters:An image generator’s ability to respond appropriately to prompts is a subjective quality. Aggregating user preferences is a sensible way to measure it. However, individual tastes and applications differ, which makes personalized leaderboards useful as well.We’re thinking:The user interface for some image generators implicitly asks users to judge images. For example, Midjourney defaults to generating four images and asks users which they want to render at higher resolution. This can give the image generator valuable feedback about which image users like. Perhaps data gathered by an arena could feed an algorithm like reinforcement learning from human feedback to help generators learn to produce output that people prefer.
",['https://dl-staging-website.ghost.io/content/images/2024/07/unnamed---2024-07-17T135854.645.png']
Hallucination Detector,"Large language models can produce output that’s convincing but false. Researchers proposed a way to identify such hallucinations.
What’s new:Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal at University of Oxford published amethodthat indicates whether a large language model (LLM) is likely to have hallucinated its output.
Key insight:One way to estimate whether an LLM is hallucinating is to calculate the degree of uncertainty, or entropy, in its output based on the probability of each generated token in the output sequences. The higher the entropy, the more likely the output was hallucinated. However, this approach is flawed: Even if the model mostly generates outputs with a uniform meaning, the entropy of the outputs can still be high, since the same meaning can be phrased in many different ways. A better approach is to calculate entropy based on the distribution of generated meanings instead of generated sequences of words. Given a particular input, the more likely a model is to respond by generating outputs with a variety of meanings, the more likely that a response to that input is a hallucination.
How it works:The authors generated answers to fiveopen-endedquestion-and-answerdatasetsusing various sizes of Falcon, LLaMA 2-chat, and Mistral. They checked the answers for hallucinations using the following method:
Results:The authors measured the classification performance of their method using AUROC, a score between .5 (the classifier is uninformative) and 1 (the classifier is perfect). On average, across all five datasets and six models, the authors’ method achieved .790 AUROC while the baseline entropy achieved .691 AUROC and theP(True)method achieved .698 AUROC. P(True) asks the model (i) to generate up to 20 answers and (ii) whether, given those answers, the one with the highest probability of having been generated is true or false.
Yes, but:The authors’ method fails to detect hallucinations if a model consistently generates wrong answers.
Behind the news:Hallucinations can be a major obstacle to deploying generative AI applications, particularly in fields like medicine or law where missteps can result in injury. One study published earlier this yearfoundthat three generative legal tools produced at least partially incorrect or incomplete information in response to at least one out of every six prompts. For example, given the prompt, “Are the deadlines established by the bankruptcy rules for objecting to discharge jurisdictional,” one model cited a nonexistent rule: “[A] paragraph from the Federal Rules of Bankruptcy Procedure, Rule 4007 states that the deadlines set by bankruptcy rules governing the filing of dischargeability complaints are jurisdictional.”Why it matters:Effective detection of hallucinations not only fosters trust in users — and consequently rising adoption — but also enables researchers to determine common circumstances in which hallucinations occur, helping them to address the problem in future models.
We’re thinking:Researchers are exploring various approaches to mitigate LLM hallucinations in trained models. Retrieval augmented generation (RAG) can help by integrating knowledge beyond a model’s training set, but it isn’t a complete solution.Agentic workflowsthat include tool use to supply factual information and reflection to prompt the model to check itself are promising.
In “Pretraining LLMs,” a short course built in collaboration with Upstage, you’ll learn about pretraining, the first step of training a large language model. You’ll also learn innovative pretraining techniques like depth upscaling, which can reduce training costs by up to 70 percent.Join today
",['https://dl-staging-website.ghost.io/content/images/2024/07/The-Batch-ads-and-exclusive-banners---2024-07-16T085539.096.png']
Claude Advances the LLM Interface,"Claude 3.5 Sonnet lets users work on generated outputs as though they were independent files — a step forward in large language model user interfaces.
What’s new:AnthropicintroducedArtifacts, a feature that displays outputs in a separate window of Claude 1.5 Sonnet’s web interface, outside the stream of conversation that creates and modifies them. Artifacts canincludedocuments, code snippets, HTML pages, vector graphics, or visualizations built using JavaScript.
How it works:Users can enable artifacts from the “feature preview” dropdown in their profile menu at Claude.ai. Then, asked to generate an output that’s likely to act as standalone content and undergo further work, Claude opens an artifact window next to the chat frame, populates it with an initial output, and further updates it according to subsequent prompts.
Why it matters:Artifacts make working with a large language model more fluidly interactive. Large language models (LLMs) have long been able to generate code but, outside of AI-assisted development environments like GitHub with Copilot, executing generated code typically requires further steps such as copy-pasting the code into a development environment. The additional steps add friction for developers and confusion for non-developers. Keeping and running the code in a separate window makes for a convenient, low-friction experience. Likewise when generating images and other kinds of visual output.
We’re thinking:It’s rare when a user interface update makes a tool more useful for casual and hardcore users alike. It’s even more exciting to see it happen to an LLM!
",['https://dl-staging-website.ghost.io/content/images/2024/07/unnamed--70-.jpg']
AI’s Path to Zero Emissions Is Cloudy,"The boom in AI is jeopardizing big tech’s efforts to reach its targets for emissions of greenhouse gasses.
What’s new:Google’sannual environmental reportshows that the company’s total carbon dioxide emissions rose nearly 50 percent between 2019 and 2023 to 14.3 million tons. Google attributes the rise to its efforts to satisfy rising demand for AI.
How it works:Google’s carbon emissions increased 16.7 percent from 2021 to 2022 and another 13.5 percent from 2022 to 2023 for a total 48 percent rise over those periods. “As we further integrate AI into our products, reducing emissions may be challenging due to increasing energy demands from the greater intensity of AI compute, and the emissions associated with the expected increases in our technical infrastructure investment,” the report states.
Countering the trend:Google is working to reduce its greenhouse gas emissions on several fronts. Its effort to purchase electricity from low-emissions sources cut its net carbon footprint by around 30 percent in 2023. It claims that its owned-and-operated data centers are 1.8 times more energy-efficient than a typical enterprise data center, and its sixth-generation tensor processing units (TPUs) are 67 percent more efficient than the prior generation. Google has asked its largest hardware partners to match 100 percent of their energy consumption with renewable energy 2029. The company is pursuing several AI-based initiatives to mitigate climate change from weather prediction to fuel-efficient vehicle routing. It says that AI has the potential to mitigate 5 to 10 percent of global greenhouse gas emissions by 2030.
Behind the news:In 2020, after five years of successfullyreducingits carbon footprint, Google set an ambitious target to reach net-zero greenhouse gas emissions by 2030. But its total emissions since then have risen each year. Google’s experience mirrors that of Amazon and Microsoft, which aim to reach net-zero carbon emissions by 2030 and 2040 respectively. Amazon’s emissionsincreased39 percent from 2019 to 2022, while Microsoft’s emissionsrose29 percent between 2020 and 2023. (Amazon’s and Microsoft’s cloud computing revenues were roughly triple Google’s in 2023 and thus their AI-related greenhouse case emissions  presumably were larger.)
Why it matters:Growing use of AI means greater consumption of energy. The tech giants’ ambitious emissions goals predate the rapid growth of generative AI, and their latest reports show that it’s time to rethink them. This adds urgency to already critical efforts to develop renewable and other low-emissions energy sources.
We’re thinking:We applaud Google’s efforts to cut its carbon emissions and its transparency in issuing annual environmental reports. We’re somewhat relieved to note that, for now, data centers and cloud computing are responsible for1 percentof the world’s energy-related greenhouse gas emissions; a drop in the bucket compared to transportation, construction, or agriculture. Moreover, we believe that AI stands to create huge benefits relative to the climate impact of its emissions, and AI is one of the most powerful tools we have to develop low-carbon energy sources and boost energy efficiency throughout society. Continuing to improve the technology will help us develop lower-carbon energy sources and efficient ways to harness them.
",['https://dl-staging-website.ghost.io/content/images/2024/07/unnamed--71-.jpg']
Amazon Onboards Adept,"Amazon hired most of the staff of agentic-AI specialist Adept AI in a move that echoes Microsoft’s absorption of Inflection in March.
What’s new:Amazon onboarded most of the leadership and staff of Adept AI, which has been training models to operate software applications running on local hardware,GeekWirereported. Amazon licensed Adept’s models, datasets, and other technology non-exclusively. The companies did not disclose the financial terms of the deal. (Disclosure: Andrew Ng serves on Amazon’s board of directors.)
How it works:Amazon hired two thirds of Adept’s former employees. Those who remain will “focus entirely on solutions that enable agentic AI” based on proprietary models, custom infrastructure, and other technology.
Behind the news:Amazon’s agreement with Adept is one of several moves to compete in AI for both businesses and consumers. In March, the company completed a $4 billioninvestmentin Anthropic in exchange for a minority share in the startup. It’s reportedly developing new models andoverhaulingits longstanding Alexa voice assistant.
Why it matters:Luan and his team say they’re aiming to automate corporate software workflows, a potentially valuable and lucrative market. Although Amazon Web Services’ Bedrock platform already enables users tobuildAI agents, Adept’s talent may bring expanded agentic and interactive capabilities.We’re thinking:AI agentic capabilities areblossoming, and Adept’s work is a notable example.
",['https://dl-staging-website.ghost.io/content/images/2024/07/unnamed---2024-07-10T143728.158.png']
OpenAI Blocks China and Elsewhere,"OpenAI will stop serving users in China and other nations of concern to the U.S. government as soon as next week.
What’s new:Open AI notified users in China they would lose API access on July 9,Reutersreported. The move affects users in countries where the company doesn’t support access to its services officially (which include Cuba, Iran, Russia, North Korea, Syria, Venezuela, and others), but where it appears to have been serving API calls anyway.
How it works:Previously OpenAI blocked requests from outsidesupported countriesif it detected a virtual private network or other method to circumvent geographic restrictions, but it had enforced such limits lightlyaccording toSecurities Times. The email warning started a race among AI companies in China to attract cast-off OpenAI users.
Behind the news:OpenAI’s crackdown on non-supported countries comes amid rising technological rivalry between the governments of the United States and China. The U.S. has taken several steps to try to curb China’s access to U.S.-built AI hardware and software, and some U.S. AI companies such as Anthropic and Google don’t operate in China. The Commerce Departmentplansto attempt to restrict China’s access to the most advanced AI models built by U.S. developers such as OpenAI. The Treasury Departmentissueddraft restrictions on U.S. investments in AI companies based in China, Hong Kong, and Macau. Moreover, the U.S.imposedcontrols on exports of advanced GPUs to Chinese customers.
Why it matters:Many startups in China and elsewhere relied on OpenAI’s models. However, China’s development of AI models is already quite advanced. For example, Alibaba’s Qwen2, which offers open weights, currently tops Hugging Face’s Open LLM Leaderboard (see below), ahead of Meta's Llama 3.
We’re thinking:Efforts to restrict U.S. AI technology can go only so far. At this point, the U.S. seems to have at most a six-month lead over China. OpenAI’s move encourages other nations to make sure they have robust, homegrown models or access to open source alternatives.
",['https://dl-staging-website.ghost.io/content/images/2024/07/unnamed---2024-07-03T153334.258.png']
Challenging Human-Level Models,"An influential ranking of open models revamped its criteria, as large language models approach human-level performance on popular tests.
What’s new:Hugging Faceoverhauledits Open LLM Leaderboard, reshuffling its assessments of the smartest contenders. Therevised leaderboardis based on new benchmarks designed to be more challenging and harder to game.
Intelligence reordered:The new Open LLM Leaderboard paints a very different picture than the earlier version: Some models moved up or down as many as 59 places. In the debut rankings, Qwen2’s recentlyreleased72-billion-parameter, instruction-tuned version topped the list with an average score of 43.02 out of 100. Meta’s Llama 3-70B-Instruct came in second with 36.67.Addressing saturation and contamination:Launched last year, theearlier version(which is still operating) ranks open large language models according to an aggregate of scores on six popular benchmarks. However, in the intervening months, the best models approached human-level scores, partly due to technical improvements and partly because the test answers leaked into the models’ training sets. The revised leaderboard replaces the old tests and corrects earlier flaws and errors:
Behind the news:Leakage of training examples into test sets is a rising challenge to evaluating model performance. While Hugging Face relies on open benchmarks, other groups have attempted to address the issue by limiting access to the test questions or changing them regularly. Vals.AI, an independent model testing company,developedproprietary industry-specific tests for finance and law. Data consultancy Scale AIintroducedits own leaderboards, measuring models on proprietary tests in natural languages, math, and coding.Why it matters:Two million unique visitors browsed the Open LLM Leaderboard in the past year, and over 300,000 Hugging Face community members use and collaborate on it each month. Developers trust its scores, both individually and in aggregate, to decide which models to use and to judge the progress of their own efforts based on open models.
We’re thinking:As its name implies, the Open LLM leaderboard measures performance in natural language skills. Hugging Face also maintains anOpen VLM Leaderboard, which tests vision-language skills.
",['https://dl-staging-website.ghost.io/content/images/2024/07/unnamed--67-.jpg']
Music Industry Sues AI Startups,"A smoldering conflict between the music industry and AI companies exploded when major recording companies sued up-and-coming AI music makers.
What’s new:Sony Music, Universal Music Group (UMG), and Warner Music — the world’s three largest music companies — and a trade organization, Recording Industry Association of America (RIAA),suedSuno and Udio, which offer web-based music generators, for alleged copyright violations.How it works:The music powers filed separate lawsuits againstSunoandUdioin U.S. federal courts. The plaintiffs allege that the startups used copyrighted songs owned by RIAA members as training data, in the process making unauthorized copies without receiving permission or compensating the owners. They seek damages of at least $150,000 per song and cessation of further AI training on their catalogs.
Behind the news:Although major music companies have a history oftakingactionagainst AI companies, music streamers, and musicians who distributed generated likenesses of music they owned, they’re also working with AI startups on their own terms. For instance, UMG iscollaboratingwith voice-cloning startup Soundlabs to create authorized synthetic voices of UMG artists. UMG, Sony, and Warner are alsonegotiatingwith YouTube to license music for a song generator to be launched this year.
Why it matters:As in similar lawsuits that involve text generators, the outcome of these actions could have an important impact on AI developers and users alike. Copyright law in the United States (and many other countries) does not address whether training AI models on copyrighted materials is a use that requires permission from copyright owners. In lieu of further legislation that answers the question, courts will decide. Assuming these cases go to trial, a verdict in favor of Suno or Udio would set a precedent that copyright doesn’t necessarily protect copyrighted works from AI training. Conversely, a verdict in favor of the music industry could restrict the use of copyrighted works in training, impeding a range of AI technologies that historically have been trained on data from the open internet.
We’re thinking:Copyright aims to prohibit unauthorized copying of intellectual property, but routine copying of data is built into the infrastructure of digital communications, never mind training AI systems. A web browser makes a temporary copy of every web page it displays, and web search engines typically copy the page they’ve indexed. It’s high time torevisecopyright law for the AI era in ways that create the most value for the most people.
",['https://dl-staging-website.ghost.io/content/images/2024/07/unnamed---2024-07-03T154036.152.gif']
U.S. to Probe AI Monopoly Concerns,"U.S. antitrust regulators are preparing to investigate a trio of AI giants.
What’s new:Two government agencies responsible for enforcing United States anti-monopoly laws agreed to investigate Microsoft, Nvidia, and OpenAI,The New York Timesreported.
How it works:The Department of Justice (DOJ) will investigate Nvidia, which dominates the market for chips that train and run neural networks. The Federal Trade Commission (FTC) will probe Microsoft and its relationship with OpenAI, which together control the distribution of OpenAI’s popular GPT-series models. In February, FTC chair Lina Khansaidthe agency would look for possible anti-competitive forces in the AI market.
Behind the news:Government attention to top AI companies is rising worldwide. Microsoft’s partnership with OpenAIfacesadditional scrutiny by European Union regulators, who are probing whether the relationship violates EU regulations that govern corporate mergers. U.K. regulators areinvestigatingAmazon’s relationship with Anthropic and Microsoft’s relationship with Mistral and Inflection AI. Last year, French regulatorsraidedan Nvidia office over suspected anti-competitive practices. In 2022, Nvidiawithdrewa bid to acquire chip designer Arm Holdings after the proposal attracted international regulatory scrutiny including an FTC lawsuit.
Why it matters:Microsoft, Nvidia, and OpenAI have put tens of billions of dollars each into the AI market, and lawsuits, settlements, judgments, or other interventions could shape the fate of those investments. The FTC and DOJ similarlydividedtheir jurisdictions in 2019, resulting in investigations into — and ongoing lawsuits against — Amazon, Apple, Google, and Meta for alleged anti-competitive practices in search, social media, and consumer electronics. Their inquiries into the AI market could have similar impacts.
We’re thinking:Governments must limit unfair corporate behavior without stifling legitimate activities. Recently, in the U.S. and Europe, the pendulum has swung toward overly aggressive enforcement. For example, government opposition to Adobe’s purchase of Figma had a chilling effect on acquisitions that seems likely to hurt startups. The UK blocked Meta’s acquisition of Giphy, which didn’t seem especially anticompetitive. We appreciate antitrust regulators’ efforts to create a level playing field, and we hope they’ll take a balanced approach to antitrust.
",['https://dl-staging-website.ghost.io/content/images/2024/06/unnamed---2024-06-26T152037.544.gif']
Chatbot for Minority Languages,"An AI startup that aims to crack markets in southern Asia launched a multilingual competitor to GPT-4.
What’s new:The company known as Two AIoffersSUTRA, a low-cost language model built to be proficient in more than 30 languages, including underserved South Asian languages like Gujarati, Marathi, Tamil, and Telugu. The company also launchedChatSUTRA, a free-to-use web chatbot based on the model.
How it works:SUTRA comprises two mixture-of-experts transformers: a concept model and an encoder-decoder for translation. Apaperincludes some technical details, but certain details and a description of how the system fits together are either absent or ambiguous.
Results:Onmultilingual MMLU(a machine-translated version of multiple-choice questions that cover a wide variety of disciplines), SUTRA outperformed GPT-4 in four of the 11 languages for which the developer reported the results: Gujarati, Marathi, Tamil, and Telugu. Moreover, SUTRA’s tokenizer is highly efficient, making the model fast and cost-effective. In key languages, it compares favorably to the tokenizer used with GPT-3.5 and GPT-4, and even narrowly outperforms GPT-4o’s improved tokenizer, according to Two AI’s tokenizer comparisonspaceon HuggingFace. In languages such as Hindi and Korean that are written in non-Latin scripts and for which GPT-4 performs better on MMLU, SUTRA’s tokenizer generates less than half as many tokens as the one used with GPT-3.5 and GPT-4, and slightly fewer than GPT-4o’s tokenizer.Yes, but:Multilingual MMLU tests only 11 of SUTRA’s 33 languages, making it difficult to fully evaluate the model’s multilingual performance.
Behind the news:Two AI was founded in 2021 by Pranav Mistry, former president and CEO of Samsung Technology & Advanced Research Labs. The startup has offices in California, South Korea, and India. In 2022, it raised $20 million in seed funding from Indian telecommunications firm Jio and South Korean internet firm Naver. Mistry aims to focus on predominantly non-English-speaking markets such as India, South Korea, Japan, and the Middle East, hetoldAnalytics India.
Why it matters:Many top models work in a variety of languages, but from a practical standpoint, multilingual models remain a frontier in natural language processing. Although SUTRA doesn’t match GPT-4 in all the languages reported, its low price and comparatively high performance may make it appealing in South Asian markets, especially rural areas where people are less likely to speak English. The languages in which SUTRA excels are spoken by tens of millions of people, and they’re the most widely spoken languages in their respective regions. Users in these places have yet to experience GPT-4-level performance in their native tongues.We’re thinking:Can a newcomer like Two AI compete with OpenAI? If SUTRA continues to improve, or if it can maintain its cost-effective service, it may yet carve out a niche.
",['https://dl-staging-website.ghost.io/content/images/2024/06/unnamed--65-.jpg']
Conversing With the Departed,"Advances in video generation have spawned a market for lifelike avatars of deceased loved ones.
What’s new:Several companies in China produce interactive videos that enable customers to chat with animated likenesses of dead friends and relatives,MIT Technology Reviewreported.
How it works:Super BrainandSilicon Intelligencehave built such models for several thousand customers. They provide a modern equivalent of portrait photos of deceased relatives and a vivid way to commune with ancestors.
Behind the news:The desire to interact with the dead in the form of an AI-generated avatar is neither new nor limited to China. In the U.S., the startup HereAfter AIbuildschatbots that mimic the deceased based on interviews conducted while they were alive. Another startup, StoryFile, markets similar capabilities to elders (pitchedby 93-year-oldStar Trekstar William Shatner) to keep their memory alive for younger family members. The chatbot appReplikabegan as a project by founder Eugenia Kuyda to virtually resurrect a friend who perished in a car accident in 2015.
Yes, but:In China, language models struggle with the variety of dialects spoken by many elders.
Why it matters:Virtualnewscastersandinfluencersare increasingly visible on the web, but the technology has more poignant uses. People long to feel close to loved ones who are no longer present. AI can foster that sense of closeness and rapport, helping to fulfill a deep need to remember, honor, and consult the dead.
We’re thinking:No doubt, virtual avatars of the dead can bring comfort to the bereaved. But they also bring the risk that providers might manipulate their customers’ emotional attachments for profit. We urge developers to focus on strengthening relationships among living family and friends.
",['https://dl-staging-website.ghost.io/content/images/2024/06/unnamed---2024-06-26T152214.754.gif']
More New Open Models,"A trio of powerful open and semi-open models give developers new options for both text and image generation.
What’s new:Nvidia and Alibaba released high-performance large language models (LLMs), while Stability AI released a slimmed-down version of its flagship text-to-image generator.How it works:The weights for Nvidia’s and Alibaba’s new models are fully open, while Stability AI’s are restricted.
Why it matters:AI models that come with published weights are proliferating, and this week’s crop further extends the opportunity to build competitive AI applications. Nemotron-4 340B provides an exceptionally large model among open LLMs. Among smaller models, Qwen2-72B poses stiff competition for Llama 3-70B, which has energized the developer community since its May release. And Stable Diffusion 3 puts Stability AI’s image generation technology into the hands of developers working on edge devices.
We’re thinking:Given the difficulty of acquiring high-quality data to train LLMs, and that the terms of service for many leading models prohibit generating data to train other models, Nvidia’s choice to equip Nemotron-4 to generate synthetic data is especially welcome. And it makes sense from a business perspective: Making it easier for developers to train their own LLMs may be good for GPU sales.
",['https://dl-staging-website.ghost.io/content/images/2024/06/unnamed---2024-06-19T154339.904.gif']
Private Benchmarks for Fairer Tests,"Scale AI offers new leaderboards based on its own benchmarks.
What’s new:Scale AI, which helps companies prepare and manage training data,introducedthe Safety, Evaluations and Alignment Lab (SEAL) Leaderboards. Four leaderboards test models’ abilities to (i) generate code, (ii) work on Spanish-language inputs and outputs, (iii) follow detailed instructions, and (iv) solve fifth-grade math problems. The company currently tests 11 models from Anthropic, Google, Meta, Mistral, and OpenAI. Developers who want to have their model ranked can contact Scale AI via email.How it works:The leaderboards track performance on proprietary datasets of roughly 1,000 examples. In all but the math tests, models to be evaluated are grouped and pitted against each other. Each pair receives 50 prompts at a time. Human annotators evaluate the models’ responses and grade which was superior and by how much. Then the models receive another 50 prompts. Models are ranked using a variation on Elo, which scores competitors relative to each other. To keep the test sets from leaking, a given model will be tested only once except in “exceptional cases” where Scale AI believes the risk of overfitting is low.
Results:As of this writing, GPT-4 Turbo tops the Coding leaderboard with GPT-4o a very close second. GPT-4o tops the Spanish and Instruction Following leaderboards, just ahead of Gemini 1.5 Pro in Spanish and GPT-4 Turbo in Instruction Following. On the Math leaderboard, Claude 3 Opus holds a narrow lead over GPT-4 Turbo (second) and GPT-4o (third).
Behind the news:As more models are trained on data scraped from the web, leakage of test data into training sets has made it more difficult to evaluate their performance on common benchmarks. Earlier this year, researchers at Shanghai Jiao Tong Universityevaluated31 open-source large language models and found that several had a high probability of inaccurate benchmark results due to data leakage. Scale AI built the GSM1k math dataset partly to show that some high-profile language models show evidence of overfitting to the common math benchmark GSM8k.
Why it matters:Traditionally, benchmarks have been open source efforts. But proprietary benchmarks are emerging to help developers evaluate their models and applications with greater confidence. By keeping their datasets under wraps, companies like Scale AI andVals AIensure that models haven’t been exposed to test questions and answers previously, making evaluations more reliable. However, private benchmarks lack the transparency of their open counterparts. A mix of public, private, and internal evals may be necessary to get a well rounded picture of a given model’s capabilities.We’re thinking:We welcome Scale AI’s contribution to the important field ofevals, which also includes open benchmarks,LMSYS Chatbot Arena, andHELM.
",['https://dl-staging-website.ghost.io/content/images/2024/06/unnamed---2024-06-19T154435.801.gif']
From Clip to Composition,"Is your song’s verse in need of a chorus? A popular text-to-music generator can extend existing recordings while maintaining their musical character.
What’s new:Paying users of Udio, a web service that generates pop-song productions from prompts, canuploadaudio clips and extend or alter them according to a text description. The service also increased its context window from 30 seconds to 2 minutes for more coherent output. You can hear the new capabilityhere. Subscriptions start at $10 per month.How it works:Given a prompt, Udio generates a 30-second passage and lets you assemble passages into compositions (previously up to four minutes long, now 15 minutes). Now users can create passages by uploading audio clips and extending them or modifying them by, say, adding or removing instruments or vocals complete with lyrics.
Behind the news:Udio competes withSuno, whose service also generates audio output with vocals, lyrics, and song structures. Also in the mix is Stability AI, whoseStable Audio 2.0enables users to upload and extend brief instrumental recordings to a length of around three minutes.
Why it matters:Udio is quickly becoming not just a song generator, but a song editor and builder. Just as the ability of text-to-image generators to edit, extend, and infill existing images made those applications more useful in a variety of creative situations, Udio’s audio-to-audio capabilities give composers and producers new horizons for enhancing, orchestrating, and structuring their own productions.
We’re thinking:Udio offers impressive capabilities for musicians (and wanna-be musicians), but its developer tools are lacking. A public-facing API would enable producers to automate the service and integrate it with other applications.
",['https://dl-staging-website.ghost.io/content/images/2024/06/unnamed---2024-06-19T154515.714.gif']
Apple’s Gen AI Strategy Revealed,"Apple presented its plan to imbue its phones and computers with artificial intelligence.What’s new:AppleannouncedApple Intelligence, a plethora of generative-AI features that integrate with iOS 18, iPadOS 18, and MacOS Sequoia. The beta version of Apple Intelligence will be available in U.S. English prior to a wider rollout near the end of the year, starting with the iPhone 15 Pro and Mac computers that useM-serieschips.On-device and in the cloud:The new capabilities rely on a suite of language and vision models. Many of the models will run on-device, while workloads that require more processing power will run on a cloud powered by Apple chips.
How it works:Appleoutlinedthe architecture that underpins the new features and compared two models of its against competitors.
Behind the news:While rivals like Microsoft and Google dove into generative AI, Applemovedmore cautiously. During the 2010s, it invested heavily in its Siri voice assistant, but the technology wasoutpacedby subsequent developments. Since then, the famously secretive company has been perceived asfalling behindbig-tech rivals in AI.Why it matters:While Apple’s big-tech competitors have largely put their AI cards on the table, Apple has held back. Now its strategy is on display: Proprietary foundation models, LoRA to fine-tune them to specific tasks, emphasis on the user experience over raw productivity, judicious use of edge and cloud computing, and deals with other model makers, all wrapped up in substantial privacy protections.We’re thinking:Apple’s control over its product ecosystem gives the company an extraordinary distribution channel. That’s why Google reportedlypaidApple $20 billion in 2022 to provide the default search engine in Apple’s Safari web browser. This advantage means that, whatever its pace of development and strategy in AI, Apple’s competitive edge remains sharp.
",['https://dl-staging-website.ghost.io/content/images/2024/06/STABLEAUDIOOPEN.png']
Audio Generation Clear of Copyrights,"Sonically minded developers gained a high-profile text-to-audio generator.
What’s new:Stability AIreleasedStable Audio Open, which takes text prompts and generates 16kHz-resolution music or sound effects. The model’s code and weights are available for noncommercial use. You can listen to a few sample outputshere.How it works:Stability AI promotes Stable Audio Open for generating not full productions but elements that will be assembled into productions. Although it’s similar to the earlierStable Audio 2.0, it has important differences.
Behind the news:Stable Audio Open competes not only with Stable Audio 2.0 but also with a handful of recent models. ElevenLabs,knownfor voice cloning and generation,introducedSound Effects, which generates brief sound effects from a text prompt. Users can input up to 10,000 prompt characters with a free account. For music generation,Udio and Sunooffer web-based systems that take text prompts and generate structured compositions including songs with lyrics, voices, and full instrumentation. Users can generate a handful of compositions daily for free.
Why it matters:Stable Audio Open is pretrained on both music and sound effects, and it can be fine-tuned and otherwise modified. The fact that its training data was copyright-free guarantees that users won’t make use of proprietary sounds — a suitable option for those who prefer to steer clear of the music industry’s brewing intellectual propertydisputes.We’re thinking:We welcome Stability AI’s latest contribution, but we don’t consider it open source. Its license doesn’t permit commercial use and thus, as far as we know, doesn’t meet the definition established by theOpen Source Initiative. We urge the AI community toward greater clarity and consistency with respect to the term “open source.”
",['https://dl-staging-website.ghost.io/content/images/2024/06/unnamed---2024-06-12T145349.047.png']
Seoul AI Summit Spurs Safety Agreements,"At meetings in Seoul, government and corporate officials from dozens of countries agreed to take action on AI safety.What’s new:Attendees at the AI Seoul Summit and AI Global Forum, both held concurrently in Seoul, formalized the broad-strokes agreements to govern AI,The Guardianreported. Presented as a sequel to November’s AI summit in Bletchley Park outside of London, the meetings yielded several multinational declarations and commitments from major tech firms.
International commitments:Government officials hammered out frameworks for promoting innovation while managing risk.
Corporate commitments:AI companies agreed to monitor their own work and collaborate on further measures.
Behind the news:Co-hosted by the UK and South Korean governments at the Korea Advanced Institute of Science and Technology, the meeting followed an initial summit held at Bletchley Park outside London in November. The earlier summitfacilitatedagreements to create AIsafetyinstitutes, test AI products before public release, and create an international panel akin to theIntergovernmental Panel on Climate Changeto draft reports on the state of AI. The panelpublishedan interim report in May. It will release its final report at thenext summitin Paris in November 2024.
Why it matters:There was a chance that the Bletchley Park summit would be a one-off. The fact that a second meeting occurred is a sign that public and private interests alike want at least a seat at the table in discussions of AI safety. Much work remains to define terms and establish protocols, but plans for future summits indicate a clear appetite for further cooperation.
We’re thinking:Andrew Ng spoke at the AI Global Forum on the importance ofregulating applications rather than technologyand chatted with many government leaders there. Discussions focused at least as much on promoting innovation as mitigating hypothetical risks. While some large companies continued to lobby for safety measures that would unnecessarily impede dissemination of cutting-edge foundation models and hamper open-source and smaller competitors, most government leaders seemed to give little credence to science-fiction risks, such as AI takeover, and express concern about concrete, harmful applications like the use of AI to interfere with democratic elections. These are encouraging shifts!
",['https://dl-staging-website.ghost.io/content/images/2024/06/unnamed---2024-06-12T145439.840.gif']
Rise of the AI PC,"Generative AI plays a starring role in the latest Windows PCs.
What’s new:Microsoftintroducedits Copilot+ PCs, an AI-first laptop specification that offers features unavailable to other Windows users. Copilot+ PCs will be available from Microsoft as well as Acer, Asus, Dell, HP, Lenovo, and Samsung starting in mid-June.How it works:Copilot+ PCs provide AI-powered generative and search functions thanks to unnamed AI models that run on-device.
Nvidia’s rejoinder:Nvidiaplansto launch Copilot+-compatible RTX AI PCs that run Nvidia’s owntoolkitfor calling and customizing models with on-device GPUs. These computers, initially built by Asus and MSI based on AMD CPUs, eventually will deliver all Copilot+ features. NvidiacriticizedMicrosoft’s NPU specification, which calls for 45 trillion operations per second (TOPS), claiming that that speed is enough to process only basic AI workloads. Meanwhile, Nvidia’s game-focused GPUsdelivermore than 1,000 TOPS.
Why it matters:Microsoft is betting that on-device AI will change the PC experience. The Copilot+ PC specification gives developers a versatile toolkit for adding AI to existing apps while opening the door to fundamentally new functionality like Recall.
We’re thinking:As wewroteearlier, makers of chips and operating systems alike have a strong incentive to promote on-device (or edge) AI. The growing presence of AI accelerators in consumer devices brings significant privacy benefits for consumers and opens exciting new opportunities for developers.
",['https://dl-staging-website.ghost.io/content/images/2024/06/unnamed---2024-06-05T180448.900.png']
Disinformation Documented,"OpenAI models were used in five disinformation campaigns, the company said.
What’s new:OpenAIdiscoveredthat operations based in Russia, China, Iran, and Israel had used the company’s models to create and/or revise text in attempts to influence international political opinion. The generated media failed to reach a mass audience, the company said. It banned the accounts.
How it works:Most of the groups primarily used OpenAI’s language models to generate inauthentic social media comments for posting on dummy accounts intended to create the illusion of popular support for certain causes. Some groups used the company’s models to debug code, generate text for websites, and produce images such as political cartoons. Four of the five groups already were known to disinformation researchers.
Behind the news:AI-produced misinformation on the internet — mostly images, videos, and audio clips — rose sharply starting in the first half of 2023, researchfoundat Google and several fact-checking organizations. By the end of that year, generative AI was responsible for more than 30 percent of media that was manipulated by computers.Why it matters:Many observers are concerned about potential proliferation of political disinformation as AI models that generate realistic text, images, video, and audio become widely available. This year will see elections in at least 64 countries including most of the world’s most populous nations — a rich opportunity for AI-savvy propagandists. While propagandists have taken advantage of OpenAI’s models, the company was able to detect them and shut them down. More such efforts are bound to follow.
We’re thinking:Generative AI’s potential to fuel propaganda is worth tracking and studying. But it’s also worth noting that the accounts identified by OpenAI failed to reach significant numbers of viewers or otherwise have an impact. So far, at least, distribution, not generation, continues to be the limiting factor on disinformation.
",['https://dl-staging-website.ghost.io/content/images/2024/06/unnamed---2024-06-05T180527.254.png']
U.S. and China Seek AI Agreement,"The United States and China opened a dialogue to avert hypothetical AI catastrophes.What’s new:Officials of the two nations met in Geneva for an initial conversation intended to prevent AI-driven accidents or worse,The Washington Postreported.
How it works:The meeting followed up on a November meeting between U.S. president Joe Biden and Chinese president Xi Jinping. The discussion was conceived as an opportunity for the nuclear-armed superpowers, both of which have pegged their strategic ambitions to AI technology, to air their concerns. It resulted in no public statements about concrete actions or commitments.
Behind the news:AI-related tensions between the two countries have intensified in recent years. The U.S. government, in an effort to maintain its technological advantage and hamper China’s AI development, hasimposedcontrols on the export of specialized AI chips like the Nvidia A100 and H100 to Chinese customers. Restrictions on the development of models that bear on U.S. national security mayfollowif further proposed export controls are enacted. Such controls haverankledthe Chinese government. Meanwhile,bothcountrieshave developed and deployed autonomous military vehicles, and autonomous weapons areproliferating. In November 2023, both countries signed the Bletchley Parkdeclarationto mitigate AI-related risks including cybersecurity, biotechnology, and misinformation.What they’re saying:“The real verdict on whether these talks were successful will be whether they continue into the future.” — Helen Toner, analyst at Georgetown University’s Center for Security and Emerging Technology and former OpenAI board member,quotedby Associated Press.
Why it matters:Officials and observers alike worry that rivalry between the U.S. and China may lead to severe consequences. However, just as thered telephoneenabled U.S. and Soviet leaders to communicate during emergencies in the Cold War, face-to-face dialogue can help bring the two countries into alignment around AI-related risks and ways to reduce them.
We’re thinking:We support harmonious relations between the U.S. and China, but we’re deeply concerned that export controls could stifle open source software. This might slow down China’s progress in AI, but would also hurt the U.S. and its allies.
",['https://dl-staging-website.ghost.io/content/images/2024/06/REASONv2-2.gif']
Heart-Risk Model Saves Lives,"A deep learning model significantly reduced deaths among critically ill hospital patients.
What’s new:A system built by Chin-Sheng Lin and colleagues at Taiwan’s National Defense Medical Center analyzed patients’ heart signals and alerted physicians if it detected a high risk of death. Itreduceddeaths of high-risk patients by 31 percent in a randomized clinical trial.
How it works:Researcherstraineda convolutional neural network, given an electrocardiogram (a measurement of the heart’s electrical activity), toestimatea risk score. The system compares a patient’s risk score against those of other patients. Scores that rank in the 95th percentile or higher are considered high risk of death within 90 days.
Results:8.6 percent of patients in the control group and 8.9 percent of patients in the experimental group raised a high-risk alert during the trial. In the experimental group, 16 percent of high-risk patients died; in the control group, 23 percent of high-risk patients died. Overall, in the experimental group, 3.6 percent of patients died; in the control group, 4.3 percent of patients died. The model was trained to predict mortality from all causes, but it showed unusually strong predictive capability for heart-related deaths. Examining causes of death, the authors found that 0.2 percent of patients in the experimental group died from heart-related conditions such as cardiac arrest versus 2.4 percent in the control group.Behind the news:Hospitals use AI-powered alert systems toidentifypatients in need of urgent medical attention. Such systems monitor emergency room patients for sepsis, predict whether those patients need intensive care, and predict the risk that discharged patients will require further care. They help hospitals to allocate resources by directing attention where it’s needed most urgently.Why it matters:It’s rare for any kind of medical intervention to reduce mortality in a subgroup by 31 percent. The authors speculate that the system not only helped direct attention to patients urgently in need of attention but also may have identified electrocardiogram features that doctors typically either don’t understand well or can’t detect.
We’re thinking:This relatively low-cost AI system unambiguously saved lives over three months at different hospitals! We look forward to seeing it scale up.
",['https://dl-staging-website.ghost.io/content/images/2024/05/unnamed---2024-05-29T152721.294.gif']
Self-Driving on Indian Roads,"Few makers of self-driving cars have braved the streets of India. Native startups are filling the gap.
What’s new:Indian developers are testing autonomous vehicles on their nation’s disorderly local roads. To cope with turbulent traffic, their systems use different technology from their Western and East Asian counterparts,IEEE Spectrumreported.
How it works:In Indian cities, two-, three-, and four-wheelers share the road with trucks, pedestrians, and animals. Drivers often contend with debris and potholes, and many don’t follow rules. These conditions demand vehicles outfitted with technology that’s more flexible (and less expensive) than the interwoven sensors, models, and 3D maps employed by self-driving cars designed for driving conditions like those found in the United States.
Behind the news:Bringing self-driving cars to India has political as well as technical dimensions. Many Indians hire full-time drivers, and the country’s minister of roads and highways hasresistedapproving the technology because of its potential impact on those jobs. Drivers cost as little as $150 per month, which puts self-driving car makers under pressure to keep their prices very low. Moreover, India’s government insists that vehicles sold there must be manufactured locally, posing a barrier to foreign makers of self-driving cars.
Why it matters:Rather than starting with an assumption that traffic follows orderly patterns with many edge cases, Indian developers assume that traffic is essentially unpredictable. For them, events that most developers would consider outliers — vehicles approaching in the wrong lanes, drivers who routinely play chicken, domestic animals in the way — are common. This attitude is leading them to develop robust self-driving systems that not only may be better suited to driving in complex environments but also may respond well to a broader range of conditions.
We’re thinking:Former Uber CEO Travis Kalanicksaidthat India would be “the last one” to get autonomous cars. These developers may well prove him wrong!
",['https://dl-staging-website.ghost.io/content/images/2024/05/unnamed---2024-05-29T152833.184.gif']
Knowledge Workers Embrace AI,"AI could offer paths to promotion and relief from busywork for many knowledge workers.
What’s new:75 percent of knowledge workers worldwide use AI even if they need to supply their own tools, according tosurveyconducted by Microsoft and Linkedin.
How it works:The authors questioned 3,800 workers in 31 countries throughout the Americas, Europe, Asia, and Australia, asking whether and how they used consumer-grade generative systems like Microsoft Copilot and OpenAI ChatGPT. Majorities of all age groups used AI at work, including 85 percent of respondents 28 or younger and 73 percent of those 58 or older.
Behind the news:The survey results agree with those of other studies of AI’s impact on the workplace. In January, the International Monetary Fundprojectedthat AI would affect 40 percent of all jobs worldwide (either complementing or replacing them), including 60 percent of jobs in countries like the UK and U.S. that have greater percentages of knowledge workers. A 2023 research paperarguedthat white-collar occupations were most likely to be affected by generative AI, in contrast to previous waves of automation that primarily affected blue-collar jobs. Automation driven by AI increased overall employment, evidence gathered by the European Central Bankshows.
Why it matters:AI is transforming work from the bottom up. Executives and managers want employees who know how to use the technology, but only 39 percent of the people who already do so received training from their employers. Company-wide encouragement to experiment with and take advantage of AI leads to the best outcomes.
We’re thinking:Knowing how to use AI tools is a plus in the current job market. Knowing how to build applications using AI opens another world of doors.
",['https://dl-staging-website.ghost.io/content/images/2024/06/RAPTORv2-1.gif']
"Faster, Cheaper Multimodality","OpenAI’s latest model raises the bar for models that can work with common media types in any combination.What’s new:OpenAIintroducedGPT-4o, a model that accepts and generates text, images, audio, and video — the “o” is for omni — more quickly, inexpensively, and in some cases more accurately than its predecessors. Text and image input and text-only output are available currently via ChatGPT and API, with image output coming soon. Speech input and output will roll out to paying users in coming weeks. General audio and video will be available first to partners before rolling out more broadly.
How it works:GPT-4o is a single model trained on multiple media types, which enables it to process different media types and relationships between them faster and more accurately than earlier GPT-4 versions that use separate models to process different media types. The context length is 128,000 tokens, equal to GPT-4 Turbo but well below the 2-million limit newly set by Google Gemini 1.5 Pro.
GPT-4o significantly outperforms Gemini Pro 1.5 at several benchmarks for understanding text, code, and images includingMMLU,HumanEval,MMMU, andDocVQA. It outperformed OpenAI’s ownWhisper-large-v3speech recognition model at speech-to-text conversion andCoVoST 2language translation.
Aftershocks:As OpenAI launched the new model,troublesresurfaced that had led to November’s rapid-fire ouster and reinstatement of CEO Sam Altman. Co-founder and chief scientist Ilya Sutskever, who co-led a team that focused on mitigating long-term risks, resigned. He did not give a reason for his departure; previously he hadarguedthat Altman didn’t prioritize safety sufficiently. The team’s other co-leader Jan Leike followed,allegingthat the company had a weak commitment to safety. The company promptlydissolvedthe team altogether and redistributed its responsibilities. Potential legal issues also flared when actress Scarlett Johansson, who had declined an invitation to supply her voice for a new OpenAI model, issued astatementsaying that one of GPT-4o’s voices sounded “eerily” like her own and demanding to know how the artificial voice was built. OpenAI denied that it had used or tried to imitate Johansson’s voice and withdrew that voice option.
Why it matters:Competition between the major AI companies is putting more powerful models in the hands of developers and users at a dizzying pace. GPT-4o shows the value of end-to-end modeling for multimodal inputs and outputs, leading to significant steps forward in performance, speed, and cost. Faster, cheaper processing of tokens makes the model more responsive and lowers the barrier for powerful agentic workflows, while tighter integration between processing of text, images, and audio makes multimodal applications more practical.
We’re thinking:Between GPT-4o, Google’s Gemini 1.5, and Meta’s newly announcedChameleon, the latest models are media omnivores. We’re excited to see what creative applications developers build as the set of tasks such models can perform continues to expand!
",['https://dl-staging-website.ghost.io/content/images/2024/05/unnamed---2024-05-22T145823.262.gif']
2 Million Tokens of Context & More,"Google’s annual I/O developers’ conference brought a plethora of updates and new models.What’s new:Googleannouncedimprovements to its Gemini 1.5 Pro large multimodal model — notably increasing its already huge input context window — as well as new open models, a video generator, and a further step in digital assistants. In addition, Gemini models will power new features inGoogle Search, Gmail, and Android.
How it works:Google launched a variety of new capabilities.
Precautionary measures:Amid the flurry of new developments, Googlepublishedprotocols for evaluating safety risks. The “Frontier Safety Framework” establishes risk thresholds such as a model’s ability to extend its own capabilities, enable a non-expert to develop a potent biothreat, or automate a cyberattack. While models are in development, researchers will evaluate them continually to determine whether they are approaching any of these thresholds. If so, developers will make a plan to mitigate the risk. Google aims to implement the framework by early 2025.
Why it matters:Gemini 1.5 Pro’s expanded context window enables developers to apply generative AI to multimedia files and archives that are beyond the capacity of other models currently available — corporate archives, legal testimony, feature films, shelves of books — and supports prompting strategies such asmany-shot learning. Beyond that, the new releases address a variety of developer needs and preferences: Gemini 1.5 Flash offers a lightweight alternative where speed or cost is at a premium, Veo appears to be a worthy competitor for OpenAI’s Sora, and the new open models give developers powerful options.
We’re thinking:Google’s quick iteration on its Gemini models is impressive. Gemini 1.0 wasannouncedless than six months ago. White-hot competition among AI companies is giving developers more choices, faster speeds, and lower prices.
In our new short course “Introduction to On-Device AI,” made in collaboration with Qualcomm, you’ll learn to deploy AI models on edge devices using local compute for faster inference and privacy. Join the next wave of AI as models go beyond the cloud!Enroll for free
","['https://dl-staging-website.ghost.io/content/images/2024/05/V3_DeepLearning_Qualcomm_C1_Banner_2070x1080--1-.png', 'https://dl-staging-website.ghost.io/content/images/2024/05/unnamed---2024-05-22T150014.575.gif']"
Music Titan Targets AI,"The world’s second-largest music publisher accused AI developers of potential copyright violations.What’s new:Sony Music Groupdeclaredthat AI developers had trained models on Sony’s intellectual property without permission and that any method of collecting media or other data owned by the company violated its copyrights. Whether AI developers actually have violated copyrights has not been established.
How it works:In astatementposted on the company’s website andlettersto developers, Sony forbade the use of its music or other media such as lyrics, music videos, album art for “training, developing, or commercializing any AI systems.”
Behind the news:In April, more than 200 music artistscalledfor streaming services and AI developers to stop using their work for training and stop generating music in the styles of specific musicians without compensation. Universal Music Group (UMG), which is Sony Music’s top competitor, has also opposed unrestricted AI-generated music.
Last year, UMGorderedApple Music and Spotify to block AI developers from downloading its recordings and issued takedown notices to YouTube and Spotify uploaders who generated music that sounds like artists who are under contract to Universal.
Why it matters:Sony Music Group’s warning comes as generated audio isapproachinga level of quality that might attract a mainstream audience, and it could chill further progress. Although it is not yet clear whether training AI systems on music recordings without permission violates copyrights, Sony Music Group hasdemonstratedits willingness to pursue both individuals and companies for alleged copyright violations. The company accounted for 22 percent of the global music market in 2023. (UMG accounted for 32 percent.) Its catalog includes many of the world’s most popular artists including AC/DC, Adele, Celine Dion, and Harry Styles.
We’re thinking:We believe that AI developers should be allowed to let their software learn from data that’s freely available on the internet, but uncertainty over the limits of copyright protection isn’t good for anyone. It’s high time toupdateto intellectual property laws for the era of generative AI.
",['https://dl-staging-website.ghost.io/content/images/2024/05/unnamed---2024-05-22T150103.924.gif']
Interpreting Image Edit Instructions,"The latest text-to-image generators can alter images in response to a text prompt, but their outputs often don’t accurately reflect the text. They do better if, in addition to a prompt, they’re told the general type of alteration they’re expected to make.What’s new:Developed by Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar and colleagues at Meta,Emu Editenriches prompts with task classifications that help the model interpret instructions for altering images. You can see exampleshere.Key insight:Typical training datasets for image-editing models tend to present, for each example, an initial image, an instruction for altering it, and a target image. To train a model to interpret instructions in light of the type of task it describes, the authors further labeled examples with a task. These labels included categories for regional alterations such as adding or removing an object or changing the background, global alterations such as changing an image’s style, and computer-vision tasks such as detecting or segmenting objects.How it works:Emu Edit comprises a pretrainedEmulatent diffusion image generator and pretrained/fine-tuned Flan-T5 large language model. The system generates a novel image given an image, text instruction, and one of 16 task designations. The authors generated the training set through a series of steps and fine-tuned the models on it.
Results:Judges compared altered images produced by the authors’ method,InstructPix2Pix, andMagicBrushusing the MagicBrush test set. Evaluating how well the generated images aligned with the instruction, 71.8 percent of the time, the judges preferred Emu Edit over InstructPix2Pix, and 59.5 percent of the time, they preferred Emu Edit over MagicBrush. Evaluating how well the generated images preserve elements from the input images, 71.6 percent preferred Emu Edit over InstructPix2Pix, and 60.4 percent preferred Emu Edit over MagicBrush.Why it matters:Richer data improves machine learning results. Specifying tasks and generating images that reflect them improved Emu Edit’s data compared to other works, enabling it to achieve better results.We’re thinking:Text-to-image generators are amazing and fun to use, but their output can be frustratingly unpredictable. It’s great to see innovations that make them more controllable.
Join FourthBrain's two live workshops next week! In these interactive sessions, you’ll build useful applications with large language models and walk away with practical skills. Enroll as an individual or register as a team for a group discount.Learn more
",['https://dl-staging-website.ghost.io/content/images/2024/05/FourthBrain-Batch-Ad-05222024.png']
Why ChatGPT Acts That Way,"OpenAI pulled back the curtain on revised rules that will guide its models.
What’s new:OpenAI published itsModel Spec, high-level guidelines for use by human labelers to steer model behavior. The company is inviting publiccommentson the spec until May 22. It has not stated whether or how it will incorporate comments.
How it works:During training, human labelers rate a model’s responses so it can be fine-tuned to conform with human preferences in the process known asreinforcement from human feedback(RLHF). The Model Spec outlines the principles — some new, some previously in use — that will drive those ratings. The principles are arranged hierarchically, and each category will override those below it.
Behind the news:OpenAI’s use of the Model Spec and RLHF contrasts withAnthropic’sConstitutional AI. To steer the behavior of Anthropic models, that company’s engineers define a constitution, or list of principles, such as “Please choose the response that is the most helpful, honest, and harmless” and “Do NOT choose responses that are toxic, racist, or sexist, or that encourage or support illegal, violent, or unethical behavior.” Rather than human feedback, Anthropic relies onAI feedbackto interpret behavioral principles and guide reinforcement learning.
Why it matters:AI developers require a degree of confidence that the models they use will behave as they expect and in their users’ best interests. OpenAI’s decision to subject its guidelines to public scrutiny could help to instill such confidence, and its solicitation of public comments might make its models more responsive to social and market forces.
We’re thinking:OpenAI’s openness with respect to its Model Spec is a welcome step toward improving its models’ safety and performance.
",['https://dl-staging-website.ghost.io/content/images/2024/05/unnamed---2024-05-15T164233.811.gif']
AlphaFold 3 Embraces All Biochemistry,"The latest update of DeepMind’s AlphaFold model is designed to find the structures of not just proteins but all biologically active molecules as well as interactions between them.
What’s new:GoogleannouncedAlphaFold 3, which models the 3D shapes of biomolecules including proteins, DNA, RNA, and ligands (molecules that bind to proteins or DNA, which includes antibodies and many drugs) in any combination. AlphaFold Serverprovidesaccess for noncommercial uses (with some limitations). Unlike earlier versions, AlphaFold 3 is not open source.Key insight:Given a sequence of amino acids (the building blocks of proteins), the previous version of AlphaFold drew on an existing knowledge of amino acid structures, computed their locations and angles, and assembled them like Lego blocks. To adapt the system for molecules that aren’t made of amino acids, AlphaFold 3 represents them as collections of individual atoms and uses a generative model to find their positions in space.How it works:Given a list of molecules, AlphaFold 3 generates their joint 3D structure, revealing how they fit together. Several transformers hone embeddings of proteins and amino acids, while a diffusion model (also a transformer) processes embeddings of atoms. The team trained the system on five datasets including ground truth protein, DNA, and RNA structures interactions in theProtein Data Bank. They also trained it on protein shapes computed by AlphaFold 2; that model’s explicit knowledge of amino acid structures helped overcome AlphaFold 3’s tendency to hallucinate in some instances. Among the key processes:
Results: OnPoseBusters, a database of protein and protein-molecule shapes, AlphaFold 3 successfully found the shapes of about 77 percent of examples, while AutoDock Vina (a non-learning program that models molecular interactions) achieved about 53 percent. On a Protein Data Bank evaluation set, AlphaFold 3 successfully found about 84 percent of protein shapes, while AlphaFold Multimer 2.3 (an update of AlphaFold 2) found 83 percent. Modeling protein-protein interactions, AlphaFold 3 achieved 77 percent, while AlphaFold Multimer 2.3 achieved 67 percent, according toDockQ(a metric for the quality of such interactions).Behind the news:The original AlphaFold solved one of the most challenging problems in molecular biology by figuring out how long chains of amino acids would fold, giving scientists clear targets for designing new bioactive molecules. Googlespun offIsomorphic Labs to apply AlphaFold 2 to drug discovery. That company will use AlphaFold 3 and control commercial access to it.Why it matters:AlphaFold 3 is a triumph of machine learning. It extends the utility of the previous version beyond proteins, and it computes with unprecedented accuracy how biological molecules will combine, allowing for a more comprehensive understanding of how drugs interact with the body. Its ability to predict how antibodies will bind to proteins could help stave off future pandemics and other illnesses.We’re thinking:Although Isomorphic Labs retains control of AlphaFold 3, biologistssaidthe information in the paper is enough for other researchers to develop similar systems. We look forward to open versions!
Learn to develop smarter search, retrieval augmented generation (RAG), and recommender systems for multimodal retrieval and generation in this short course, built in collaboration with Weaviate.Enroll today!
","['https://dl-staging-website.ghost.io/content/images/2024/05/The-Batch-ads-and-exclusive-banners---2024-05-14T170959.188.png', 'https://dl-staging-website.ghost.io/content/images/2024/05/unnamed---2024-05-15T164459.448.png']"
Building an AI Oasis,"Saudi Arabia plans to spend billions of dollars to become a global AI hub.
What's new:The desert kingdom has allocated $100 billion to invest in AI and other technologies,The New York Timesreported. The massive potential outlay is attracting AI giants and startups alike.
How it works:Saudi Arabia, whose economy is based on large reserves of oil, aims to channel its considerable wealth into more sustainable industries. AI is a major target.
Behind the news:Where AI is concerned, Saudi Arabia is competing with the neighboring United Arab Emirates (UAE). In March, UAE member state Abu Dhabiestablishedits own multibillion-dollar investment fund, MGX, which aims to secure deals in AI models, data centers, and semiconductors. One of MGX’s founding partners (and a cornerstone in the UAE’s AI efforts) is G42, a conglomerate with ties to the Emirati government that owns numerous AI research labs and other assets. G42 recentlyreceived$1.5 billion from Microsoft. Last year, itpaidU.S. chip designer Cerebras an initial $100 million to build up to nine supercomputers.Yes, but:Saudi investments have not always arrived on the expected schedule. Founders of startups that were promised GAIA funding havecomplainedof delays and nonpayments. Moreover, U.S. partners such as Microsoft have drawncriticismfor working with Saudi Arabia, which has beenaccusedof violating human rights. The U.S. governmentblockedfulfillment of the King Abdullah University’s purchase of Nvidia chips because it may help researchers associated with the Chinese military to circumvent U.S.restrictionson the export of advanced semiconductors. Earlier this year, U.S.-based generative AI startup Anthropicrejectedpotential investment from PIF citing national security concerns.
Why it matters:AI is fast becoming a source of national power, and many countries are eager to build their capabilities. Saudi Arabia’s investment could go a long way toward building facilities and talent in a part of the world that has not been known for high tech. For the country itself, it could bring economic growth and geopolitical advantage. For foreign companies and talent, it’s an immense new source of funding to pursue valuable projects and gain practical experience.
We're thinking:We are happy to see AI hubs emerge around the world, especially in places that can provide more opportunities for people who live outside of established AI centers.
",['https://dl-staging-website.ghost.io/content/images/2024/05/unnamed---2024-05-15T164549.473.gif']
Brain-Controlled Robots Get More Versatile,"Brain-to-computer interfaces that enable users to control robots with their thoughts typically execute a single type of task such as reaching and grasping. Researchers designed a system that responds to a variety of intentions.
What's new:Ruohan Zhang and colleagues at Stanford introducedNeural Signal Operated Intelligent Robots (NOIR). Their method commands a robot to perform practical tasks, such as ironing a cloth or making a sandwich, via signals from an electroencephalogram (EEG), a non-invasive way to measure brain waves via electrodes attached to the scalp.
Key insight:Currently neuroscientists can derive from EEG signals only simple thoughts, such as the intention to move a limb. However, a sequence of simple thoughts can drive an arbitrarily complex action. Specifically, simple thoughts (such as the intention to move a hand) can drive a robot to perform complex actions by repeatedly (i) selecting an object, (ii) selecting an action to apply to the object, and (iii) selecting the part of the object to act upon. For instance, to iron a cloth, the initial sequence would be: (i) select the iron and (ii) grasp it (iii) by the handle. This sequence might be followed by (i) select the cloth and (ii) slide the iron across it (iii) starting at the nearest portion. And so on.
How it works:Users who wore EEG electrodes concentrated on specific sequences of thoughts to execute tasks as they watched a screen that displayed the output of a camera attached to either arobotic armorwheeled robot with two arms.
Results:Three users controlled the two robots to execute 20 everyday tasks. On average, the system selected objects with 81.2 percent accuracy, actions with 42.2 percent accuracy, and locations with 73.9 percent accuracy. Users took an average of about 20 minutes to complete each task.
Why it matters:Brain signals are enormously complex, yet relatively simple statistical techniques — in this case, QDA — can decode them in useful ways.
We're thinking:Sometimes the simplest solution to a difficult problem is not to train a larger model but to break down the problem into manageable steps.
In “Multi AI Agent Systems with crewAI,” you’ll learn key principles for designing AI agents and organizing teams of agents to perform complex, multi-step tasks. You’ll apply these concepts to automate six common business processes.Sign up for free!
",['https://dl-staging-website.ghost.io/content/images/2024/05/The-Batch-ads-and-exclusive-banners---2024-05-14T171114.198.png']
Coding Assistance Start to Finish,"GitHub Copilot’s latest features are designed to help manage software development from plan to pull request.
What’s new:GitHubunveileda preview of Copilot Workspace, a generative development environment that’s designed to encompass entire projects. Users can sign up for awaitlistto gain access to Workspace until the preview ends. Afterward, Copilot Workspace will be available to subscribers to GitHub Copilot (which starts at $10 per month for individuals and $19 per month for businesses).
How it works:Copilot Workspace is based on GPT-4 Turbo and integrated with GitHub code repositories and libraries. Where GitHub Copilot previously generated code snippets and provided suggestions for editing code segments, Copilot Workspace integrates these tasks within a larger plan.
Yes, but:Initial usersnotedthat Copilot Workspace is best at solving straightforward, well defined problems and struggles with more complex ones. Choices can be difficult to unwind later on, and the system is slower than simpler AI coding assistants.
Behind the news:Generative coding assistants quickly have become central tools for software development. Copilot hasattracted1.3 million paid subscribers as of April 2024, including 50,000 businesses. Amazon’sQ Developer(formerly CodeWhisperer), Google’sGemini Code Assist(formerly Duet AI), andCursoroffer coding companions that integrate with or fork popular integrated development environments like Microsoft’s VSCode. On the frontier areagentic toolsthat plan and carry out complex, multi-step coding tasks.Why it matters:Copilot Workspace attempts to extend Copilot’s code-completion and chat capabilities to a wider swath of the software development cycle. Simpler coding assistants have been shown toboostproductivity markedly. Bringing natural-language prompting to tasks like planning, testing, and reading documentation is a natural step.
We’re thinking:There are many ways to use AI in coding. To learn about a few more, check out our short course, “Pair Programming With a Large Language Model,” taught by Google AI advocate Laurence Moroney.
",['https://dl-staging-website.ghost.io/content/images/2024/05/unnamed--58-.jpg']
OpenAI Licenses News Archives,"OpenAI has been making deals with publishers to gain access to high-quality training data. It addedFinancial Timesto the list.
What’s new:OpenAIlicensedthe archive of business news owned byFinancial Times(FT) for an undisclosed sum. The agreement lets OpenAI train its models on the publisher’s articles and deliver information gleaned from them. This is OpenAI’s fifth such agreement with major news publishers in the past year.
How it works:Although the parties didn’t disclose the length of their agreement, OpenAI’s other news licensing deals will end within a few years. The limited commitment suggests that these arrangements are experimental rather than strategic. The deal includes articles behind the publisher’s paywall; that is, not freely available on the open internet. This enables OpenAI to train its models on material that competitors may not have. Other deals have given OpenAI exclusive access, shutting competitors out.
Behind the news:Archives of news articles may be handy if OpenAI proceeds with a rumored search servicereportedby in February byThe Information. Licensing is a way to get such material that is unambiguously legal. Although AI researchers commonly scrape data from the web and use it for training models without obtaining licenses for copyrighted works, whether a license is required to train AI models on works under copyright in the U.S. has yet to be determined. Copyright owners lately have challenged this practice in court. In December 2023,The New York TimessuedOpenAI and Microsoft, claiming that OpenAI infringed its copyrights by training models on its articles. In April 2024, eight U.S. newspapers owned by Alden Global Capital, a hedge fund,fileda lawsuit against the same defendants on similar grounds. Licensing material from publishers gives OpenAI access to their works while offering them incentives to negotiate rather than sue.Why it matters:AI developers need huge amounts of media to train larger and larger models. News publishers have huge archives with high-quality text, relatively well written and fact-checked, that’s relevant to current events of interest to a broad audience. Licensing those archives gives developers access to what they need without incurring legal risk. Furthermore, making news archives available for retrieval augmented generation makes chatbots more capable and reliable.
We’re thinking:We support efforts to clarify the legal status of training AI models on data scraped from the web. It makes sense to treat the open web pages and paywalled content differently, but we advocate that AI models be free to learn from the open internet just as humans can.
Explore the newest additions to our short courses with “Quantization in Depth,” where you’ll build a quantizer in PyTorch, and “Building Agentic RAG with LlamaIndex,” which teaches how to build agents capable of tool use, reasoning, and making decisions based on your data.Sign up now!
","['https://dl-staging-website.ghost.io/content/images/2024/05/4--3-.png', 'https://dl-staging-website.ghost.io/content/images/2024/05/unnamed---2024-05-08T164752.068.png']"
Landmine Recognition,"An AI system is scouring battlefields for landmines and other unexploded ordnance, enabling specialists to defuse them.
What’s new:The military hardware firm Safe Pro Group developed Spotlight AI, a computer vision system that identifies mines based on aerial imagery,IEEE Spectrumreported. Nongovernmental organizations that remove landmines, including the Norwegian People's Aid and the HALO Trust, are using the system in Ukraine.
How it works:SpotlightAI processes visual-light imagery taken by flying drones. The system provides centimeter-resolution maps that guide mine-removal teams through the territory.
Behind the news:In addition to drones, satellites can help machine learning models to find deadly remnants of warfare. In 2020, Ohio State University researchersestimatedthe number of undetonated explosives in Cambodia by collating bomb craters in satellite images identified by a computer vision model with records of U.S. military bombing campaigns in that country in the 1960s and 1970s.
Why it matters:Unexploded mines, bombs, and other types of munitionskilled or injuredmore than 4,700 people — 85 percent of them civilians and half of them children where military status and age were known — in 2022 alone. Efforts to remove every last mine from a former battlefield likely will continue to rely on traditional methods — manual analysis of overhead imagery along with sweeps by human specialists and explosive-sniffing dogs — but machine learning can significantly reduce the hazard and accelerate the work.
We’re thinking:Although this system locates unexploded mines and shells, removing them often still falls to a brave human. We hope for speedy progress in robots that can take on this work as well.
",['https://dl-staging-website.ghost.io/content/images/2024/05/unnamed---2024-05-08T164933.555.gif']
Streamlined Inference,"It’s not necessary to activate all parts of a large language model to process a given input. Using only the necessary parts saves processing.
What’s new:Zichang Liu and collaborators at Rice University, Zhe Jiang University, Stanford, University of California San Diego, ETH Zürich, Adobe, Meta, and Carnegie Mellon proposedDeja Vu, an algorithm that accelerates inferencing of large language models (LLMs) by using small vanilla neural networks to predict which parts of it to use.
Key insight:Transformer-based neural networks can save a lot of time at inference by activating only a fraction of (i) attention heads and (ii) neurons in fully connected layers. But it’s necessary to activate the right neurons, because different parts of the network learn about different patterns of inputs. By using the input to decide which parts of the network to activate, the network can maintain accuracy using only the parts relevant for the current input.
How it works:The authors used pretrainedOPTmodels of various sizes (175, 66, and 30 billion parameters). They built a dataset by feeding examples fromOpenBookQAandWiki-Textto the OPTs and recording the outputs of all attention heads and fully-connected-layer neurons. By activating various portions of these networks, they learned that, for a given input, they could discard most of an OPT’s lowest-output attention heads and fully-connected-layer neurons without degrading its performance.
Results:Deja Vu (175 billion parameters) produced a sequence of 128 tokens in 20 milliseconds, while an Nvidia implementation of OPT of the same size needed 40 milliseconds and a Hugging Face implementation of OPT of the same size needed 105 milliseconds. Moreover, Deja Vu achieved these speedups without reducing accuracy. OnWikiTextandC4, Deja Vu’s ability to predict the next word held steady while activating 25 percent of attention heads and fully-connected-layer neurons. On datasets such asWinoGrandeandOpenBookQA, it maintained its accuracy while activating 35 percent of attention heads and fully-connected-layer neurons.
Why it matters:Efficient use of processing power becomes increasingly important as models become larger. Moreover, faster token generation benefits agentic workflows, which can consume large numbers of tokens.
We’re thinking:Deja Vu’s design is in the spirit of the mixture of experts (MoE) architecture: For each transformer layer, MoE uses a neural-network layer to choose which fully connected layer to use. In contrast, for each attention head and fully-connected-layer neuron, Deja Vu uses small neural networks to decide which to activate.
New course with Qualcomm coming soon! In “Introduction to On-Device AI,” you’ll learn how to deploy AI models on edge devices using local computation for faster inference and privacy. Join the next wave of AI as models go beyond the cloud.Sign up for the waitlist!
",['https://dl-staging-website.ghost.io/content/images/2024/05/V4_Waitlist_DeepLearning_Qualcomm_C1_Banner_2070x1080.png']
ThinkDifferentSmall,"Apple is thinking small — very small — with a new family of open large language models.
What's new:Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, and colleagues at Apple releasedOpen Source Efficient LLM(OpenELM), a family of smaller large language models. OpenELM ranges from 270 million parameters — plenty small enough to fit on a phone — to 3 billion parameters.
How it works:OpenELM comes in pretrained and instruction-tuned versions with parameter counts of 270 million, 450 million, 1.1 billion, and 3 billion. They can process 2,048 tokens of context. Thereleaseincludes weights, code for training and inference, and code for running the models on Apple chips.
Results:OpenELM beat a number of other open-source models trained solely on publicly available data.
Why it matters:After years of becoming only larger, neural networks lately have also been getting smaller. The smallest OpenELMs are tiny compared to, say, Microsoft’s Phi-3-mini. Apple has an extra incentive to make models capable of running on edge devices like phones. The company makes a major selling point of user privacy, and models run entirely on a smartphone (as opposed to in the cloud) keep the user’s activity under wraps.
We're thinking:DeLighTintroduced this layer-scaling approach in 2020. Sometimes it takes a while for good ideas to catch on!
",['https://dl-staging-website.ghost.io/content/images/2024/05/AIINDEX_1200px_14secHolds--1-.gif']
AI Trends in Depth,"More expensive models, superhuman performance, growing impacts on society — an extensive report takes stock of developments in machine learning over the past year.
What's new:Stanford’s Institute for Human-Centric AIpublishedthe seventh “AI Index Report,” its annual overview of the state of AI. The report documents rising costs and capabilities, a shift from academic to corporate dominance, and the public’s anxiety as the technology becomes ever more embedded in daily life.
Themes and findings:The 500-page report collates a wide variety of papers, benchmarks, market research, and surveys published in 2023. It delves deeply into AI technology, economics, governance, and impact. Among its key conclusions:
Behind the news:The differences between the new one and the initial, 2018 edition highlight the field’s rapid pace of change. For instance, the 2018 report opened by trumpeting the nearly 9x growth of AI research papers published between 2000 and 2017. The new one opened not with the annual rate of research publications (though it has roughly doubled since 2017) but with a graph of industry’s growing dominance in innovation.The Batchhascoveredseveral editions.
Why it matters:The “AI Index Report” offers a detailed snapshot of AI as it advances at an unprecedented rate and shows potential to revolutionize virtually every field of human endeavor. It dives deeply into areas of special concern to researchers (such as Gemini’s nearly $200 million training cost), practitioners (for instance, the slightly narrowing gender gap among computer science PhDs), businesses (the sharply rising number of regulations), and users (half of those who are aware of ChatGPT use it weekly). This year’s report includes new emphases on public opinion and geopolitics.
We're thinking:It’s heartening to see AI thriving. The field faces daunting challenges, yet the report highlights achievements in foundation models, science, medicine, and elsewhere that portend greater benefits directly ahead. What an exciting time for AI!
Expand your prompting skills with our new short course, “Prompt Engineering for Vision Models.” Learn how to prompt and fine-tune vision models to accomplish tasks from image generation to object detection.Start learning today
","['https://dl-staging-website.ghost.io/content/images/2024/05/The-Batch-ads-and-exclusive-banners---2024-04-29T162119.517.png', 'https://dl-staging-website.ghost.io/content/images/2024/05/unnamed---2024-05-01T152511.553.png']"
Amazon Rethinks Cashier-Free Stores,"Amazon is removing grab-and-go shopping from its cart.
What’s new:Amazon withdrew Just Walk Out, an AI-driven checkout service, from most of its Amazon Fresh grocery stores,The Informationreported. Instead, the stores will provide smart shopping carts. (Disclosure: Andrew Ng is a member of Amazon’s Board of Directors.)Checking out:Just Walk Outenables shoppers to scan a payment method upon entering a store, take items from shelves tracked by computer vision and weight-detection sensors, and simply exit with their purchases, bypassing the checkout counter. Amazon had installed the system in 47 Amazon Fresh stores in the U.S. and UK. In most of those locations. Amazon will replace Just Walk Out withDash Cart, a shopping cart that enables customers to scan purchases as they shop. Amazon will retain Just Walk Out in its Amazon Go convenience stores and an unspecified number of smaller, UK-based Amazon Fresh stores. It has licensed the system to other retailers including Hudson Markets and plans to install in more third-party stores this year.
Behind the news:AmazonintroducedJust Walk Out in 2016 at its first Amazon Go convenience store in Seattle. Itextendedthe system to Amazon Fresh in 2020. Between September 2020 and September 2022, Amazon opened 44 Fresh stores in the U.S. and 19 in the UK, most of which included Just Walk Out. But Amazon’s brick-and-mortar locationssufferedduring the COVID-19 pandemic. From September 2022 to mid-2024, amid broader cost-cutting efforts, the companypausedopening new grocery stores.
Why it matters:Grab-and-go shopping seems like a solid bet, given the increasing focus of retailing on immediate gratification. Yet Amazon’s retreat from Just Walk Out in larger stores suggests that the technology is less well suited to such environments. In addition, shoppers may not have adjusted easily to grab-and-go behavior, which removes social interactions with cashiers and encourages customers to spend without reviewing the bill.
We’re thinking:AI has the potential to revolutionize every field, including retailing, and it’s important to find productive uses for it. Not all experiments will succeed, but patient investment and experimentation can illuminate productive paths forward.
",['https://dl-staging-website.ghost.io/content/images/2024/05/SCIENCE-FIX.gif']
Songs Made to Order,"A new breed of audio generator produces synthetic performances of songs in a variety of popular styles.
What’s new:Udiolauncheda web-based, text-to-song generator that creates songs in styles from barbershop to heavy metal.Suno, which debuted its service late last year with similar capabilities, upgraded to its offering.
How it works:Both services take text prompts and generate full-band productions complete with lyrics, vocals, and instrumental solos, two separate generations per prompt. Users can generate lyrics to order or upload their own words, and they can download, share, and/or post the results for others to hear. Leaderboards rank outputs according to plays and likes.
Behind the news:Most earlier text-to-music generators were designed to produce relatively free-form instrumental compositions rather than songs with structured verses, choruses, and vocals. Released earlier this month,Stable Audio 2generates instrumental tracks up to three minutes long that have distinct beginnings, middles, and endings. Users can also upload audio tracks and use Stable Audio 2.0 to modify them.
Yes, but:Like text-to-image generators circa last year, current text-to-music models offer little ability to steer their output. They don’t respond consistently to basic musical terminology such as “tempo” and “harmony,” and requesting a generic style like “pop” can summon a variety of subgenres from the last 50 years of popular music.
Why it matters:With the advent of text-to-music models that produce credible songs, audio generation seems primed for a Midjourney moment, when the public realizes that it can produce customized music at the drop of a prompt. Already Udio’s and Suno’s websites are full of whimsical paeans to users’ pets and hobbies. The technology has clear implications for professional performers and producers, who, regrettably, have little choice but toadaptto increasing automation. But for now fans have fun, new toys to play with.
We’re thinking:You can dance to these algo-rhythms!
",['https://dl-staging-website.ghost.io/content/images/2024/04/VALS-4Leaderboards_1200px--1-.gif']
Benchmarks for Industry,"How well do large language models respond to professional-level queries in various industry domains? A new company aims to find out.
What’s new:Vals.AI, an independent model testing service, developed benchmarks that rank large language models’ performance of tasks associated with income taxes, corporate finance, and contract law; it also maintains a pre-existing legal benchmark. Open AI’s GPT-4 and Anthropic’s Claude 3 Opus did especially well in recent tests.
How it works:Vals AI hosts leaderboards that compare the performance of several popular large language models (LLMs) with respect to accuracy, cost, and speed, along with with analysis of the results. The company worked with independent experts to develop multiple-choice and open-ended questions in industrial domains. The datasets are not publicly available.
Results:Among 15 models, GPT-4 and Claude 3 Opus dominated Vals.AI’s leaderboards as of April 11, 2024. GPT-4 topped CorpFin and TaxEval, correctly answering 64.8 and 54.5 percent of questions, respectively. Claud 3 Opus narrowly beat GPT-4 on ContractLaw and LegalBench, achieving 74.0 and 77.7 percent, respectively. The smaller Claude 3 Sonnet took third place in ContractLaw, CorpFin, and TaxEval with 67.6, 61.4, and 37.1 percent. Google’s Gemini Pro 1.0 took third place in LegalBench with 73.6 percent.
Behind the news:Many practitioners infinanceandlawuse LLMs in applications that range from processing documents topredicting interest rates. However, LLM output in such applications requires oversight. In 2023, a New York state judgereprimandeda lawyer for submitting an AI-generated brief that referred to fictitious cases.
Why it matters:Typical AI benchmarks are designed to evaluate general knowledge and cognitive abilities. Many developers would like to measure more directly performance in real-world business contexts, where specialized knowledge may come into play.
We’re thinking:Open benchmarks can benefit from public scrutiny, and they’re available to all developers. However, they can be abused when developers cherry-pick benchmarks on which their models perform especially well. Moreover, they may find their way into training sets, making for unfair comparisons. Independent testing on proprietary benchmarks is one way to address these issues.
Join “Getting Started with Mistral” and access Mistral AI’s open source and commercial models via API calls. Learn to select the right model for your use case and get hands-on with features like JSON mode, function calling, and effective prompting techniques.Enroll for free!
","['https://dl-staging-website.ghost.io/content/images/2024/04/The-Batch-ads-and-exclusive-banners---2024-04-23T091059.531.png', 'https://dl-staging-website.ghost.io/content/images/2024/04/MANUFACTURING_14-SecHolds_1200px--1---1-.gif']"
AI Progress Report: Manufacturing,"Manufacturers are embracing AI even as they struggle to find the talent and data required.
What’s new:The market-research arm ofMIT Technology Reviewsurveyedmanufacturers’ use of AI in engineering, design, procurement, and production. All respondents were at least experimenting with AI, and many expect to launch their first deployments in the next year or two. Microsoft sponsored the research.
How it works:The authors interviewed executives at 300 manufacturers in aerospace, automotive, chemicals, electronics, and heavy equipment. All were either applying or considering AI in product design or factory operations.
Behind the news:Manufacturers are using AI to helpdesign products,visually inspect goods, andmaintain equipment. The field has attracted major players: Last year, Microsoft and Siemenslauncheda pilot of Industrial Copilot, which enables users to interact in natural language with software that drives assembly lines.
Why it matters:Manufacturers want to use AI, but many face obstacles of talent and data. That spells opportunities for budding practitioners as well as for manufacturers that lack infrastructure for collecting and managing data.
We’re thinking:One key to successful implementation of AI in manufacturing is tailoring systems to the unique circumstances of each individual facility. The highly heterogeneous tasks, equipment, and surroundings in different factories mean that one model doesn’t fit all. Developers who can solve this long-tail problem stand to reap rewards.
",['https://dl-staging-website.ghost.io/content/images/2024/04/unnamed---2024-04-24T134817.631.gif']
"Custom Agents, Little Coding","Google is empowering developers to build autonomous agents using little or no custom code.
What’s new:GoogleintroducedVertex AI Agent Builder, a low/no-code toolkit that enables Google’s AI models to run external code and ground their responses in Google search results or custom data.How it works:Developers on Google’s Vertex AI platform can build agents and integrate them into multiple applications. The servicecosts$12 per 1,000 queries and can use Google Search for $2 per 1,000 queries.
Behind the news:Vertex AI Agent Builder consolidates agentic features that some of Google’s competitors have rolled out in recent months. For instance, OpenAI’sAssistants APIlets developers build agents that respond to custom instructions, retrieve documents (limited by file size), call functions, and access a code interpreter. Anthropic recentlylaunchedClaude Tools, which lets developers instruct Claude language models to call customized tools. Microsoft’sWindows CopilotandCopilot Buildercan call functions and retrieve information using Bing search and user documents stored via Microsoft Graph.
Why it matters:Making agents practical for commercial use can require grounding, tool use, multi-agent collaboration, and other capabilities. Google’s new tools are a step in this direction, taking advantage of investments in its hardware infrastructure as well as services such as search. As tech analyst Ben Thompsonwrites, Google’s combination of scale, interlocking businesses, and investment in AI infrastructure makes for a compelling synergy.
We’re thinking:Big-tech offerings like Vertex Agent Builder compete with an expanding universe of open source tools such as AutoGen, CrewAI, and LangGraph. The race is on to provide great agentic development frameworks!
",['https://dl-staging-website.ghost.io/content/images/2024/04/unnamed---2024-04-17T170803.947.png']
Hallucination Creates Security Holes,"Language models can generate code that erroneously points to software packages, creating vulnerabilities that attackers can exploit.
What’s new:A cybersecurity researcher noticed that large language models, when used to generate code, repeatedly produced a command to install a package that was not available on the specified path,The Registerreported. He created a dummy package of the same name and uploaded it to that path, and developers duly installed it.How it works:Bar Lanyado, a researcher at Lasso Security, found that the erroneous commandpip install huggingface-cliappeared repeatedly in generated code. The packagehuggingface-clidoes exist, but it is installed using the commandpip install -U “huggingface_hub[cli]"". The erroneous command attempts to download a package from a different repository. Lanyado published some of his findings in ablog post.
Testing:LanyadotestedCohere AI’s Coral, Google’s Gemini Pro, and OpenAI’s GPT-4 and GPT-3.5. His aim was to determine how often they hallucinated packages and how often they referred repeatedly to the same hallucinated package. First he collected roughly 47,000 “how to” questions related to over 100 subjects in Go, .NET, Node.js, Python, and Ruby. Then he identified questions that produced hallucinated packages from a zero-shot prompt. He selected 20 of these questions at random and prompted each model 100 times to see whether it would refer to the same package every time.
Why it matters:Lanyado’s method is not known to have been used in an attack, but it may be only a matter of time given its similarity tohackslike typosquatting, dependency confusion, and masquerading.
We’re thinking:Improved AI-driven coding tools should help to address this issue. Meanwhile, the difference between a command like pip install huggingface-cli and pip install -U ""huggingface_hub[cli]"" is subtle. In cases like this, package providers can look out for potential doppelgangers and warn users from being misled.
In the short course “Quantization Fundamentals with Hugging Face,” you’ll learn how to cut the computational and memory costs of AI models through quantization. Learn to quantize nearly any open source model!Join today
","['https://dl-staging-website.ghost.io/content/images/2024/04/The-Batch-ads-and-exclusive-banners---2024-04-16T091800.593.png', 'https://dl-staging-website.ghost.io/content/images/2024/04/unnamed---2024-04-17T171422.978.gif']"
GPT Store Shows Lax Moderation,"OpenAI has been moderating its GPT Store with a very light touch.
What’s new:In a survey of the GPT Store’s offerings,TechCrunchfoundnumerous examples of custom ChatGPT instances that appear to violate the store’s ownpolicies.
How it works:The GPT Store has a low bar for entry by design — any paid ChatGPT user can create a custom-prompted variation of the chatbot, known as a GPT, and include it in the store. The store lists GPTs in several categories, such as Writing, Productivity, Programming, and Lifestyle. While many are useful, some are questionable.
Behind the news:OpenAIlaunchedthe GPT Store in January. Since then, users have uploaded more than 3 million GPTs that include enhanced search engines, creative writing aids, and tools that produce short videos. The most popular GPTs have millions of downloads. Despite its “store” name, the GPT Store’s contents are free to download. OpenAI ispilotinga program in which U.S.-based uploaders of popular GPTs can earn money.
Why it matters:The GPT Store is the chatbot era’s answer to Apple’s App Store or Android’s Google Play Store. If it succeeds, it could democratize chatbot development just as the App Store helped to popularize building smartphone applications. How OpenAI moderates the store may have real financial and reputational impacts on developers in the years ahead.We’re thinking:The GPT Store’s low barrier to entry is a boon to well-meaning developers, but it may encourage less responsible actors to take advantage of lax moderation. We applaud OpenAI’s willingness to execute an ambitious vision and hope it finds a workable balance.
",['https://dl-staging-website.ghost.io/content/images/2024/04/unnamed---2024-04-17T171539.453.gif']
Coding Agents Proliferate,"New coding tools act like agents to automate software programming tasks.
What’s new:A wave of open source software-development tools based on large language models take advantage of the ability of large language models to plan, critique their own work, and extend themselves by calling functions.
How it works:These projects follow hot on the heels of Cognition’sDevin, a commercial system billed as a semi-autonomous software developer that’s available to selected customers upon request. Some, like Devin, provide sandboxed chat for natural-language commands, command line shell, code editor, and/or a web browser through which the agent can test code or find documentation. Given a prompt, they generate a step-by-step plan and execute it. They may ask for further information or instructions, and users can interrupt to modify their requests.
Behind the News:Code-completion tools like Github Copilot and Code Llama quickly have becomeubiquitous.AutoGPT, released in 2023, is an open-source generalist AI agent based on GPT-4 that has been used to write and debug code. Recently Replit, known for its Ghostwriter code-completion and chatbot applications, began buildingits own LLMsfor automated code repair.
Why it matters:Agentic coding tools are distinguished bytechniquesthat enable large language models to plan, reflect on their work, call tools, and collaborate with one another. Usersreportthat, unlike previous coding assistants, the new tools are better at sustaining extended tasks and correcting their own work.
We’re thinking:Many software developers worry that large language models will make human coders obsolete. We doubt that AI will replace coders, but we believe that coders who use AI will replace those who don’t. Agent-based tools still have a long way to go, but they seem likely to augment programmers’ abilities in a larger development pipeline.
",['https://dl-staging-website.ghost.io/content/images/2024/04/unnamed---2024-04-10T141020.606.png']
What Users Do With Generative AI,"Generative AI is being used mostly to generate ideas.
What’s new:The tech consultancy Filteredstudiedthe most common uses for generative AI. While most gen AI users produced text, the study surprisingly found that users were slightly more likely to generate videos than images.How it works:The analysts sifted through tens of thousands of posts on popular online forums for anecdotes that described uses of generative AI. The analysts grouped the posts into a list of 100 most popular uses of generative AI and ranked each one by reach and value added.
Behind the news:The range of use cases reflects the huge number of people, from all walks of life and all parts of the world, who are using generative AI tools. In a given week in November 2023, more than 100 million peopleusedChatGPT, the most popular of these tools. Independently, in February 2024, Pew Researchfoundthat 23 percent of U.S. adults had used ChatGPT at least once, including 43 percent of respondents under 30 years old and 37 percent of those with postgraduate degrees. According to the Pew report, 20 percent of all Americans had used ChatGPT for work, and 17 percent had used it for entertainment, with younger and more educated users leading the way.Why it matters:It’s clear that millions of people use generative AI but less clear how they use it. Understanding how and where they actually apply it is helpful for anyone who aims to develop new generative AI products and services or plans to integrate the tech into their organization.
We’re thinking:While it’s encouraging that more than a fifth of U.S. adults have tried ChatGPT,  it also suggests huge room for growth in generative AI at large.
Integrate diverse data types into your LLM applications in our new short course built in collaboration with Unstructured. Learn techniques to extract and normalize data from PDFs, tables, and images into a structured format.Sign up today
","['https://dl-staging-website.ghost.io/content/images/2024/04/The-Batch-ads-and-exclusive-banners---2024-04-09T091357.311.png', 'https://dl-staging-website.ghost.io/content/images/2024/04/unnamed---2024-04-10T141216.341.gif']"
Instability at Stability AI,"The CEO of Stability AI resigned as the company faces an increasingly competitive market.
What’s new:Emad Mostaque stepped down from Stability AI, developer of the Stable Diffusion image generator among other models, amid financial woes, uncertain direction, and sinking confidence from investors and employees alike,Forbesreported. Mostaque’s departure followed the exits of numerous executives and key employees.
How it works:StabilityconfirmedMostaque’s departure in a blog post. The company’s chief operating officer Shan Shan Wong and chief technology officer Christian Laforte will act as co-CEOs until its directors find a permanent replacement. They inherit a company with troubles beyond leadership.
Behind the news:Despite its troubles, Stability continued to release new models. In February, itopenedthe waitlist for the third-generation version of Stable Diffusion. Last month, itreleasedStable Video 3D, a project in which the team produced three-dimensional objects from images. This month, itreleasedStable Audio 2.0, which can produce music files up to three minutes long from a text prompt.Why it matters:Stability has been a standard bearer for open-source AI in a field where tech giants aim to dominate with closed models. Effective leadership could have a major impact on the models available to developers in the years ahead.
We’re thinking:Stability helped capture the public imagination during the generative AI boom of 2022, and its open models, particularly its diffusion models, have been a huge benefit to the AI community. We hope new leadership puts the company on firm footing.
",['https://dl-staging-website.ghost.io/content/images/2024/04/unnamed---2024-04-10T141759.545.gif']
Microsoft Absorbs Inflection,"Microsoft took over most of the once high-flying chatbot startup Inflection AI in an unusual deal.
What’s new:Microsoft hired Inflection CEO Mustafa Suleyman and much of the startup’s staff and paid roughly $650 million for access to its models and legal protections,Bloombergreported. Inflection willshiftfrom serving consumers to focusing on large companies.How it works:Microsoft did not formally purchase any assets of Inflection, which remains a separate, independent company. $650 million is significantly less than the $1.3 billion in investment that Inflectionreceivedlast year at a $4 billion valuation.
Behind the news:Inflection was co-founded in 2022 by Suleyman (a founder of DeepMind, now a division of Google), Simonyan, and LinkedIn chairman Reed Hoffman with funding partly from Microsoft. The startup initially positioned itself as a competitor to OpenAI and Anthropic, seeking to develop AI assistants for consumers. Its flagship product wasPi, a chatbot trained to provide emotional support. Microsoft CEO Satya Nadella began courting Suleyman several months ago, and Suleyman wanted to bring Inflection’s staff along with him. Microsoft made a similar offer to OpenAI in November, during that company’s leadershipshakeup, when the tech giant proposed hiring briefly-ousted CEO Sam Altman and many of his co-workers to staff a new organization at Microsoft.
Yes, but:The unusual nature of the deal — with Microsoft absorbing most of Inflection’s staff while leaving the startup intact as a company — may have beendesignedto avoid the antitrust scrutiny that comes with acquisitions. The deal doesn’t automatically trigger areviewby U.S. regulators because Microsoft did not acquire Inflection assets. Microsoft’s close relationship with OpenAI has attracted attention from regulators in theU.S.,UK, andEU.Why it matters:Tech giants are searching for an edge in AI development after being briefly leapfrogged in the market by large language model startups. Microsoft invested $13 billion inOpenAI, and Nadella says that partnership remains a strategic priority. This year, Microsoft has sought to diversify its AI interests, sealing deals withMistraland now Inflection, while also beefing up its internal efforts. The distribution channel for AI models increasingly runs through large companies and their cloud services.We’re thinking:Even with strong talent, powerful backing, and a multibillion-dollar valuation, Inflection struggled to gain traction. Its journey from hot consumer startup to streamlined enterprise software provider shows how competitive the chatbot sector has become.
",['https://dl-staging-website.ghost.io/content/images/2024/04/unnamed---2024-04-03T133929.961.png']
Nvidia Revs AI Engine,"Nvidia’s latest chip promises to boost AI’s speed and energy efficiency.
What’s new:The market leader in AI chipsannouncedthe B100 and B200 graphics processing units (GPUs) designed to eclipse its in-demand H100 and H200 chips. The company will also offer systems that integrate two, eight, and 72 chips.
How it works:The new chips are based on Blackwell, an updated chip architecture specialized for training and inferencing transformer models. Compared to Nvidia’s earlier Hopper architecture, used by H-series chips, Blackwell features hardware and firmware upgrades intended to cut the energy required for model training and inference.
Price and availability:The B200 will cost between $30,000 and $40,000, similar to thegoing ratefor H100s today, Nvidia CEO Jensen HuangtoldCNBC. Nvidia did not specify when the chip would be available. Google, Amazon, and Microsoftstatedintentions to offer Blackwell GPUs to their cloud customers.
Behind the news:Demand for the H100 chip has been so intense that the chip has beendifficultto find, driving some users to adopt alternatives such as AMD’s MI300X. Moreover, in 2022, the U.S.restrictedthe export of H100s and other advanced chips to China. The B200 also falls under the ban.Why it matters:Nvidiaholdsabout 80 percent of the market for specialized AI chips. The new chips are primed to enable developers to continue pushing AI’s boundaries, training multi-trillion-parameter models and running more instances at once.We’re thinking:Cathie Wood, author of ARK Invest’s “Big Ideas 2024”report, estimated that training costs are falling at a very rapid 75 percent annually, around half due to algorithmic improvements and half due to compute hardware improvements. Nvidia’s progress paints an optimistic picture of further gains. It also signals the difficulty of trying to use model training to build a moat around a business. It’s not easy to maintain a lead if you spend $100 million on training and next year a competitor can replicate the effort for $25 million.
In our new short course “Red Teaming LLM Applications,” you will learn industry-proven manual and automated techniques to proactively test, attack, and improve the robustness of your large language model (LLM) applications.Join now!
","['https://dl-staging-website.ghost.io/content/images/2024/04/The-Batch-ads-and-exclusive-banners---2024-04-02T085923.011.png', 'https://dl-staging-website.ghost.io/content/images/2024/04/unnamed---2024-04-03T133956.061.gif']"
Toward Managing AI Bio Risk,"Scientists pledged to control their use of AI to produce potentially hazardous biological materials.
What’s new:More than 150 biologists in Asia, Europe, and North Americasigneda voluntary commitment to internal and external oversight of machine learning models that can be used to design proteins.
How it works:The scientists made 10 voluntary commitments regarding synthetic biology research. They promised broadly to avoid research likely to enable harm and to promote research that responds to infectious disease outbreaks or similar emergencies.
Behind the news:The potential role of AI in producing bioweapons is a major focus of research in AI safety. The current pledge arose from a University of Washington meeting on responsible AI and protein design held late last year. TheAI Safety Summit, which took place at around the same time, also addressed the topic, and Helena, a think tank devoted to solving global problems, convened a similar meeting in mid-2023.
Why it matters:DeepMind’sAlphaFold, which finds the structures of proteins, hasspawnedmodels that enable users to design proteins with specific properties. Their output could help scientists cure diseases, boost agricultural production, and craft enzymes that aid industrial processes. However, their potential for misuse has led to scrutiny bynationalandinternationalorganizations. The biology community’s commitment to use such models safely may reassure the public and forestall onerous regulations.
We’re thinking:The commitments are long on general principles and relatively short on concrete actions. We’re glad they call for ongoing revision and action, and we hope they lead to the development of effective safeguards.
",['https://dl-staging-website.ghost.io/content/images/2024/04/unnamed---2024-04-03T133938.278.png']
"One Agent, Many Environments","AI agents are typically designed to operate a particular software environment. Recent work enabled a single agent to take actions in a variety of three-dimensional virtual worlds.
What's new:A team of 90 people at Google and University of British Columbia announcedScalable Instructable Multiworld Agent(SIMA), a system that learned to follow text instructions (such as “make a pile of rocks to mark this spot” or “see if you can jump over this chasm”) in seven commercial video games and four research environments.
How it works:SIMA’s architecture consists of several transformers and a vanilla neural network. The authors trained it to mimic human players using a dataset of gameplay broken into 10 second tasks, including onscreen images, text instructions, keyboard presses, and mouse motions. The video games included Goat Simulator 3 (a third-person game in which the player takes the form of a goat), No Man’s Sky (a first- or third-person game of exploration and survival in outer space), Hydroneer (a first-person game of mining and building), and others.
Results:Judges evaluated SIMA’s success or failure at completing nearly 1,500 instructions that spanned tasks in nine categories like action (“jump”), navigation (“go to your ship”), and gathering resources (“get raspberries”). In Goat Simulator 3, SIMA completed 40 percent of the tasks. In No Man’s Sky, the judges compared SIMA’s performance to that of the human players whose gameplay produced the training data. SIMA was successful 34 percent of the time, while the players were successful 60 percent of the time. Judges also compared SIMA to versions that were trained to be experts in a single game. SIMA was successful more than 1.5 times more often than the specialized agents.
Behind the news:SIMA extends Google’s earlier successes building agents that rival or beat human players at individual games includingGo,classic Atari games, andStarCraft II.
Why it matters:Training agents to follow directions in various environments, seeing the same things humans would, is a step toward building instructable agents that can work in any situation. The authors point to potential applications in robotics, simulations, and gaming; wherever an agent might need to be guided through diverse challenges.
We're thinking:This work shows that an agent trained on multiple games can perform better than an agent trained on just one, and that the richer the language inputs in a gameworld, the better the agent can perform. With only a handful of training environments under its belt, SIMA doesn’t demonstrate superhuman performance, but it gets the job done a surprising amount of the time!
",['https://dl-staging-website.ghost.io/content/images/2024/03/CELLS.jpg']
Cross-Species Cell Embeddings,"Researchers used an AI system to identify animal cell types from gene sequences, including a cell type that conventional approaches had discovered only in the past year.
What’s new:Biologists at Stanford trained asystemto produce embeddings that represent individual cells in an organism. This enabled them to find cell types that have common function in different animals; for instance, the Norn cell, a type of kidney cell that biologists had previously theorized butdiscoveredonly in 2023.
How it works:Universal Cell Embedding (UCE) comprises two transformers that produce embeddings of genes and cells respectively, plus a classifier based on a vanilla neural network. The authors trained the classifier, given embeddings of a gene and cell, to classify whether or not the cell produces the protein coded by that gene. The training dataset included RNA sequences of 36.2 million cells from eight animal species (humans and mice accounted for 33.9 million) along with related protein structures.
Results:Cell embeddings produced by UCE enabled the authors to identify cell types in animal species that weren’t in the training set. For instance, the authors embedded a dataset of mouse cells and appliedUMAPclustering to differentiate the types. They labeled the clusters as specific cell types (including Norn cells, which biologists took more than a century to find) based on the presence of certain genes that distinguish one cell type from another. Using the labels, they trained a logistic classifier. They applied the classifier to their training dataset and found Norn cells, among other cell types, in species other than mice. They verified the findings by looking for genes that tend to show up only in Norn cells.
Why it matters:UCE’s embeddings encode biologically meaningful information about individual cells, enabling a clustering algorithm to group them into recognized cell types. The fact that the recently discovered Norn cell was among those clusters suggests that UCE may yield further discoveries that accelerate development of new medicines, lab processes, and research methods. In fact, the model found Norn cells — which are known to occur in the kidney — in organs where they have not been seen before. If this result turns out to be valid, UCE will have made a discovery that has eluded biologists to date.
We’re thinking:It’s a truism that a machine learning model is only as good as its data. That makes this work all the more impressive: Its training data included a handful of species, yet it generalized to others.
Join our short course on “JavaScript RAG Web Apps with LlamaIndex” to learn how to build full-stack JavaScript web applications that let you chat with your data. Harness the capabilities of large language models and retrieval augmented generation (RAG)!Enroll for free
","['https://dl-staging-website.ghost.io/content/images/2024/03/The-Batch-ads-and-exclusive-banners---2024-03-25T154251.995.png', 'https://dl-staging-website.ghost.io/content/images/2024/03/MAVEN.jpg']"
U.S. Deploys AI-Assisted Targeting,"The United States military is using computer vision to target enemy positions in the Red Sea and elsewhere.What’s new:Maven, a system that analyzes satellite and geolocation data, has been used to identify targets in real-world conflicts,Bloombergreported. The system was developed primarily byPalantirand integrates technology from Amazon, Microsoft, information technology firms ECS Federal and L3Harris, aerospace firms Maxar and Sierra Nevada, and other unnamed companies.How it works:The 18th Airborne Corps, a U.S. Army unit organized for rapid deployment around the world, used Maven in live-fire training exercises. The system helped locate surface vessels in the Red Sea, rocket launchers in Yemen, and potential airstrike targets in Iraq and Syria. The U.S. used it to help Ukraine’s armed forces to locate Russian equipment, anonymous sources said.
Behind the news:Google initially developed Maven for the U.S. Defense Department around 2017. Palantirinheritedthe project after Google, facing protests by employees who did not want to contribute to government intelligence systems,declinedto renew its contract in 2018. The U.S. military now has more than 800 active AI projects with a wide range of technology partners and contractors. Other countries are deploying similar technology:IsraelandUkrainehave used AI-assisted targeting in their ongoing conflicts.
Yes, but:Some U.S. military experts worry about Maven’s accuracy. In tests, Maven successfully identified objects about 60 percent of the time, while human analysts working with the 18th Airborne Corps did so 84 percent of time. Moreover, the system’s training data emphasizes deserts, and its success rate drops in other types of environments.Why it matters:Maven and similar systems offer some advantages over human analysts. They can observe and integrate multiple data streams simultaneously, and they can identify potential targets much more quickly. It’s likely that more data will make these systems more accurate. On the other hand, they represent a further step toward automated warfare in which automated assistance could come to displace human decision-making.
We’re thinking:Automated targeting is increasingly used in military applications, and less-sophisticated systems have been inusefor decades. However, humans should always be in control of decisions to fire. We support a globalbanon fully autonomous weapons.
",['https://dl-staging-website.ghost.io/content/images/2024/03/SOCCER-ezgif.com-optimize.gif']
Conversational Robots,"Robots equipped with large language models are asking their human overseers for help.
What's new:Andrew Sohn and colleagues at CovariantlaunchedRFM-1, a model that enables robots to respond to instructions, answer questions about what they see, and request further instructions. The model is available to Covariant customers.
How it works:RFM-1 is a transformer that comprises 8 billion parameters. The team started with a pretrained large language model and further trained it, given text, images, videos, robot actions, and/or robot sensor readings, to predict the next token of any of those types. Images and videos are limited to 512x512 pixels and 5 frames per second.
Behind the news:Covariant’s announcement follows a wave ofroboticsresearchinrecentyearsthat enables robots to take action in response totextinstructions.
Why it matters:Giving robots the ability to respond to natural language input not only makes them easier to control, it also enables them to interact with humans in new ways that are surprising and useful. In addition, operators can change how the robots work by issuing text instructions rather than programming new actions from scratch.
We're thinking:Many people fear that robots will make humans obsolete. Without downplaying such worries, Covariant’s conversational robot illustrates one way in which robots can work alongside humans without replacing them.
",['https://dl-staging-website.ghost.io/content/images/2024/03/BACKDOOR2.gif']
Some Models Pose Security Risk,"Security researchers sounded the alarm about holes in Hugging Face’s platform.What’s new:Models in the Hugging Face open source AI repository can attack users’ devices,according tocybersecurity experts at JFrog, a software firm. Meanwhile, a different team discovered avulnerabilityin one of Hugging Face’s own security features.Compromised uploads:JFrog developed scanned models on Hugging Face for known exploits. They flagged around 100 worrisome models. Flagged models may have been uploaded by other security researchers but pose hazards nonetheless, JFrog said.
Malicious mimicry:Separately, HiddenLayer, a security startup,demonstrateda way to compromiseSafetensors, an alternative to Pickle that stores data arrays more securely. The researchers built a malicious PyTorch model that enabled them to mimic the Safetensors conversion bot. In this way, an attacker could send pull requests to any model that gives security clearance to the Safetensors bot, making it possible to execute arbitrary code; view all repositories, model weights, and other data; and replace users’ models.
Behind the News:Hugging Face implements a variety of security measures. In most cases, it flags potential issues but does not remove the model from the site; users download at their own risk. Typically, security issues on the site arise when users inadvertently make their own information available. For instance, in December 2023, Lasso Securitydiscoveredavailable API tokens that afforded access to over 600 accounts belonging to organizations like Google, Meta, and Microsoft.
Why it matters:As the AI community grows, AI developers and users become more attractive targets for malicious attacks. Security teams have discovered vulnerabilities in popular platforms, obscure models, and essential modules like Safetensors.
We’re thinking:Security is a top priority whenever private data is concerned, but the time is fast approaching when AI platforms, developers, and users must harden their models, as well as their data, against attacks.
In our new short course “Efficiently Serving Large Language Models,” you’ll pop the hood on large language model inference servers. Learn how to increase the performance and efficiency of your LLM-powered applications!Enroll today
","['https://dl-staging-website.ghost.io/content/images/2024/03/The-Batch-ads-and-exclusive-banners---2024-03-18T100446.882.png', 'https://dl-staging-website.ghost.io/content/images/2024/03/unnamed---2024-03-20T163811.726.png']"
Deepfakes Become Politics as Usual,"Synthetic depictions of politicians are taking center stage as the world’s biggest democratic election kicks off.
What’s new:India’s political parties haveembracedAI-generated campaign messages ahead of the country’s parliamentary elections, which will take place in April and May,Al Jazeerareported.
How it works:Prime Minister Narendra Modi, head of the ruling Bharatiya Janata Party (BJP), helpedpioneerthe use of AI in campaign videos in 2020. They’ve become common in recent state elections.
Meanwhile, in Pakistan:Neighboring Pakistan was deluged with deepfakes in the run-up to its early-February election. Former prime minister Imran Khan, who has been imprisoned oncontroversialcharges since last year,communicatedwith followers via a clearly marked AI-generated likeness. However, he found himself victimized by deepfakery when an AI-generated likeness of him, source unknown,urgedhis followers to boycott the polls.
Behind the news:Deepfakes have proliferated in India in the absence of comprehensive laws or regulations that govern them. Instead of regulating them directly, government officials have pressured social media operators like Google and Meta to moderate them.
What they’re saying:“Manipulating voters by AI is not being considered a sin by any party,” an anonymous Indian political consultant toldAl Jazeera. “It is just a part of the campaign strategy.”
Why it matters:Political deepfakes are quickly becoming a global phenomenon. Parties fromArgentina,the United States, andNew Zealandhave distributed AI-generated imagery or video. But the sheer scale of India’s national election — in which more than900 million peopleare eligible to vote — has made it an active laboratory for synthetic political messages.
We’re thinking:Synthetic media has legitimate political uses, especially in a highly multilingual country like India, where it can enable politicians to communicate with the public in a variety of languages and dialects. But unscrupulous parties can also use it to sow misinformation andunderminetrust in politicians and media. Regulations are needed to place guardrails around deepfakes in politics. Requiring identification of generated campaign messages would be a good start.
",['https://dl-staging-website.ghost.io/content/images/2024/03/unnamed---2024-03-20T163859.691.gif']
Anthropic Ups the Ante,"Anthropic announced a suite of large multimodal models that set new states of the art in key benchmarks.
What’s new:Claude 3 comprises three language-and-visionmodels: Opus (the largest and most capable), Sonnet (billed as the most cost-effective for large-scale deployments), and Haiku (the smallest, fastest, and least expensive to use). Opus and Sonnet are available via the Claude API, onAmazon Bedrock, and in a private preview onGoogle Cloud. Opus also is  available with the Claude Pro chatbot, which costs $20 monthly. Sonnet powers Claude’s free chatbot.
How it works:The models, whose parameter counts are undisclosed, were trained on public, proprietary, and synthetic data ending in August 2023. They can process 200,000 tokens of context. Opus can accommodate up to 1 million tokens of context, comparable toGoogle’s Gemini 1.5 Pro, upon request.
Test recognition:Opus aced “needle-in-a-haystack” tests to evaluate its ability to track long inputs. It also exhibited interesting behavior: In one such test, amid random documents that covered topics including startups, coding, and work culture, engineers inserted a sentence about pizza toppings and questioned the model on that topic. Not only did the model answer the question accurately, it also deduced that it was being tested, as Anthropic prompt engineer Alex Albertreportedin a post on X. “I suspect this pizza topping ‘fact’ may have been inserted as a joke or to test if I was paying attention,” Opus said, “since it does not fit with the other topics at all.”
Inside the system prompt:In a separate post on X, Anthropic alignment specialist Amanda Askell provided a rare peek at the thinking behind Claude 3’s system prompt, text prepended to user prompts to condition the model’s responses. To ground them in time, the models receive the current date and its training cut-off. To avoid rambling output, they’re directed to be concise. In an effort to correct for political and social biases that the team has observed, the models are asked to assist users even if it “personally disagrees with the views being expressed,” refrain from negative stereotyping of majority groups, and focus on objective information when addressing controversial topics. Finally, it’s directed to avoid discussing the system prompt unless it’s directly relevant to a query. “You might think this part is to keep the system prompt secret from you,” Askell wrote. “The real goal of this part is to stop Claude from excitedly telling you about its system prompt at every opportunity.”
Why it matters:Anthropic began with a focus onfine-tuning for safety, and its flagship model now tops several benchmark leaderboards as well. The Claude 3 family gives developers access to state-of-the-art performance at competitive prices.
We’re thinking:Three highly capable “GPT-4-class” large language models (LLMs) are now widely available: GPT-4, Gemini Pro, and Claude 3. The pressure is on for teams to develop an even more advanced model that leaps ahead and differentiates. What a great time to be building applications on top of LLMs!
",['https://dl-staging-website.ghost.io/content/images/2024/03/unnamed--52-.jpg']
India Warns Devs: No Unreliable AI,"India advised major tech companies to seek government approval before they deploy new AI models.
What’s new:India’s Ministry of Electronics and Information Technology (MeitY)issueda nonbinding “advisory” to technology firms, including Google, Meta, and OpenAI, to seek government permission before releasing AI models their developers consider unreliable or still in testing.
How it works:Thenoticeasks platforms and other intermediaries to label AI-generated media clearly and to warn customers that AI systems may output inaccurate information. It also says that models should avoid bias, discrimination, and undermining the integrity of the electoral process.
Behind the news:India has regulated AI with a light touch, but it appears to be reconsidering in light of the growingroleof AI-generated campaign ads in its upcoming elections.
Why it matters:National governments worldwide, in formulating their responses to the rapid evolution of AI, must balance the benefits of innovation against fears of disruptive technology. Fear seems to weigh heavily in India’s new policy. While the policy’s scope is narrower than it first appeared, it remains unclear what constitutes a significant platform, how to certify an AI model as reliable, whether services like ChatGPT are considered social platforms that would be affected, and how violations might be punished.
We’re thinking:While combating misinformation is important, forcing developers to obtain government approval to release new models will hold back valuable innovations. We urge governments to continue to develop regulations that guard against harms posed by specific applications while allowing general-purpose technology to advance and disseminate rapidly.
Knowledge graphs can structure complex data, drive intelligent search functionality, and help you build powerful AI applications that reason over different data types. In this course, you’ll learn how to use knowledge graphs to enhance large language models (LLMs).Sign up for free
","['https://dl-staging-website.ghost.io/content/images/2024/03/V2_DeepLearning_Neo4js_Banner_2070x1080--1-.png', 'https://dl-staging-website.ghost.io/content/images/2024/03/unnamed--53-.jpg']"
Google Tests Generative News Tools,"Google is paying newsrooms to use a system that helps transform press releases into articles.What’s new:Google has recruited a small number of independent news outlets for a one-year test of generative publishing tools,Adweekreported. The system reads external web pages and produces articles that editors can revise and publish.How it works:Google requires publishers to use the system to produce and publish three articles per day, one newsletter per week, and one marketing campaign per month. (It doesn’t require them to label the system’s output as AI-generated.) In exchange, publishers receive a monthly stipend that amounts to more than $10,000 annually.
Behind the news:The pilot program is part of theGoogle News Initiative, through which the tech giant provides media literacy programs, fact-checking tools, and digital publishing tools to news outlets. Last year, Googledemonstrateda tool known as Genesis to news outlets includingThe New York Times,The Washington Post, andThe Wall Street Journal. Like the new system, Genesis took in public information and generated news articles. It also suggested headlines and different writing styles. Then, as now, observers worried that Google eventually would use its tools to bypass news outlets by publishing news summaries directly in search results.
Why it matters:Such partnerships could yield dividends for Google and publishers alike. Google can learn what publishers need and how a generative model built to produce news holds up under the pressure of deadlines and audiences. Publishers can gain experience that may help them avoid the criticisms that greeted outlets likeCNET,Gizmodo, andSports Illustrated, whose initial efforts to publish generated articles were either hidden behind false bylines or marred by factual inaccuracies.
We’re thinking:Text generation could be a boon to publishers. Checking generated text (or, indeed, any synthetic media) for similarity to its source material is a sensible feature that could be useful in a variety of applications. Yet the utility of a system that summarizes individual web pages is limited, and the temptation to echo competitors may be hard to resist. We look forward to further improvements that enable agents that can assimilate and analyze text from disparate sources.
",['https://dl-staging-website.ghost.io/content/images/2024/03/unnamed---2024-03-13T145040.685.gif']
Mistral AI Extends Its Portfolio,"European AI champion Mistral AI unveiled new large language models and formed an alliance with Microsoft.
What’s new:Mistral AIintroducedtwo closed models, Mistral Large and Mistral Small (joining Mistral Medium, which debuted quietly late last year). Microsoft invested $16.3 million in the French startup, and itagreedto distribute Mistral Large on its Azure platform and let Mistral AI use Azure computing infrastructure. Mistral AI makes the new models available to try for freehereand to use on itsLa Plateformeand via custom deployments.
Model specs:The new models’ parameter counts, architectures, and training methods are undisclosed. Like the earlier, open source Mistral 7B and Mixtral 8x7B, they can process 32,000 tokens of input context.
Behind the news:Mistral AI was founded in early 2023 by engineers from Google and Meta. The French government has touted the company as a home-grown competitor to U.S.-based leaders like OpenAI. France’s representatives in the European Commissionarguedon Mistral’s behalf to loosen the European Union’s AI Act oversight on powerful AI models.
Yes, but:Mistral AI’s partnership with Microsoft has divided European lawmakers and regulators. The European Commission, which already wasinvestigatingMicrosoft’s agreement with OpenAI for potential breaches of antitrust law,plansto investigate the new partnership as well. Members of President Emmanuel Macron’s Renaissance partycriticizedthe deal’s potential to give a U.S. company access to European users’ data. However, other French lawmakerssupportthe relationship.
Why it matters:The partnership between Mistral AI and Microsoft gives the startup crucial processing power for training large models and greater access to potential customers around the world. It gives the tech giant greater access to the European market. And it gives Azure customers access to a high-performance model that’s tailored to Europe’s unique regulatory environment.
We’re thinking:Mistral AI has made impressive progress in a short time, especially relative to the resources at its disposal as a startup. Its partnership with a leading hyperscaler is a sign of the tremendous processing and distribution power that remains concentrated in the large, U.S.-headquartered cloud companies.
",['https://dl-staging-website.ghost.io/content/images/2024/03/unnamed---2024-03-06T161718.283.png']
Robot Chemist,"A robot outperformed human chemists at synthesizing chemicals.
What’s new:Researchers at University of Amsterdam builtRoboChem, an integrated robotic system that learned to design light-activated chemical reactions while achieving optimal yields and throughput.
How it works:RoboChem includes a computer that runs a machine learning model and a set of automated lab instruments including a liquid handler, syringe pumps, and a photochemical reactor, all enclosed in an airtight vacuum chamber. Given a set of reagents and resulting product, RoboChem aimed to find conditions that maximize the yield (the ratio of the amount of a product synthesized to the potential amount, expressed as a percentage) and throughput (rate of synthesis) in the fewest experimental runs. It followed a 3-part cycle: (i) determine experimental conditions (amounts and concentrations of the given reagents, intensity of light, and time spent in the reactor), (ii) combine the reagents under those conditions, and (iii) evaluate the yield and throughput via a spectrometer.
Results:Robochem executed reactions to produce 18 substances. In all cases, it found experimental conditions that had either higher throughput and yield, or higher throughput and nearly equivalent yield, than the best conditions previously known. In one reaction, RoboChem achieved yield of 58 percent and throughput of 95.6 g/Lh (gram yield per liter in the reactor per hour), while previous work had achieved 45 percent and 2.8 g/Lh. In another reaction, RoboChem achieved 81 percent and 1720 g/Lh, where previous best results achieved 82 percent and 3 g/Lh — 1 percent lower yield but 573 times greater throughput.
Behind the news:In 2020, researchers at the University of Liverpooltraineda mobile robot arm to navigate a chemistry lab, mix chemicals, and operate equipment. That robot used a similar optimization method. However, the Amsterdam robot is much less expensive and proved itself in a wider range of experiments.
Why it matters:The authors believe that RoboChem could dramatically increase lab productivity at lower cost in time and money. The light-activated reactions they focused on have applications in fields including pharmaceuticals, household chemicals, and renewable energy.We’re thinking:These researchers clearly are in their element.
In “Open Source Models with Hugging Face,” our latest short course, you’ll use open source models to build chatbots, language translators, and audio narrators using Hugging Face tools like the model hub, transformers library, Spaces, and Gradio.Join now
","['https://dl-staging-website.ghost.io/content/images/2024/03/The-Batch-ads-and-exclusive-banners---2024-03-05T085620.866.png', 'https://dl-staging-website.ghost.io/content/images/2024/03/unnamed---2024-03-06T161846.471.png']"
Google Releases Open Source LLMs,"Google asserted its open sourcebona fideswith new models.
What’s new:Googlereleasedweights for Gemma-7B, an 8.5 billion-parameter large language model intended to run GPUs, and Gemma-2B, a 2.5 billion-parameter version intended for deployment on CPUs and edge devices. Each size isavailablein two versions: pretrained base model and one fine-tuned to follow instructions.
How it works:Gemma models arebasedon the architecture used in Google’s larger Gemini. Unlike Gemini, they’re not multimodal.
Behind the news:Google has a rich history of open source AI projects including AlphaFold, TensorFlow, several versions of BERT and T5, and the massive Switch. Lately, though, its open source efforts have been overshadowed by open large language models (LLMs) from Meta, Microsoft, and Mistral.ai. LLMs small enough to run on a laptop have opened open source AI to an expanding audience of developers.
Why it matters:Gemma raises the bar for models of roughly 7 billion parameters. It delivers exceptional performance in a relatively small parameter counts, expanding the options for developers who are building on top of LLMs.
We’re thinking:Gemma confirms Google’s commitment to open source and outperforms top open models of equal size. It’s likely to spur further innovation, especially in AI foredge devices, and keep the Google name in front of enterprising open source developers.
",['https://dl-staging-website.ghost.io/content/images/2024/03/unnamed---2024-03-06T161921.552-1.gif']
Context Is Everything,"Correction: This article has been corrected to state that Gemini 1.0 produced anachronistic images of historical scenes. An earlier edition incorrectly stated that Gemini 1.5 Pro generated anachronistic images.
An update of Google’s flagship multimodal model keeps track of colossal inputs, while an earlier version generated some questionable outputs.
What's new:GoogleunveiledGemini 1.5 Pro, a model that can converse about inputs as long as books, codebases, and lengthy passages of video and audio (depending on frame and sample rates). However an earlier version, recently enabled to generate images, produced wildly inaccurate images of historical scenes.
How it works:Gemini 1.5 Proupdates the previous model with a mixture-of-experts architecture, in which special layers select which subset(s) of a network to use depending on the input. This enables the new version to equal or exceed the performance of the previousGemini 1.0 Ultrawhile requiring less computation.
Alignment with what?:The earlier Gemini 1.0 recently wasupdatedto allow users to generate images using a specially fine-tuned version ofImagen 2. However, this capability backfired when social media posts appeared in which the system, prompted to produce pictures of historical characters and situations, anachronistically populated them with people of color, who would not have been likely to be present. For instance, the model illustrated European royalty, medieval Vikings, German soldiers circa 1943 — all of whom were virtually exclusively white — as Black, Asian, or Native American. Google quicklydisabledimage generation of people for “the next couple of weeks” and explained that fine-tuning intended to increase diverse outputs did not account for contexts in which diversity was inappropriate, and fine-tuning intended to keep the model from fulfilling potentially harmful requests also kept it from fulfilling harmless requests. But other users found flaws in text output as well. One asked Gemini who had a greater negative impact on society: Adolf Hitler, who presided over the murder of roughly 9 million people, or “Elon Musk tweeting memes.” The model replied, “It is difficult to say definitively who had a greater negative impact on society.” The ensuing controversy called into question not only Google’s standards and procedures for fine-tuning to ensure ethics and safety, but also its motive for building the model.
Why it matters:Gemini 1.5 Pro’s enormous context window radically expands potential applications and sets a high bar for the next generation of large multimodal models. At the same time, it’s clear that Google’s procedures for aligning its models to prevailing social values were inadequate. This shortcoming derailed the company’s latest move to one-up its big-tech rivals and revived longstanding worries that its management places politics above utility to users.
We’re thinking:How to align AI models to social values is a hard problem, and approaches to solving it are in their infancy. Google acknowledged Gemini’s shortcomings, went back to work on image generation, and warned that even an improved version would make mistakes and offend some users. This is a realistic assessment following a disappointing product launch. Nonetheless, the underlying work remains innovative and useful, and we look forward to seeing where Google takes Gemini next.
",['https://dl-staging-website.ghost.io/content/images/2024/02/GROQ-LLMPERF.webp']
Blazing Inference Speed,"An upstart chip company dramatically accelerates pretrained large language models.
What’s new:Groq offers cloud access to Meta’s Llama 2 and Mistral.ai’s Mixtral at speeds an order of magnitude greater than other AI platforms. Registered users can try ithere.
How it works:Groq’s cloud platform is based on its proprietary GroqChip, a processor specialized for large language model inference that the company calls a language processing unit or LPU. The company plans to serve other models eventually, but its main business is selling chips. It focuses on inference on the theory that demand for a model’s inference can increase while demand for its training tends to be fixed.
Behind the news:Groq founder Jonathan Ross previously worked at Google, where he spearheaded the development of that company’stensor processing unit(TPU), another specialized AI chip.
Why it matters:Decades of ever faster chips have proven that users need all the speed they can get out of computers. With AI, rapid inference can make the difference between halting interactions and real-time spontaneity. Moreover, Groq shows that there’s plenty of innovation left in computing hardware as processors target general-purpose computing versus AI, inference versus training, language versus vision, and so on.
We’re thinking:Autonomous agents based on large language models (LLMs) can get a huge boost from very fast generation. People can read only so fast, the faster generation of text that’s intended to be read by humans has little value beyond a certain point. But an agent (as well as chain-of-thought and similar approaches to prompting) might need an LLM to “think” through multiple steps. Fast LLM inference can be immensely useful for building agents that can work on problems at length before reaching a conclusion.
Join “Prompt Engineering with Llama 2” and learn best practices for model selection and prompting, advanced prompting techniques, and responsible use of large language models, all while using Meta Llama 2 Chat, Llama Guard, and Code Llama.Sign up for free
","['https://dl-staging-website.ghost.io/content/images/2024/02/The-Batch-ads-and-exclusive-banners---2024-02-20T090733.220.png', 'https://dl-staging-website.ghost.io/content/images/2024/02/AGENT_1200px--1-.gif']"
OpenAI’s Next Act?,"OpenAI is focusing on autonomous agents that take action on a user’s behalf.
What’s new:The maker of ChatGPT is developing applications designed to automate common digital tasks by controlling apps and devices,The Informationreported.
How it works:OpenAI has two agent systems in the works. It has not revealed any findings, products, or release dates.
Behind the news:Agents are on Silicon Valley’s radar, especially since January’s Consumer Electronics Showdebutof the Rabbit R1, which accepts voice commands to play music, order food, call a car, and so on. Several other companies, academic labs, and independent developers are pursuing the concept as well.
Why it matters:Training agents to operate software designed for humans can be tricky. Some break down tasks into subtasks but struggle with executing them. Others have difficulty with tasks they haven’t encountered before oredge casesthat are unusually complex. However, agents are becoming more reliable in a wider variety of settings as developers push the state of the art forward.
We’re thinking:We’re excited about agents! You can learn about agent technology in our short course, “LangChain for LLM Application Development,” taught by LangChain CEO Harrison Chase and Andrew.
",['https://dl-staging-website.ghost.io/content/images/2024/02/PRUNING--1-.gif']
Generated Video Gets Real(er),"OpenAI’s new video generator raises the bar for detail and realism in generated videos — but the company released few details about how it built the system.What’s new:OpenAI introducedSora, a text-to-video model that can produce extraordinarily convincing, high-definition videos up to one minute long. You can see exampleshere.What we know:Sora is alatent diffusionmodel that learned to transform noise into videos using an encoder-decoder and transformer. The system was trained on videos up to 1,920x1,080 pixels and up to one minute long.
What we don’t know:OpenAI is sharing the technology with outside researchers charged with evaluating its safety,The New York Timesreported. Meanwhile, the company published neither quantitative results nor comparisons to previous work. Also missing are detailed descriptions of model architectures and training methods. (Some of the results suggest that Sora was trained not only to remove noise from tokens, but also topredict future tokensandgenerate tokens in between other tokens.) No information is available about the source(s) of the dataset or how it may have been curated.Qualitative results:Sora’s demonstration output is impressive enough to have sparkedargumentsover the degree to which Sora “understands” physics. A photorealistic scene in which “a stylish woman walks down a Tokyo street filled with warm glowing neon” shows a crowded shopping district filled with believable pedestrians. The woman’s sunglasses reflect the neon signs, as does the wet street. Halfway through its one-minute length, the perspective cuts — unprompted and presumably unedited — to a consistent, detailed close-up of her face. In another clip, two toy pirate ships bob and pitch on a frothing sea of coffee, surrounded by a cup’s rim. The two ships maintain their distinctiveness and independence, their flags flutter in the same direction, and the liquid churns fantastically but realistically. However, as OpenAI acknowledges, the outputs on display are not free of flaws. For instance, the pirate-battle cup’s rim, after camera motion has shifted it out of the frame, emerges from the waves. (Incidentally, the Sora demos are even more fun withsoundtracksgenerated by Eleven Labs.)
Why it matters:While we’ve seentransformers for video generation,diffusion models for video generation, anddiffusion transformers for images, this is an early implementation of diffusion transformers for video generation (along with a recentpaper). Sora shows that diffusion transformers work well for video.
We’re thinking:Did Sora learn a world model? Learning to predict the future state of an environment, perhaps given certain actions within that environment, is not the same as learning depict that environment in pixels — just like the ability to predict that a joke will make someone smile is different than the ability to draw a picture of that smile. Given Sora’s ability to extrapolate scenes into the future, it does seem to have some understanding of the world. Its world model is also clearly flawed — for instance, it will synthesize inconsistent three-dimensional structures — but it’s a promising step toward AI systems that comprehend the 3D world through video.
",['https://dl-staging-website.ghost.io/content/images/2024/02/unnamed---2024-02-21T181123.082.png']
Competition Heats Up in AI Chips,"Huawei is emerging as an important supplier of AI chips.
What’s new:Amid a U.S. ban on exports of advanced chips to China, demand for Huawei’s AI chips is so intense that the company is limiting production of the chip that powers one of its most popular smartphones so it can serve the AI market,Reutersreported.
Demand and supply:China’s biggest chip fabricator, Semiconductor Manufacturing International Corp. (SMIC), fabricates both the Ascend 910B, which is optimized to process neural networks, and the Kirin chip that drives Huawei’s popular Mate 60 phone. Production capacity is limited, so making more Ascend 910Bs means making fewer Kirins.
Behind the news:Nvidia accounted for 90 percent of the market for AI chips in China prior to the advent of U.S. export restrictions. China has responded to the limits by building its ability to manufacture advanced chips domestically — a tall order, since it requires technology that is very difficult to develop. In August, Baidu ordered 1,600 Ascend 910B chips for delivery by the end of the year, according to an earlierReutersreport. The order, which is tiny compared to typical data center purchases, nonetheless demonstrated that SMIC could manufacture the chips and that Baidu was experimenting with alternatives to Nvidia in anticipation of even tighter U.S. restrictions on AI chips that took effect in October. Currently, SMIC isgearing upto produce Huawei’s next-generation Ascend chips.
Why it matters:For years, Nvidia’s GPUs have been the only practical choice for processing deep learning models. The company’s lead over competitors both in hardware implementation and software support are likely to protect its dominant position for some time to come. However, competitors like AMD and Huawei are beginning to nip at Nvidia’s heels. That means more hardware options for developers, and the competition may drive lower prices and still higher performance.
We’re thinking:AI chips are at the heart of the current technologicalcompetitionbetween the U.S. and China. While Huawei and SMIC still have a lot to prove in terms of scaling up production, their rate of progress is impressive and illustrates the limits of the current U.S. restrictions.
In our next live workshop, we’ll share how to build high-quality and production-ready applications using tools like Pinecone Canopy and TruLens. Notebooks will be available for participants to explore!Register now
","['https://dl-staging-website.ghost.io/content/images/2024/02/The-Batch-ads-and-exclusive-banners---2024-02-21T092947.270.png', 'https://dl-staging-website.ghost.io/content/images/2024/02/unnamed---2024-02-21T181325.090.gif']"
Gymnastics Judge’s Helper,"Judges in competitive gymnastics are using an AI system to double-check their decisions.
What’s new:Olympic-level gymnastic contests have adopted Judging Support System (JSS), an AI-based video evaluation system built by Fujitsu,MIT Technology Reviewreported. In September and October,  for the first time, judges at the 2023 World Artistic Gymnastics Championships in Antwerp used JSS in competitions that involved the full range of gymnastics equipment including mat, balance beam, parallel bars, pommel horse, and so on.
How it works:Judges penalize gymnasts for imperfections in any pose or move. JSS identifies deviations that correspond to particular penalties. The system can evaluate roughly 2,000 poses and moves with 90 percent accuracy compared to human judges. It can assess both isolated actions and entire routines.
Behind the news:Sporting authorities have embraced AI both inside and outside the arena.
Why it matters:Gymnastic competitors arescoredon subjective criteria such as expression, confidence, and personal style as well as technical competence, raising questions of unconscious bias and whether some judges might favor certain competitors over others. An AI system that tracks technical minutiae may help judges to avoid bias while focusing on the sport’s subjective aspects.
We’re thinking:Tracking gymnasts in motion sets a high bar for AI!
",['https://dl-staging-website.ghost.io/content/images/2024/02/unnamed---2024-02-21T181416.147.png']
Ancient Scrolls Recovered,"Three researchers decoded scrolls that had gone unread since they were turned into charcoal by the eruption of Mount Vesuvius in the year 79.
What’s new:Youssef Nader, Luke Farritor, and Julian Schilliger used neural networks to win the $700,000 grand prize in theVesuvius Challenge, a competition to translate charred papyrus scrolls found in the ruins of a villa at the Roman town of Herculaneum in southern Italy.
How it works:The volcanic eruption covered Herculaneum in ash. It also transformed into carbon the papyrus scrolls, which originally would have unrolled to lengths as long as 30 feet.
Behind the news:The Vesuvius Challenge launched in March 2023 with funding provided by GitHub CEO Nat Friedman.
Why it matters:The winning team’s achievement testifies to the ability of deep learning to help solve difficult problems. And their work may have broader significance: Recovering the entire Herculaneum library could provide insights into literature, philosophy, history, science, and art at the time of Caesar.
We’re thinking:University of Kentucky computer scientist Brent Seales, who helped design the contest as well as pioneering the use of medical imaging and machine learning to read ancient texts, reckons that over 1,000 teams worked on the problem, amounting to 10 person-years and two compute-years. It's a great example of the power of global collaboration and open resources — central facets of the AI community — to find solutions to hard problems.
",['https://dl-staging-website.ghost.io/content/images/2024/02/unnamed--98-.png']
U.S. Restricts AI Robocalls,"The United States outlawed unsolicited phone calls that use AI-generated voices.
What’s new:The Federal Communications Commissionruledthat the current legal restriction on voice communications that use “artificial or prerecorded voices” covers AI-powered voice generation. The ruling followed an incident in which calls that featured the cloned voice of U.S. President Biden were delivered with the apparent intent of interfering with an election.
How it works:The ruling interprets the 1991 Telephone Consumer Protection Act, which controls automated calls, or robocalls. The law gives state attorneys general the power to prosecute robocallers. The FCC hadproposedthe move in January.
Behind the news:In January, two days before a presidential primary election, thousands of members of the Democratic Party in the state of New Hampshirereceivedphone calls in which a cloned voice of President Biden, who is a Democrat, urged them not to vote in a presidential primary election. The call used voice cloning software from Eleven Labs, according to researcherscitedbyWired. New Hampshire investigated the calls as a case of illegal voter suppression. It traced them to two telecommunications companies in the state of Texas and issued cease-and-desist orders and subpoenas to both firms. One, Lingo Telecom, said it is cooperating with federal and state investigators.
Why it matters:For all its productive uses, generative AI offers fresh opportunities to scammers to deceive their marks into handing over things of value. Voice cloning can elevate their appeal by simulating personal, business, political, and other relationships. Election officials are especially concerned about AI’s potential to influence voting, especially as we enter a year that will see over 100 elections in seven of the 10 most populous nations.
We’re thinking:The question of how to safeguard elections against manipulations like the Biden robocall is an urgent one. Devising a tamper-resistant watermark that identifies generated output would discourage misuse. However, providers will have a financialincentivenot to apply such watermarks unless regulators require it.
In our latest course, built in collaboration with Amazon Web Services, you’ll learn to deploy applications based on large language models using a serverless architecture. This enables rapid deployment and liberates you from the complexities of managing and scaling infrastructure.Start today!
","['https://dl-staging-website.ghost.io/content/images/2024/02/The-Batch-ads-and-exclusive-banners---2024-02-05T095912.572.png', 'https://dl-staging-website.ghost.io/content/images/2024/02/unnamed--99-.png']"
GPU Data Centers Strain Grid Power,"The AI boom is taxing power grids and pushing builders of data centers to rethink their sources of electricity.
What’s new:New data centers packed with GPUs optimized for AI workloads are being approved at a record pace,The Informationreported. The extreme energy requirements of such chips are pushing builders to place data centers near inexpensive power sources, which may be far away from where users live.
How it works:The coming generation of GPU data centers promises to supply processing power for the burgeoning AI era. But builders aren’t always able to find electricity to run them.
What they’re saying:“We still don’t appreciate the energy needs of [AI] technology. There's no way to get there without a breakthrough.” — Sam Altman, CEO, OpenAI, on January 16, 2024,quotedbyReuters.
Behind the news:Data centers aloneaccount for1 to 1.5 percent of global demand for electricity. It’s unclear how much of that figure is attributable to AI, but the share is likely to grow.
Why it matters:The world needs innovation in both energy resources and power-efficient machine learning. The dawning era of pervasive AI brings with it the challenge of producing energy to develop and deploy the technology, which can contribute to pollution that disrupts ecosystems and accelerates climate change. Fortunately, AI can shrink the environmental footprint of some energy-intensive activities; for example, searching the web for information generates far less CO2emissions than driving to a library.
We’re thinking:Climate change is a slow-motion tragedy. We must push toward AI infrastructure that uses less energy (for example, by using more efficient algorithms or hardware) and emits less carbon (for example, by using renewable sources of energy). That said, concentrating computation in a data center creates a point of significant leverage for optimizing energy usage. For example, it’s more economical to raise the energy efficiency of 10,000 servers in a data center than 10,000 PCs that carry out the same workload in 10,000 homes.
",['https://dl-staging-website.ghost.io/content/images/2024/02/unnamed---2024-02-14T153023.937.gif']
"Better Images, Less Training","The longer text-to-image models train, the better their output — but the training is costly. Researchers built a system that produced superior images after far less training.
What's new:Independent researcher Pablo Pernías and colleagues at Technische Hochschule Ingolstadt, Université de Montréal, and Polytechnique Montréal builtWürstchen, a system that divided the task of image generation between two diffusion models.
Diffusion model basics:During training, a text-to-image generator based on diffusion takes a noisy image and a text embedding. The model learns to use the embedding to remove the noise in successive steps. At inference, it produces an image by starting with pure noise and a text embedding, and removing noise iteratively according to the text embedding. A variant known as alatent diffusionmodel uses less processing power by removing noise from a noisy image embedding instead of a noisy image.
Key insight:A latent diffusion model typically learns to remove noise from an embedding of an input image based solely on a text prompt. It can learn much more quickly if, in addition to the text prompt, a separate diffusion model supplies a smaller, noise-free version of the image embedding. During training, the two models can be trained separately, enabling them to learn their tasks in a fraction of the usual time. At inference, the models can work efficiently as a stack: one to generate smaller embeddings and the other to generate larger embeddings based on the smaller ones.
How it works:Würstchen involves three components that required training: the encoder-decoder fromVQGAN, a latent diffusion model based onU-Net, and another latent diffusion model based onConvNeXt. The authors trained the models separately on subsets ofLAION-5B, which contains matched images and text descriptions scraped from the web.
Results:The authors compared Würstchen toStable Diffusion 2.1. While they trained both on subsets of LAION-5B, they trained Würstchen  for 25,000 GPU hours while Stable Diffusion took 200,000 GPU hours. The authors generated images based on captions fromMS COCOandParti-prompts. They asked 90 people which output they preferred. The judges expressed little preference regarding renderings of MS COCO captions: They chose Würstchen 41.3 percent of the time, Stable Diffusion 40.6 percent of the time, and neither 18.1 percent of the time. However, presented with the results of Parti-prompts, they preferred Würstchen 49.5 percent of the time, Stable Diffusion’s 32.8 percent of the time, and neither 17.7 percent of the time.
Why it matters:Training a latent diffusion model to denoise smaller embeddings accelerates training, but this tends to produce lower-quality images. Stacking two diffusion models enabled Würstchen to match or exceed the output quality of models with large embeddings while achieving the training speed of models with small embeddings.
We're thinking:25,000 GPU hours is a big reduction from 200,000! Given the cost of GPU hours, an eightfold saving is a big deal.
Join us for an intensive three-week workshop where you’ll learn to operationalize large language models at scale, from learning advanced retrieval augmented generation (RAG) to deploying multimodal applications on the cloud.Register today!
",['https://dl-staging-website.ghost.io/content/images/2024/02/FourthBrain_BatchAd_2.14.24_v1.png']
Nude Deepfakes Spur Legislators,"Sexually explicit deepfakes of Taylor Swift galvanized public demand for laws against nonconsensual, AI-enabled pornography.
What’s new:U.S. lawmakers responded to public outcry over lewd AI-generated images of the singer by proposing legislation that wouldcrack downon salacious images generated without their subject’s permission.
High-profile target:In late January, AI-generated images that appeared to depict Swift in the nude appeared on social media sites including X (formerly known as Twitter) and messaging apps such as Telegram. The deepfakesoriginatedon the image-sharing site 4chan, where users competed to prompt text-to-image generators in ways that bypassed their keyword filters. Swift fans reported the images, which subsequently were removed. Swiftreportedlyis considering legal action against websites that hosted the images.
Behind the news:Sexually explicit deepfakes oftentargetcelebrities, but several recent incidents involve private citizens who were minors at the time.
Why it matters:The Swift incident dramatizes the growing gap between technological capabilities and legal restrictions. The rapid progress of image generators enables unscrupulous (or simply cruel) parties to prey on innocent victims in ways that exact a terrible toll for which reparation may be inadequate or impossible. In many jurisdictions, the laws against nonconsensual pornography don’t account for AI-generated or AI-edited images. To be actionable, for instance, such images must depict the victim’s own body rather than a generated look-alike.We’re thinking:No one, whether a public or private figure, child or adult, should be subject to the humiliation and abuse of being depicted in nonconsensual pornographic images. The U.S., whose constitution guarantees free speech, has weaker tools for silencing harmful messages than other countries. Nonetheless, we hope that Swift gets the justice she seeks and that lawmakers craft thoughtful legislation to protect citizens and provide recourse for victims without banning legitimate applications.
",['https://dl-staging-website.ghost.io/content/images/2024/02/unnamed---2024-02-07T153540.653.gif']
"New Leaderboards Rank Safety, More","Hugging Face introduced four leaderboards to rank the performance and trustworthiness of large language models (LLMs).
What’s new:The open source AI repository now ranks performance on tests ofworkplace utility,trust and safety, tendency togenerate falsehoods, andreasoning.How it works:The new leaderboards implement benchmarks developed by HuggingFace’s research and corporate partners. Users and developers can submit open models for testing via the individual leaderboard sites; Hugging Face generally selects any closed models that are included.
Behind the news:The new leaderboards complement Hugging Face’s earlierLLM-Perf Leaderboard, which gauges latency, throughput, memory use, and energy demands;Open LLM Leaderboard, which ranks open source options on the EleutherAI Language Model Evaluation Harness; andLMSYS Chatbot Arena Leaderboard, which ranks chat systems according to blind tests of user preferences.
Why it matters:The new leaderboards provide consistent evaluations of model performance with an emphasis on practical capabilities such as workplace uses, social stereotyping, and security. Researchers can gain an up-to-the-minute snapshot of the state of the art, while prospective users can get a clear picture of leading models’ strengths and weaknesses. Emerging regulatory regimes such as Europe’sAI Actand the U.S.’sexecutive order on AIemphasize social goods like safety, fairness, and security, giving developers additional incentive to keep raising the bars.
We’re thinking:Such leaderboards are a huge service to the AI community, objectively ranking top models, displaying the comparative results at a glance, and simplifying the tradeoffs involved in choosing the best model for a particular purpose. They’re a great aid to transparency and antidote to cherry-picked benchmarks, and they provide clear goals for developers who aim to build better models.
Retrieval Augmented Generation (RAG) is a powerful way to extend large language models, but to implement it effectively, you need the right retrieval techniques and evaluation metrics. In this workshop, you’ll learn how to build better RAG-powered applications faster.Register now
","['https://dl-staging-website.ghost.io/content/images/2024/02/The-Batch-ads-and-exclusive-banners---2024-02-06T172247.774.png', 'https://dl-staging-website.ghost.io/content/images/2024/02/unnamed---2024-02-07T153736.731.gif']"
GPT-4 Biothreat Risk is Low,"GPT-4 poses negligible additional risk that a malefactor could build a biological weapon, according to a new study.
What’s new:OpenAIcomparedthe ability of GPT-4 and web search to contribute to the creation of a dangerous virus or bacterium. The large language model was barely more helpful than the web.
How it works:The researchers asked both trained biologists and biology students to design a biological threat using either web search or web search plus GPT-4.
Results:Participants who used GPT-4 showed slight increases in accuracy and completeness.Students with GPT-4 scored 0.25 and 0.41 more points on average, respectively, than students in the control group. Experts with access to the less restricted version of GPT-4 scored 0.88 and 0.82 points higher on average, respectively, than experts in the control group. However, these increases were not statistically significant. Moreover, participants who used GPT-4 didn’t show greater innovation, take less time per task, or view their tasks as easier. Even if GPT-4 could be prompted to provide information that would facilitate a biological attack, the model didn’t provide more information than a user could glean by searching the web.
Why it matters:AI alarmists have managed to create a lot of anxiety by promoting disaster scenarios, such as human extinction, that the technology has no clear way to bring about. Meanwhile, the unfounded fears stand to slow down developments that could do tremendous good in the world. Evidence that GPT-4 is no more likely than web search to aid in building a bioweapon is a welcome antidote. (Though we would do well to consider removing from the web unnecessary information that may aid in the making of bioweapons.)
We’re thinking:Large language models, like other multipurpose productivity tools such as web search or spreadsheet software, are potentially useful for malicious actors who want to do harm. Yet AI’s potential in biothreat development garners headlines, while Excel’s is rarely mentioned. That makes it doubly important to quantify the risk in ways that can guide regulators and other decision makers.
",['https://dl-staging-website.ghost.io/content/images/2024/02/unnamed---2024-02-07T153903.061.gif']
Resources for Research,"The United States government wants to connect U.S. AI researchers with resources that can help them develop their projects.
What’s new:The National Artificial Intelligence Research Resource (NAIRR)announcedthe first call for proposals in its pilot program, which will accept applications through March 1. Winning proposals can receive processing power, data, software, and training provided by partner organizations. Another round will kick off in the second quarter of 2024.
How it works:Led by the National Science Foundation, NAIRR aims to support innovative AI research by organizing national compute and other infrastructure to be shared among researchers and educators. The initiative pulls together 10 other federal agencies and 25partnersincluding heavyweights like Amazon, Google, Intel, and OpenAI; startups like Allen Institute for Artificial Intelligence, Anthropic, EleutherAI, Hugging Face, and Weights & Biases; and hardware companies like AMD, Intel, and Nvidia.
Behind the news:Policymakersplannedto organize a national infrastructure for AI research after calls fromprominentresearchers. NAIRR is now open thanks to anexecutive orderissued by the White House in October.
Why it matters: AI has potential to affect all corners of society yet, generally, only wealthy companies can bear the high costs of building and running large machine learning models. Partnership between government, industry, and academia can pool AI resources to cultivate talent throughout society and support important projects that may not serve a corporate agenda.We’re thinking:This is an exciting bid to proliferate AI research. Sharing the fruits of such research via open publications and open source software will bring the technology’s benefits to a wider range of people.
",['https://dl-staging-website.ghost.io/content/images/2024/01/unnamed---2024-01-31T154457.554.gif']
High Yields for Small Farms,"Indian farmers used chatbots and computer vision to produce higher yields at lower costs.
What’s new:The state government of Telangana in South Indiapartneredwith agricultural aid organizationDigital Greento provide AI tools to chili farmers.
How it works:The program, called Saagu Baagu, initially engaged 7,000 small-farm growers of chili peppers. Saagu Baagu provided AI-based tools developed by various Indian tech firms to help the farmers collect market data.
Results:The pilot program lasted 18 months, or three cycles of planting, growing, and harvesting peppers. Farmers in the program grew 21 percent more plants per acre while using 9 percent less pesticide and 5 percent less fertilizer, according to the World Economic Forum. Moreover, with a higher-quality harvest, the farmers increased their sale prices by 8 percent. The Telangana government has expanded the program to 500,000 farmers who grow a wider range of crops including chickpeas, cotton, groundnuts, rice, and turmeric.
Behind the news:The promise of AI-driven agriculture is attracting investments around the world. Last year, Microsoftopen-sourceda suite of AI tools to analyze overhead imagery and sensor data to map soil conditions in real time and forecast temperature, precipitation, and soil moisture for days ahead.
Why it matters:Many of the Telangana farmers rely on what they can grow and sell to support themselves and their families. That makes them especially vulnerable to market fluctuations and climate change. Their situation is not unique to India. Programs like Saagu Baagu could help support small-scale farming across the world.
We’re thinking:Saagu Baagu worked in part because WhatsApp is widely popular throughout India and the chatbot spoke the local language. Smart localization that addresses local technological infrastructures, languages, and agricultural practices can proliferate the benefits of AI in agriculture.
In our new course with Pinecone, you’ll learn how to build six applications that use vector databases, including retrieval augmented generation, facial similarity, and anomaly detection.Sign up now
","['https://dl-staging-website.ghost.io/content/images/2024/01/The-Batch-ads-and-exclusive-banners---2024-01-30T091637.902.png', 'https://dl-staging-website.ghost.io/content/images/2024/01/unnamed---2024-01-31T154942.886.gif']"
AI Jobs Grow Beyond Established Hubs,"An analysis of United States job listings shows AI jobs are growing rapidly outside traditional tech hubs.
What’s new:Researchers at University of Marylandanalyzedthe distribution of AI jobs among U.S. job postings. California hosts the largest concentration, followed by the Washington D.C. metropolitan area (which includes more than one state).
How it works:The authors used an unspecified large language model to identify AI jobs, which they define as ones that require AI skills. They categorized each job by the U.S. state in which it was located. To determine whether a given state’s AI economy was growing or shrinking, they calculated the percentage of total U.S. AI jobs in each state in 2018 and 2023. They also calculated the percentage of each state’s total jobs that required AI skills for both dates.
Behind the news:A 2021 Brookingsreporton U.S. AI jobs focused on metropolitan areas and analyzed not only job postings but also federal grants, research papers, patent filings, and companies. Despite the differences in methodology, it agreed with the new report that investment was driving AI growth outside of the Bay Area. The new report suggests a much wider geographical distribution of AI jobs in 2024 than in 2021. It appears some of the then-emerging industrial investment in AI is bearing fruit.Why it matters:For people who aim to make a career in AI, this report contains double good news: (i) Established AI hubs in the U.S. still host the most new openings and (ii) AI jobs are growing far and wide! As the industry becomes more dispersed geographically, AI builders have more options, organizations can select from a more diverse talent pool, and the technology’s benefits can be shared more broadly.We’re thinking:Although this report focused on the U.S., we believe that growth in AI jobs is a global trend. One contributor is growing acceptance of remote work (which remains more prevalent than it was a few years ago despite its decline as the Covid pandemic has wanted). This means more AI opportunities for everyone, everywhere!
",['https://dl-staging-website.ghost.io/content/images/2024/01/unnamed---2024-01-31T155118.727.gif']
Early Detection for Pancreatic Cancer,"A neural network detected early signs of pancreatic cancer more effectively than doctors who used the usual risk-assessment criteria.
What’s new:Researchers at MIT and oncologists at Beth Israel Medical Center in Bostonbuilta model that analyzed existing medical records to predict the risk that an individual will develop the most common form of pancreatic cancer. The model outperformed commonly used genetic tests.How it works:The authors trained PrismNN, a vanilla neural network, to predict a patient’s risk of receiving a diagnosis of pancreatic ductal adenocarcinoma (PDAC) in the next 6 to 18 months.
Results:PrismNN identified as high-risk 35.9 percent of patients who went on to develop PDAC, with a false-positive rate of 4.7 percent. In comparison, the genetic criteria typically used to identify patients for pancreatic cancer screening flags 10 percent of patients who go on to develop PDAC. The model performed similarly across age, race, gender, and location, although some groups (particularly Asian and Native American patients) were underrepresented in its training data.
Behind the news:AI shows promise in detecting various forms of cancer. In a randomized, controlled trial last year, a neural networkrecognizedbreast tumors in mammograms at a rate comparable to human radiologists. In 2022, an algorithm successfullyidentifiedtumors in lymph node biopsies.
Why it matters:Cancer of the pancreas is one of the deadliest. Only 11 percent of patientssurvivefor 5 years after diagnosis. Most cases aren’t diagnosed until the disease has reached an advanced stage. Models that can spot early cases could boost the survival rate significantly.
We’re thinking:The fact that this study required no additional testing is remarkable and means the authors’ method could be deployed cheaply. However, the results were based on patients who had already been diagnosed with cancer. It remains for other teams to replicate them with patients who have not received a diagnosis, perhaps followed by a randomized, controlled clinical trial.
",['https://dl-staging-website.ghost.io/content/images/2024/01/unnamed---2024-01-24T145719.589.gif']
"AI Creates Jobs, Study Suggests","Europeans are keeping their jobs even as AI does an increasing amount of work.
What’s new:Researchers at the European Central Bankfoundthat employment in occupations affected by AI rose over nearly a decade.
How it works:The authors considered jobs that were found to be affected by AI over the past decade according totwostudies. As a control group, they considered jobs affected by software generally (“recording, storing, and producing information, and executing programs, logic, and rules”), as detailed in one of the studies. They measured changes in employment and wages in those jobs based on asurveyof workers in 16 European countries between 2011 and 2019.
Results:The researchers found that exposure to AI was associated with greater employment for some workers and had little effect on wages.
Behind the news:Other studies suggest that automation in general and AI technology in particular may benefit the workforce as a whole.
Yes, but:It may be too soon to get a clear view of AI’s impact on employment, the authors point out. The data that underlies every study to date ends in 2019, predating ChatGPT and the present wave of generative AI. Furthermore, the impact of AI in European countries varies with their individual economic conditions (for instance, Greece tends to lose more jobs than Germany).
Why it matters:Many employees fear that AI — and generative AI in particular — will take their jobs. Around the world, the public isnervousabout the technology’s potential impact on employment. Follow-up studies using more recent data could turn these fears into more realistic — and more productive — appraisals.
We’re thinking:AI is likely to take some jobs. We feel deeply for workers whose livelihoods are affected, and society has a responsibility to create a safety net to help them. To date, at least, the impact has been less than many observers feared. One reason may be that jobs are made up of many tasks, and AI automates tasks rather than jobs. In many jobs, AI can automate a subset of the work while the jobs continue to be filled by humans, who may earn a higher wage if AI helps them be more productive.
Automated testing of applications based on large language models can save significant development time and cost. In this course, you’ll learn to build a continuous-integration pipeline to evaluate LLM-based apps at every change and fix bugs early for efficient, cost-effective development.Enroll for free
","['https://dl-staging-website.ghost.io/content/images/2024/01/The-Batch-ads-and-exclusive-banners---2024-01-23T083847.050.png', 'https://dl-staging-website.ghost.io/content/images/2024/01/unnamed---2024-01-24T145859.354.gif']"
Sovereign AI,"Governments want access to AI chips and software built in their own countries, and they are shelling out billions of dollars to make it happen.What’s new:Nations across the world are supporting homegrown AI processing and development,The Economistreported.How it works:Governments want AI they can rely upon for state use. The U.S. and China each promised to invest around $40 billion in the field in 2023. Another 6 countries — France, Germany, India, Saudi Arabia, the UAE, and the UK — pledged a combined $40 billion. Different governments are emphasizing different capabilities.
Behind the news:Even as governments move toward AI independence, many are attempting to influence international politics and trade to bolster their positions.
Why it matters:AI has emerged as an important arena for international competition, reshaping global society and economics, generating economic growth, and affecting national security. For engineers, the competition means that governments are competing to attract talent and investment, but they’re also less inclined to share technology across borders.We’re thinking:We understand governments’ desires to ensure access to reliable AI, but focusing on sovereignty above all is misguided. In a networked world, developments can’t be contained to one country. Cooperation ensures that development proceeds at a rapid pace and benefits everyone.
",['https://dl-staging-website.ghost.io/content/images/2024/01/unnamed--94-.png']
AI Busts Out at CES,"The 2024 Consumer Electronics Show in Las Vegas showcased products that take advantage of increasingly powerful, increasingly accessible AI capabilities.
What’s new:Many debuts at themassiveCES show showed that large language models (LLMs) are moving beyond browsers and smartphones.
Best of show:The show’s surprise hit was a portable personal assistant. LLM-powered automobile dashboards and an AI accelerator card also stood out.
Why it matters:Flashy CES demos often mask underdeveloped products and vaporware. But this year, AI for processing voice, text, and images is mature enough to enable product designers to focus on everyday use cases and intuitive user experiences. While some of this year’s AI-powered debuts seemed like overkill — for instance, the computer vision-equippedFlappiecat door that won’t open while your pet has a mouse in its jaws — others suggest that startups and giants alike are rethinking the technology’s capacity to simplify and enhance daily life and work.
We’re thinking:Not long ago, simply connecting a home appliance to the internet earned the designation “smart.” Increasingly, AI is making that label credible.
",['https://dl-staging-website.ghost.io/content/images/2024/01/unnamed---2024-01-17T145556.800.gif']
OpenAI Expands Platform Play,"The GPT Store is open for business, providing curated, searchable access to millions of chatbots tailored for specific purposes.
What’s new:OpenAIlaunchedthe GPT Store for paid ChatGPT accounts, making it far easier to find useful GPTs (instances of ChatGPT conditioned by user-submitted prompts). The store lets subscribers browse by category, search by keywords, and create their own chatbots. The company introduced GPTs in November as a free offering without search or curation.
How it works:Access to the store is rolling out in phases and isn’t yet available to all subscribers as of this writing.
Why it matters:The GPT Store strengthens ChatGPT’s utility as a platform for others to build upon and seems designed to drive paid subscriptions. It enables developers to share applications based on OpenAI’s technology and holds out hope that they’ll be rewarded for their effort.We’re thinking:The GPT concept enables anyone, even without a background in coding, to build and share powerful applications quickly and easily. The current implementation seems like a toe in the water. If it proves popular, it could significantly deepen OpenAI’s moat, as the Apple and Android stores have done for Apple and Google respectively.
Learn about machine learning operations for large language models (LLMOps) in our new short course, built in collaboration with Google Cloud. Explore the LLMOps pipeline for pre-processing data, fine-tuning LLMs, and deploying custom LLMs tailored to your applications.Enroll now
","['https://dl-staging-website.ghost.io/content/images/2024/01/The-Batch-ads-and-exclusive-banners---2024-01-16T090702.307.png', 'https://dl-staging-website.ghost.io/content/images/2024/01/unnamed---2024-01-17T145821.116.gif']"
Standard for Media Watermarks,"An alliance of major tech and media companies introduced a watermark designed to distinguish real from fake media starting with images.
What’s new:TheCoalition for Content Provenance and Authenticity(C2PA) offers an open standard that marks media files with information about their creation and editing. C2PA’s 30 members, including both tech powers (Adobe, Google, Intel, Microsoft, X) and media outlets (BBC, CBC,The New York Times) will deploy the standard in the coming year,IEEE Spectrumreported.
How it works:The C2PA’sContent Credentialsspecification accommodates a variety of file types, but currently it’s implemented mainly for images.
Who’s using it:Image generators from Adobe and Microsoft stamp their outputs with Content Credential watermarks, marking them as synthetic; Microsoft alsopromoteswatermarking by political campaigns to help voters differentiate synthetic from non-generated campaign messages. Camera manufacturers Canon,Leica, and Nikon have built prototype cameras that use Content Credentials to mark the origin of photographs. BBC is using the technology to mark images on its website on a trial basis, and Canada’s CBC plans to deploy it in mid-2024.
Yes, but:It may be difficult to fake Content Credentials, but it’s easy toremovethe watermark from images, even from AI-generated ones. Using a Content Credentials-compliant tool like Photoshop, you can disable Content Credentials and save a watermarked image to a different format. This produces an identical image without the watermark.
Behind the news:The C2PA unites the Content Authenticity Initiative (led by Adobe) and Project Origin (led by media companies). Nonetheless, the field remains fragmented. For instance, Meta (not a C2PA member) has aimed to identify AI-generated media using detection software. However, C2PA argues that detectors aren’t sufficiently effective; the winner of a Meta deepfake-detection challenge identified generated content only65 percentof the time. Top AI companiescommittedto developing their own watermarking mechanisms, but they haven’t settled on Content Credentials or another standard.
Why it matters:Distinguishing generated text, imagery, and audio from media that accurately depicts real-world events is a key challenge for the generative AI era. The coming year will test that ability as 78 countries gear up elections that will affect roughly half the world’s population. Already, campaigns have used generated imagery inArgentina,New Zealand,South Korea, theUnited States, and other nations.GoogleandMetaresponded by tightening restrictions on political advertisers’ use of generative AI. The EU’s AI Act willrequireclear labeling of AI-generated media, and the U.S. Federal Election Commission plans torestrictads that depict political opponents saying or doing things they did not actually say or do. If Content Credentials proves effective in the coming election season, it may ease the larger problem of identifying generated media in a variety of venues where authenticity is important.
We’re thinking:A robust watermark can identify both traditional and AI-generated media for users and algorithms to treat accordingly. It can also potentially settle claims that a doctored image was authentic or that authentic work was doctored. However, we worry that watermarking generated outputs may prove to be adisadvantage in the market, creating a disincentive for makers of software tools to provide it and users to use it. With heavyweight members from both tech and media, C2PA may be able to build sufficient momentum behind the watermarking to make it stick.
",['https://dl-staging-website.ghost.io/content/images/2024/01/unnamed--91-.png']
Deep Learning Discovers Antibiotics,"Biologists used neural networks to find a new class of antibiotics.
What’s new:Researchers at MIT and Harvardtrainedmodels to screen chemical compounds for those that kill methicillin-resistantStaphylococcus aureus(MRSA), the deadliest among bacteria that have evolved to be invulnerable to common antibiotics, and aren’t toxic to humans.
How it works:The authors built a training set of 39,312 compounds including most known antibiotics and a diverse selection of other molecules. In a lab, they tested each compound for its ability to inhibit growth of MRSA and its toxicity to human liver, skeletal muscle, and lung cells. Using the resulting data, they trained four ensembles of 20 graph neural networks each to classify compounds for (i) antibiotic properties, (ii) toxicity to the liver, (iii) toxicity to skeletal muscles, and (iv) toxicity to the lungs.
Results:Of the compounds predicted to be likely antibiotics and nontoxic, the authors lab-tested 241 that were not known to work against MRSA. Of those, 8.7 percent inhibited the bacterium’s growth. This exceeds the percentage of antibiotics in the training set (1.3 percent), suggesting that the authors’ approach could be a useful first step in finding new antibiotics. The authors also tested 30 compounds predicted not to be antibiotics. None of them (0 percent) inhibited the bacterium’s growth — further evidence that their approach could be a useful first step. Two of the compounds that inhibited MRSA share a similar and novel mechanism of action against bacteria and also inhibited other antibiotic-resistant infections in lab tests. One of them proved effective against MRSA infections in mice.
Behind the news:Most antibiotics currently in use were discovered in the mid-20th century, a golden age of antibiotics, which brought many formerly deadly pathogens under control. Modern techniques, including genomics and synthetic antibiotics, extended discoveries through the end of the century by identifying variants on existing drugs. However, in the 21st century, new antibiotics have either been redundant or haven’t been clinically successful, a report by the National Institutes of Healthnoted. At the same time, widespread use of antibiotics has pushed many dangerous bacteria to evolve resistance. Pathogens chiefly responsible for a variety of ailments are generally resistant even to antibiotics reserved for use as a last resort.Why it matters:Antibiotic-resistant infections are among the top global public health threats directly responsible for 1.27 million deaths in 2019,according tothe World Health Organization.New options, as well as efforts to fight the emergence of resistant strains, are needed.
We’re thinking:If neural networks canidentifynew classes of medicines, AI could bring a golden age of medical discovery. That hope helps to explain why pharmaceutical companies arehiringmachine learning engineers at unprecedented rates.
",['https://dl-staging-website.ghost.io/content/images/2024/01/unnamed---2024-01-10T150551.860.gif']
OpenAI Revamps Safety Protocol,"Retrenching after its November leadership shakeup, OpenAI unveiled a new framework for evaluating risks posed by its models and deciding whether to limit their use.
What’s new:OpenAI’ssafety frameworkreorganizes pre-existing teams and forms new ones to establish a hierarchy of authority with the company’s board of directors at the top. It defines four categories of risk to be considered in decisions about how to use new models.
How it works:OpenAI’sPreparedness Teamis responsible for evaluating models. The Safety Advisory Group, whose members are appointed by the CEO for year-long terms, reviews the Preparedness Team’s work and recommends approaches to deploying models and mitigating risks, if necessary. The CEO has the authority to approve and oversee recommendations, overriding the Safety Authority Group if needed. OpenAI’s board of directors can overrule the CEO.
Behind the news:The Preparedness Team and Safety Advisory Group join a number of safety-focused groups within OpenAI. TheSafety Systems Teamfocuses on mitigating risks after a model has been deployed; for instance, ensuring user privacy and preventing language models from providing false information. TheSuperalignment Team, led by Ilya Sutskever and Jan Leike, is charged with making sure hypothetical superintelligent systems, whose capabilities would surpass humans, adhere to values that benefit humans.Why it matters:AI is an extraordinarily powerful technology whose ultimate impacts are difficult to foresee. OpenAI has invested consistently in AI safety since its inception — even if purportedly cautious moves like keeping its GPT-2 large language model under wraps often looked as much like publicity stunts as safety measures — and its practices are likely to influence those of other AI companies. Furthermore, OpenAI has faced internalchaospartly over concerns about safety and governance. Clear protocols in these areas could prevent future strife and stabilize the company to the benefit of its users, employees, and investors.
We’re thinking:OpenAI’s safety framework looks like a step forward, but its risk categories focus on long-term, low-likelihood outcomes (though they stop short of considering AI’s hypothetical, and likely mythical, existential risk to humanity). Meanwhile, clear and present safety issues, such as social bias and factual accuracy, are well known to afflict current models including OpenAI’s. We hope that the Preparedness Team promptly adds categories that represent safety issues presented by today’s models.
In this short course, you’ll dive into LangChain.js, a JavaScript framework for building applications based on large language models, and learn how to craft powerful, context-aware apps. Elevate your machine learning-powered development skills using JavaScript.Sign up today
","['https://dl-staging-website.ghost.io/content/images/2024/01/Version2_DeepLearning_LangchainJS_Banner_2070x1080.png', 'https://dl-staging-website.ghost.io/content/images/2024/01/unnamed--48-.jpg']"
AGI Defined,"How will we know if someone succeeds in building artificial general intelligence (AGI)? A recent paper defines milestones on the road from calculator to superintelligence.
What’s new:Researchers at Google led by Meredith Ringel Morrisproposea taxonomy of AI systems according to their degree of generality and ability to perform cognitive tasks. They consider today’s large multimodal models to be “emerging AGI.”
AGI basics:Artificial general intelligence is commonly defined as AI that can perform any intellectual task a human can. Shane Legg (who co-founded DeepMind) and Ben Goertzel (co-founder and CEO ofSingularityNet) coined the term AGI for a 2007 collection ofessays. Subsequently, companies like DeepMind and OpenAI, which explicitly aim to develop AGI, propelled the idea into the mainstream.
How it works:The taxonomy categorizes systems as possessing narrow skills (not AGI) or general capabilities (AGI). It divides both narrow and general systems into five levels of performance beyond calculator-grade Level 0. It also includes a metric for degree of autonomy.
Yes, but:The authors’ definition identifies some classes of tasks that contribute to generality, but it includes neither a list of tasks a system must perform to be considered general nor a method for selecting them. Rather, the authors call on the research community to develop a “living benchmark” for generality that includes a mechanism for adding novel tasks.
Why it matters:AGI is one of the tech world’s hottest buzzwords, yet it has had no clear definition, and various organizations propose different definitions. This lack of specificity makes it hard to talk about related technology, regulation, and other topics. The authors’ framework, on the other hand, supports a more nuanced discussion of the path toward AGI. And it may have high-stakes business implications: Under the terms of their partnership, OpenAI can withhold from Microsoft models that attain AGI. Applying the authors’ taxonomy would make it harder for one of the parties to move the goalposts.
We’re thinking:Defining AGI is tricky! For instance, OpenAI defines AGI as “a highly autonomous system that outperforms humans at most economically valuable work.” This definition, had it been formulated in the early 1900s, when agriculture accounted for 70 percent of work globally, would have described the internal combustion engine.
",['https://dl-staging-website.ghost.io/content/images/2024/01/unnamed---2024-01-10T150942.421.gif']
"Text or Images, Input or Output","GPT-4V introduced a large multimodal model that generates text from images and, with help from DALL-E 3, generates images from text. However, OpenAI hasn’t fully explained how it built the system. A separate group of researchers described their own method.
What's new:Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov at Carnegie Mellon University proposedGenerating Images with Large Language Models(GILL), a training method that enables a large language model and a text-to-image generator to use both text and images as either input or output. Given text and/or image input, it decides whether to retrieve existing images or generate new ones.
Key insight:Models like CLIP and ImageBind map text and image inputs to a similar embedding space, so closely related text and images have similar embeddings. This approach enables a large multimodal model to process both data types. Text outputs, too, can be mapped to the same embedding space, so an image decoder, such as a diffusion model, can use them to produce images or an image retriever to retrieve images.
How it works:The authors used a pretrainedOPTlarge language model,ViT-Limage encoder (taken from CLIP), and pretrained Stable Diffusion text-to-image generator. The authors trained ViT-L to map its embeddings to those produced by OPT. They trained OPT to recognize prompts that request an image and enabled the system to either generate or retrieve images. Finally, a separate linear classifier learned whether to retrieve or generate images.
Results:VISTis a dataset of 20,000 visual stories, each of which comprises five captioned images. The authors evaluated GILL’s and Stable Diffusion’s abilities, given the final caption or all five captions, to generate the final image in each story based on CLIP similarity scores between generated and ground-truth images. Given one caption, GILL achieved 0.581 similarity and Stable Diffusion achieved 0.592 similarity. Given five captions, GILL achieved 0.612 similarity and Stable Diffusion scored 0.598 similarity, highlighting GILL’s ability to use the context afforded by more extensive input. It did even better (0.641 similarity) given both captions and images, which Stable Diffusion couldn’t handle. The authors also evaluated how well their system retrieved the correct last image from VIST given the 5 captions and the first 4 images. GILL retrieved the correct image 20.3 percent of the time, while their ownFROMAGeretrieved the correct image 18.2 percent of the time. In comparison, CLIP, given the 5 captions (without the images), retrieved the correct image 8.8 percent of the time.
Why it matters:Models that wed text and images are advancing rapidly. GILL and other recent models extend single-image input and/or output to any combination of images and text. This capability — which GILL achieves by mapping embeddings of image and text to one another — gives the models more context to generate more appropriate output.
We’re thinking:The authors add an interesting twist: Rather than generating images, the system can choose to retrieve them. Sometimes an existing image will do.
Join a free webinar on January 11, 2024, featuring experts from Snowflake and Landing AI to explore how large vision models (LVMs) are transforming the image processing landscape.Learn more about the session and register here
",['https://dl-staging-website.ghost.io/content/images/2024/01/image--10-.png']
GPT-4 Wouldn’t Lie to Me . . . Would It?,"It’s well known that large language models can make assertions that are blatantly false. But can they concoct outright lies?
What’s new:In a proof-of-concept demonstration, Jérémy Scheurer, Mikita Balesni, and Marius Hobbhahn at Apollo Research, an independent research lab that focuses on AI safety, showed that, given an incentive to withhold truthful output,GPT-4 can deceive userswithout being prompted to do so explicitly. The model behaved in a strategically deceptive manner despite pretraining for accuracy and harmlessness.
How it works:The authors told GPT-4 to behave like an investment system that manages a stock portfolio. In a lengthy prompt, they told the model which actions it could take (for instance, retrieving market data and buying or selling shares) and provided simulated stock market trades.
Results:GPT-4 bought the tipped stock about 75 percent of the time. Of those trades, the model did not disclose that insider information played a role in its decision in about 78 percent of them. Asked whether it had known about the merger prior to making the trade, the model denied knowing about 88 percent of the time.
Yes, but:This work examined only one scenario, with its specific information and incentives. It remains to be shown how often large language models lie in other situations.
Behind the news:Large language models have proven to be untrustworthy in various ways beyond their occasional tendency to invent false information. They have showndeceptive behavior. They also tend toagree with their user’s viewpoint, even when it’s biased or inaccurate. Furthermore, they are known torepeat common misconceptions.
Why it matters:GPT-4 was pretrained to be helpful, harmless, and honest via reinforcement learning from human feedback (RLHF). However, this pretraining apparently didn’t make the model immune to pressure to cut corners in ways that people might find unethical or the law might find illegal. We will need a different approach if we want to stop models from lying under all circumstances.
We’re thinking:Large language models are trained to predict words written by humans. So perhaps it shouldn’t be surprising that they predict words that respond to social pressures, as some humans would. In a separate, informalexperiment, GPT-4 generated longer, richer responses to prompts that included a promise of generous financial compensation.
",['https://dl-staging-website.ghost.io/content/images/2024/01/unnamed--85-.png']
Sharper Vision for Cancer,"A microscope enhanced with augmented reality is helping pathologists recognize cancerous tissue.
What’s new:The United States Department of Defense is usingmicroscopesthat use machine learning models based on research from Google to detect cancers.How it works:The microscope, which costs $90,000 to $100,000, looks like a typical lab instrument, but it connects to a computer that superimposes the output of computer vision models over the view. Two controlled studies are underway at government hospitals, Defense Department research centers, and at the Mitre Corp., a nonprofit technology lab, where 13 units have been integrated into the regular pathology workflow.
Behind the news:Google researchersproposedan AI-powered augmented reality microscope in 2018, andpublishedits research inNaturein 2019. The U.S. government joined the project in 2020. A 2022 paperdemonstratedthe breast-cancer algorithm’s success at detecting tumors in lymph nodes.
Why it matters:Cancer can be deadly, and early identification of a cancer’s type — and thus how aggressive it is — is a key to effective treatment. Microscopes equipped with computer vision can help pathologists diagnose tumors faster and more accurately. They also may be useful for training new pathologists to identify cancers visually.We’re thinking:Some previous medical AI projects, after initialexcitement, turned out to behardto operationalize due to variations in the surrounding environment and other factors. The relatively controlled nature of pathology samples seems like a good bet for deployment of augmented-reality microscopes. We look forward to the conclusions of the currently ongoing studies.
Learn advanced retrieval-augmented generation (RAG) from Chroma founder Anton Troynikov! In this course, you’ll gain skills in retrieval beyond basic semantic search and experiment with cutting-edge RAG techniques.Sign up for free
","['https://dl-staging-website.ghost.io/content/images/2024/01/The-Batch-ads-and-exclusive-banners--95-.png', 'https://dl-staging-website.ghost.io/content/images/2024/01/unnamed--86-.png']"
AI Against Climate Change,"How can AI help to fight climate change? A new report evaluates progress so far and explores options for the future.What’s new:The Innovation for Cool Earth Forum, a conference of climate researchers hosted by Japan, published a roadmap for the use of data science, computer vision, and AI-driven simulation to reduce greenhouse gas emissions. The roadmap evaluates existing approaches and suggests ways to scale them up.How it works:The roadmap identifies 6 “high-potential opportunities”: activities in which AI systems can make a significant difference based on the size of the opportunity, real-world results, and validated research. The authors emphasize the need for data, technical and scientific talent, computing power, funding, and leadership to take advantage of these opportunities.
Why it matters:AI has demonstrated its value in identifying sources of emissions, optimizing energy consumption, and developing and understanding materials. Scaling and extending this value in areas that generate the most greenhouse gasses — particularly energy generation, manufacturing, food production, and transportation — could make a significant dent in greenhouse gas emissions.We’re thinking:AI also has an important role to play in advancing the science of climate geoengineering, such as stratospheric aerosol injection (SAI), to cool down the planet. More research is needed to determine whether SAI is a good idea, but AI-enabled climate modeling will help answer this question.
",['https://dl-staging-website.ghost.io/content/images/2024/01/unnamed--46-.jpg']
Generating 2024,"Only one year into the mainstreaming of generative AI, a wondrous horizon stretches before us. We no longer search the internet, we chat with it; we don’t write emails, we ask our assistant in the cloud to do it. We converse with code, conjure images, mold video clips. This new world holds great promise, but also great worries about whether these powers will serve all of us, and serve us well. So,asinyearspast, we asked some of AI’s brightest minds: What is your hope for the coming year? Their answers hold clues to the future and our place in it.
",['https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--81-.png']
Anastasis Germanidis: New Tools to Tell New Stories,"The year 2023 was an inflection point in the development of broadly useful AI systems across text, image, video, audio, and other modalities. At Runway alone, we saw the release of video-generation models such as Gen-1 and Gen-2, as well as tools that enable new forms of creative control with those models. In the coming year, here are some areas where I expect to see continued progress:
Beyond technological advancements, the most rewarding part of building these systems is that, with every update and increase in capabilities, new audiences are introduced to them and new stories are told that weren’t told before. I’m excited to see how that will continue to happen in the coming year.
Anastasis Germanidis is co-founder and CTO of Runway, an applied AI research company shaping the next era of art, entertainment, and human creativity.
",['https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--82-.png']
Sara Hooker: Prioritize Inclusion,"The past year has seen incredible innovation in AI, and I expect as much or more in 2024. The coming year undoubtedly will be a year of rapid progress in models – multimodal, multilingual, and (hopefully) smaller and faster.
To date, models and datasets used for training have been heavilybiasedtowards English-speaking and Western European countries, offering little representation of languages from the Global South and Asia. Even when languages from the Global South are represented, the data almost always is translated from English or Western European languages. In 2023, the “rich got richer” and the “poor got poorer” as breakthroughs facilitated use of widely spoken languages like English while further impeding access for speakers of languages for which much less data is available.
Next year will be the year of Robin Hood, when we try to reshare the gains by closing the language gap. We will see rapid improvement in state-of-the-art multilingual models, as well as innovation in synthetic data generation to build foundation models for specific languages. I believe we will make progress in closing the language gap and strengthen our collective effort to incorporate research, training data, and individuals from across the globe. This will include projects likeAya, a model from Cohere For AI that will cover 101 languages. Bridging the gap is not just a matter of inclusivity, it’s key to unlocking the transformative power of AI and ensuring that it can serve a global audience, irrespective of language or cultural background.
In addition, I expect 2024 to be a year for research bets.Multimodalwill become a ubiquitous term as we move away from subfields dedicated to language, computer vision, and audio in isolation. Models will be able to process multiple sensory inputs at once, more like humans. We will care urgently about model size as we deploy more models in resource-constrained environments. AI models will become smaller and faster. Our lab is alreadypushing the limitsof efficiency at scale, data pruning, and adaptive computing. Localization of models using retrieval augmented generation (RAG) and efficient fine-tuning will be paramount, as everyday users look to unlock the potential in frontier models.
In the coming year, it will be even more important to interrogate the defaults of where, how, and by whom research is done. To date, state-of-the-art models have come from a handful of labs and researchers. The community responsible for recent breakthroughs is so small that I know many of the people involved personally. However, we need to broaden participation in breakthroughs to include the best minds. At Cohere For AI, we are in the second cohort of ourScholars Program, which provides alternative points of entry into research for AI talent around the world.
The compute divide will persist in the coming year. Shortages of compute combined with stockpiling of GPUs mean there won’t be immediate changes in the availability of compute. This year, we launched ourresearch grant program, so independent and academic researchers can access frontier models at Cohere. More needs to be done at national and global scales to bridge the divide for researchers and practitioners.
We are in an interesting time, and it is rare to work on research that is being adopted so quickly. Our ideas not only resonate in AI conferences but have a profound impact on the world around us. In 2024, expect more rapid change and some breakthroughs that make this technology immediate and usable to more humans around the world. By prioritizing inclusivity in model training and fundamental research, we can help ensure that AI becomes a truly global technology, accessible to users from all backgrounds.
Sara Hooker is a senior VP of research at Cohere and leads Cohere For AI, a nonprofit machine learning research lab that supports fundamental enquiry and broad access.
",['https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--43-.jpg']
Percy Liang: Transparency for Foundation Models,"Only a year ago, ChatGPT woke the world up to the power of foundation models. But this power is not about shiny, jaw-dropping demos. Foundation models will permeate every sector, every aspect of our lives, in much the same way that computing and the Internet transformed society in previous generations. Given the extent of this projected impact, we must ask not only what AI can do, but also how it is built. How is it governed? Who decides?
We don’t really know. This is becausetransparencyin AI is on the decline. For much of the 2010s, openness was the default orientation: Researchers published papers, code, and datasets. In the last three years, transparency has waned. Very little is known publicly about the most advanced models (such as GPT-4, Gemini, and Claude): What data was used to train them? Who created this data and what were the labor practices? What values are these models aligned to? How are these models being used in practice? Without transparency, there is no accountability, and we have witnessed the problems that arise from the lack of transparency in previous generations of technologies such as social media.
To make assessments of transparency rigorous, the Center for Research on Foundation Models introduced theFoundation Model Transparency Index, which characterizes the transparency of foundation model developers. The good news is that many aspects of transparency (e.g., having proper documentation) are achievable and aligned with the incentives of companies. In 2024, maybe we can start to reverse the trend.
By now, policymakers widely recognize the need to govern AI. In addition to transparency, among the first priorities isevaluation, which is mentioned as a priority in the United States executive order, the European Union AI Act, and the UK’s new AI Safety Institute. Indeed, without a scientific basis for understanding the capabilities and risks of these models, we are flying blind. About a year ago, the Center for Research on Foundation Models released theHolistic Evaluation of Language Models(HELM), a resource for evaluating foundation models including language models and image generation models. Now we are partnering with MLCommons to develop anindustry standard for safety evaluations.
But evaluation is hard, especially for general, open-ended systems. How do you cover the nearly unbounded space of use cases and potential harms? How do you prevent gaming? How do you present the results to the public in a legible way? These are open research questions, but we are on a short fuse to solve them to keep pace with the rapid development of AI. We need the help of the entire research community.
It does not seem far-fetched to imagine that ChatGPT-like assistants will be the primary way we access information and make decisions. Therefore, the behavior of the underlying foundation models — including any biases and preferences — is consequential. These models are said to align to human values, but whose values are we talking about? Again, due to the lack of transparency, we have no visibility into what these values are and how they are determined. Rather than having these decisions made by a single organization, could we imagine a moredemocraticprocess for eliciting values? It is the integrity and legitimacy of the process that matters. OpenAI wants tofundwork in this area, and Anthropic has someresearchin this direction, but these are still early days. I hope that some of these ideas will make their way into production systems.
The foundation-models semi truck will barrel on, and we don’t know where it is headed. We need to turn on the headlights (improve transparency), make a map to see where we are (perform evaluations), and ensure that we are steering in the right direction (elicit values in a democratic way). If we can do even some of this, we will be in a better place.
Percy Liang is an associate professor of computer science at Stanford, director of the Center for Research on Foundation Models, senior fellow at the Institute for Human-Centered AI, and co-founder of Together AI.
We wish you a skillful new year! Take your generative AI knowledge to the next level with short courses from DeepLearning.AI. Our catalog is available for free for a limited time.Check it out
","['https://dl-staging-website.ghost.io/content/images/2023/12/The-Batch-ads-and-exclusive-banners--93-.png', 'https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--83-.png']"
Sasha Luccioni: Respect for Human Creativity and Agency,"Before this past year, when I told people I worked in AI, more often than not I was met with a blank stare and sometimes a question along the lines of: “You mean like robots?” In the last year, the seemingly magical abilities of AI models, especially large language models (LLMs), have broken into mainstream awareness, and now I’m greeted with questions like: “How does ChatGPT really work?” But if we were more transparent about the sheer amount of human time and labor that went into training LLMs, I’m sure the questions would be more along the lines of: “How do I keep my data from being used for training AI models?” Because as impressive as ChatGPT’s knock-knock jokes or chocolate chip cookie recipes are, they are definitely not magical — they are built upon the work and creativity of human beings, who should be attributed for their contributions.
AI models are black boxes that, to a user, appear to save labor. But, in fact, huge amounts of labor are required to develop them: from the books, websites, drawings, photos, and videos hoovered up without consent to the invisible armies of underpaid workers who spend their days ranking and improving LLM outputs. And all of this training is powered by massive amounts of natural resources that are extracted by still more human labor: rare metals to make those precious GPUs, water to cool them, energy to make them crunch numbers and output probabilities.
Until very recently, issues of copyright and consent were overlooked when it came to AI training data. Existing laws were assumed not to apply to training AI models, and the “move fast and break things” motto prevailed. But in the past year, authors likeSarah SilvermanandGeorge R.R. Martinhave sued AI companies to assert their rights as content creators whose work was used without their permission to train AI models. While it’s too early to say how these lawsuits (and others) will pan out and how that will shape the future of copyright law in the United States and beyond, I hope that new mechanisms will be developed to allow content creators more control over their work. We are starting to see this from organizations likeSpawning, which helped create ai.txt files that restrict the use of content for commercial AI training. I hope to see more AI developers respect these mechanisms and adopt opt-in (as opposed to opt-out) approaches for gathering consent-based datasets.
Apart from training data, development itself requires increasing amounts of labor. A new step recently has been added to the training process:RLHF, or reinforcement learning from human feedback. This step employs human annotators to rank text generated by large language models, providing feedback that makes them better at responding to human instructions and less likely to produce toxic output. This ranking process is done at scale by outsourced workers inoffices in Kenyaandprisons in Finland. Some of these workers are paid less than $2 an hour to label texts for hours on end, although we don’t have the overall numbers because AI companies are increasingly opaque about how they train AI models. Creating data for AI has become anew gig economy— but all this immense amount of human labor and creativity remains largely unseen and unrecognized.
And as AI is increasingly pushing out the very designers and artists whose life’s work was used to train the models in the first place (why pay a photographer when you canuse AI to generate a custom stock photograph on demand), it’s crucial that we stop and reflect upon the relationship between human labor and creativity and AI. AI is truly an exciting new technology, and one that is set to provide huge profits to many tech companies, but artists and gig workers are barely getting crumbs of the pie, if anything at all. It’s not too late to reimagine AI as a technology that respects human agency and creativity by properly recognizing the human time and effort that goes into training AI models.
My hope in 2024 is that we start recognizing the knowledge, wisdom, and creativity that goes into training AI models, being more transparent about AI’s human costs, and developing increasingly human-centric technologies.
Sasha Luccioni is a research scientist and climate lead at HuggingFace, a founding member of Climate Change AI, and a board member of Women in Machine Learning.
",['https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--44-.jpg']
Pelonomi Moiloa: Smaller Models That Learn More From Less Data,"One of my favourite flavours of conversation is listening to reinforcement learning experts talk about their children as reinforcement learning agents. These conversations highlight just how comically far behind humans our machine learning models are. Especially when comparing the ability to acquire knowledge without being told explicitly what to learn and when comparing the amount of information required for that learning.My co-founder has a three-year-old son who is obsessed with cars. It would seem his objective function is to be exposed to as many cars as possible. So much so that he came home from a supercar show ranting and raving about the Daihatsu he saw in the parking lot, because he had never seen a Daihatsu before. On another occasion, when my co-founder told him the vehicle he was pointing at and enquiring about was a truck, the child did not hesitate to know thattruckwas a descriptor for a class of vehicle and not the name of the car.
What makes his little brain decide what is important to learn? How does it make connections? How does it make the inference so quickly across such a vast domain? Fueled solely by a bowl of Otees cereal?What we have been able to achieve with our models as a species is quite impressive. But what I find far less impressive is how big the models are and the exorbitant resources of data, compute, capital, and energy required to build them. My co-founder's child learns far more from far less data, with a lot less energy.This is not only a conundrum of resources for machine learning architects. It has profound implications for implementing AI in parts of the world where not only data but also electricity and computing equipment are severely limited. As AI practitioners, we need to understand how to build smaller, smarter models with less data.Although efforts to put today's top-performing models on mobile devices are driving development of smaller models, prioritising small models that learn from relatively small datasets runs counter to mainstream AI development.AI has the potential to help us understand some of the biggest questions of the universe, and it could provide solutions to some of the most pressing issues of our lifetime, like ensuring that everyone has access to clean energy, clean water, nutritious meals, and quality healthcare; resolving conflict; and overcoming the limitations of human greed. Yet the current mainstream of AI largely overlooks the lives affected by such problems. An approach that does not require the level of capital investment typical of AI would open the AI domain to more people, from more places, so they too can leverage the power of AI for the benefit of their communities.I hope for many things for AI: that regulation and governance will improve, that the people who build the technology will do so with intention and with principles and values grounded in the connection of humanity. But the hope I am focusing on for now is more building of smaller, smarter models with less data to share the benefits of AI throughout the world. What are we working toward if not to make the world a sustainably better place for more people?
Pelonomi Moiloa is CEO of Lelapa AI, a socially grounded research and product lab that focuses on AI for Africans, by Africans.
",['https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--84-.png']
A Year of Innovation and Consternation,"Recent years brought systems that, given a text prompt, generate high-quality text, pictures, video, and audio. In 2023, the wave of generative AI washed over everything. And its expanding capabilities raised fears that intelligent machines might render humanity obsolete.Asinpastyearsat this season, we invite you to settle by the fire and savor 12 months of technological progress, business competition, and societal impact.
",['https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--37-.jpg']
Generative AI Everywhere,"This year, AI became virtually synonymous with generative AI.
What happened:Launched in November 2022, OpenAI’s ChatGPT ushered in a banner year for AI-driven generation of text, images, and an ever widening range of data types.
Driving the story:Tech giants scrambled to launch their own chatbots and rushed cutting-edge natural language processing research to market at a furious pace. Text-to-image generators (also sparked by OpenAI with DALL·E in early 2021) continued to improve and ultimately began to merge with their text-generator counterparts. As users flocked to try out emerging capabilities, researchers rapidly improved the models’ performance, speed, and flexibility.
Gold rush:Generative AI didn’t just thrill customers and businesses; it generated a flood of funding for AI developers. Microsoft invested $13 billion in OpenAI, and Amazon and Google partnered with the nascent startup Anthropic in respective multibillion-dollar investments. Other generative AI startupsraisedhundreds of millions of dollars.
Where things stand:In the span of a year, we went from one chat model from OpenAI to numerous closed, open, and cloud-hosted options. Image generators have made strides in their ability to interpret prompts and produce realistic output. Video and audio generation are becoming widelyavailablefor short clips, and text-to-3D isevolving. 2024 is primed for a generative bonanza, putting developers in a position to build a wider variety of applications than ever before.
",['https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--38-.jpg']
Hollywood Squares Off,"The movie capital became a front line in the battle over workplace automation.
What happened:U.S. film and television writers went on strike in May, and actors followed in July. They took up a variety of issues with their employers, but concern that AI would damage their job prospects prolonged the work stoppage. Both groups inked agreements shortly before the year ended.
Driving the story:Screenwriters negotiated for 148 days, and actors for 118, winning limits on their employers’ abilities to replace them with machine learning models.
AI on the silver screen:Traditional Hollywood studios negotiated alongside the film departments of Amazon, Apple, and Netflix, tech powerhouses that have access to considerable AI expertise. All are likely to use AI to generate text, images, audio, and video.
Where things stand:The unions and studios agreed to use AI while enabling writers and actors to continue to ply their trades. The agreements will remain in force for three years — time enough for both sides to learn a bit about what the technology is and isn’t good for, and to form a vision of its role in the future. Now Hollywood faces the next challenge: Using AI to make better movies that grow the pie for producers and creatives alike.
",['https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--39-.jpg']
Can I Use This Data?,"Information may not want to be free after all.
What happened:The age-old practice of training AI systems on data scraped from the web came into question as copyright owners sought to restrict AI developers from using their works without permission.
Driving the story:Individual copyright holders filed lawsuits against AI companies for training models on their data without obtaining explicit consent, giving credit, or providing compensation. Concurrently, formerly reliable repositories of data on the open web started to require payment or disappeared entirely.
Copyright conundrum:Whether copyright restricts training machine learning models is largely an open question. Laws in most countries don’t address the question directly, leaving it to the courts to interpret which uses of copyrighted works do and don’t require a license. (In the U.S., the Copyright Office deemed generated images ineligible for copyright protection, so training corpuses made up of generated images are fair game.) Japan is a notable exception: The country’s copyright law apparently allows training machine learning models on copyrighted works.
Where things stand:Most copyright laws were written long ago. The U.S. Copyright Act was established in 1790 and was last revised in 1976! Copyright will remain a battlefield until legislatorsupdatelaws for the era of generative AI.
",['https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--40-.jpg']
High Anx-AI-ety,"Angst at the prospect of intelligent machines boiled over in moves to block or limit the technology.
What happened:Fear of AI-related doomsday scenarios prompted proposals to delay research and soul searching by prominent researchers. Amid the doomsaying, lawmakers took dramatic regulatory steps.
Driving the story:AI-driven doomsday scenarios have circulated at least since the 1950s, when computer scientist and mathematician Norbert Weiner claimed that “modern thinking machines may lead us to destruction.” Such worries, amplified by prominent members of the AI community, erupted in 2023.
Regulatory reactions:Lawmakers from different nations took divergent approaches with varying degrees of emphasis on preventing hypothetical catastrophic risks.
Striking a balance:AI has innumerable beneficial applications that we are only just beginning to explore. Excessive worry over hypothetical catastrophic risks threatens to block AI applications that could bring great benefit to large numbers of people. Some moves to limit AI would impinge on open source development, a major engine of innovation, while having the anti-competitive effect of enabling established companies to continue to develop the technology in their own narrow interest. It’s critical to weigh the harm that regulators might do by limiting this technology in the short term against highly unlikely catastrophic scenarios.
Where things stand:AI development is moving too quickly for regulators to keep up. It will require great foresight — and a willingness to do the hard work of identifying real, application-level risks rather than imposing blanket regulations on basic technology — to limit AI’s potential harms without hampering the good that it can do. The EU’s AI Act is a case in point: The bill, initially drafted in 2021, has needed numerous revisions to address developments since then. Should it gain final approval, it will not take effect within two years. By then, AI likely will raise further issues that lawmakers can’t see clearly today.
",['https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--41-.jpg']
Google’s Multimodal Challenger,"Google unveiled Gemini, its bid to catch up to, and perhaps surpass, OpenAI’s GPT-4.
What’s new:Googledemonstratedthe Gemini family of models that accept any combination of text (including code), images, video, and audio and output text and images. The demonstrations and metrics were impressive — but presented in misleading ways.
How it works:Geminiwill come in four versions. (i) Gemini Ultra, which will be widely available next year, purportedly exceeds GPT-4 in key metrics. (ii) Gemini Pro offers performance comparable to GPT-3.5. This model now underpins Google’s Bard chatbot for English-language outside Europe. It will be available for corporate customers who use Google Cloud’s Vertex AI service starting December 13, and Generative AI Studio afterward. (Google did not disclose parameter counts for Pro or Ultra.) Two distilled versions — smaller models trained to mimic the performance of a larger one — are designed to run on Android devices: (iii) Gemini Nano-1, which comprises 1.8 billion parameters, and (iv) Nano-2, at 3.25 parameters. A Gemini Nano model performs tasks like speech recognition, summarization, automatic replies, image editing, and video enhancement in theGoogle Pixel 8 Prophone.
Misleading metrics:The metrics Google promoted to verify Gemini Ultra’s performance are not entirely straightforward. Google pits Gemini Ultra against GPT-4. However, Gemini Ultra is not yet available, while GPT-4 Turbo already surpasses GPT-4, which outperforms Gemini Pro. Gemini Ultra achieved 90 percent accuracy (human-expert level) on MMLU, which tests knowledge and problem-solving abilities in fields such as physics, medicine, history, and law. Yet this achievement, too, is misleading. Ultra achieved this score via chain-of-thought prompting with 32 examples, while most scores on the MMLUleaderboardare 5-shot learning. By the latter measure, GPT-4 achieves better accuracy.
Manipulated demo:Similarly, avideoof Gemini in action initially made a splash, but it was not an authentic portrayal of the model’s capabilities. A Gemini model appeared to respond in real time, using a friendly synthesized voice, to audio/video input of voice and hand motions. Gemini breezily chatted its way through tasks like interpreting a drawing in progress as the artist added each line and explaining a sleight-of-hand trick in which a coin seemed to disappear. However, Googleexplainedin a blog post that the actual interactions did not involve audio or video. In fact, the team had entered words as text and video as individual frames, and the model had responded with text. In addition, the video omitted some prompts.
Why it matters:Gemini joins GPT-4V and GPT-4 Turbo in handling text, image, video, and audio input and, unlike the GPTs, it processes those data types within the same model. The Gemini Nano models look like strong entries in an emerging race to put powerful models on small devices at theedgeof the network.We’re thinking:We celebrate the accomplishments of Google’s scientists and engineers. It’s unfortunate that marketing missteps distracted the community from their work.
",['https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--80-.png']
Europe Clamps Down,"Europe’s sweeping AI law moved decisively toward approval.
What’s new:After years of debate, representatives of the European Union’s legislative and executive branchesagreedon a draft of the AI Act, a comprehensive approach to regulating AI. As the legislative session drew to a close, the representatives negotiated nearly nonstop to approve the bill before the deadline. It will return to Europe’s parliament and member countries for final approval in spring 2024 and take effect roughly two years later.How it works:Theframeworklimits uses of AI that are considered especially risky. Last-minute agreements lightened the burdens on small companies and open source development. It includes the following provisions:
What’s next:The representatives have agreed on these broad strokes, but they will continue to revise the details. After further vetting, the European Parliament will vote again, and acouncilof deputies from each EU member state will also vote, most likely in early 2024. If both bodies approve the bill, it will take effect no later than 2026.
Behind the news:The AI Act has been under construction since 2021. The technology has evolved significantly since then, and the proposal has undergone several revisions to keep pace. The advent of ChatGPT prompted a round of revisions to control foundation models. Negotiations reached fever pitch in late December. France, Germany, and Italy, seeking to protect developers in their countries,soughtto weaken restrictions on foundation models. They were opposed by Spain, whichsoughtto strengthen oversight of the most powerful foundation models. The final negotiations concerned exceptions for police and military use of AI within member states. Franceleda group of countries that pushed for greater military exemptions.
Why it matters:The AI Act is the broadest and most detailed effort to regulate AI to date. The stakes are high: Not only does Europe have a budding AI industry of its own, but EU lawsoften dictatecompanies’ practices outside the union. Yet the bill won’t take effect for years — when AI may present very different challenges.
We’re thinking:Effective regulation should mitigate harm without stifling innovation. The best approach is to regulate applications rather than underlying technology such as foundation models. While the EU restricts some applications in helpful ways, it also limits foundational technology in ways that we expect will hurt innovation in EU member states. We welcome the provisions added at the last moment to lighten the load on small companies and open source developers. These 11th-hour wins reflect the efforts of many people who pushed to protect innovation and openness.
Join our new short course, “Reinforcement Learning from Human Feedback,” and learn a key method to align large language models with human values and preferences. Gain a detailed understanding of the technique and use it to fine-tune Llama 2 for an application.Sign up now
","['https://dl-staging-website.ghost.io/content/images/2023/12/DeepLearning1_GoogleCloudPlatfomr_Banner2_2070x1080--1--1.png', 'https://dl-staging-website.ghost.io/content/images/2023/12/ALLIANCE-1.jpg']"
Champion for Openness,"A new consortium aims to support open source AI.
What’s new:Led by Meta and IBM, dozens of organizations from the software, hardware, nonprofit, public, and academic sectors formed theAI Alliance, which plans to develop tools and programs that aid open development.
How it works:The AI Alliance’s 57 founding members include established companies like AMD, Intel, Oracle, and Sony; startups like Cerebras and Stability AI; nonprofits such as HuggingFace and the Linux Foundation, public institutes like the European Council for Nuclear Research (CERN) and U.S. National Aeronautics and Space Administration (NASA); and universities in Asia, Europe, and North America. The group stated its intention to pursue a variety of projects:
Behind the news:The membership includes organizations that have prioritized open source development including Meta, Stability AI, and the Linux Foundation. Yet several organizations that provide popular open-source models are not represented, including models released under more permissive open source licenses likeGPT Neo​​ andMistral. Major companies like Apple and Google, who have released some of their work under open source licenses, are also absent.
Yes, but:The meaning of “open” is contentious, and AI Alliance does not clearly define it. In large language models, for instance, the spectrum of openness includes:
Why it matters:More openness means faster sharing of knowledge and a greater pace of innovation. The AI Alliance can put substantial resources and breadth of influence behind proponents of openness, potentially acting as a counterweight against well financed commercial interests that are threatened by open source development. For instance, some companies claim that restricting access to AI models is necessary to ensure that bad actors don’t misuse them; of course, it would also eliminate open source competition with those companies. On the other hand, open source advocatesarguethat transparency makes AI models less likely to be dangerous, since anyone can spot dangers and alter the code to reduce them.
We’re thinking:Open source is a powerful engine of innovation that enables people to build freely on earlier developments for the benefit of all. The AI Alliance’s gathering of commercial, institutional, and academic clout looks like a promising approach to promoting openness.
",['https://dl-staging-website.ghost.io/content/images/2023/12/I-JEPA-1.jpg']
Amazon Joins Chatbot Fray,"Amazon launched a chatbot for large companies even as internal tests indicated potential problems.
What’s new:Amazon introducedQ, an AI-powered assistant that enables employees to query documents and corporate systems. Days later, the tech newsletterPlatformerobtainedinternal documents that indicate the model can generates falsehood and leak confidential information. (Amazon Q is not to be confused with OpenAIQ*.)
How it works:Currently available as a free preview, Q analyzes private documents, databases, and code to answer questions, generate content, and take actions. Amazonplansto offer two tiers of service: a basic chatbot ($20 per month) and the chatbot plus code generation, troubleshooting, security evaluation, and human assistance from Amazon Web Services ($25 per month). Amazon promises not to train machine learning models on Q users’ data.
The issues:Three days after Amazon unveiled Q, employees began to flag issues on internal Slack and security reporting channels.
Behind the news:Amazon is not the only major AI company whose chatbot has leaked private information. Google researchers recentlyfoundthat they could prompt OpenAI’s ChatGPT to divulge personal information found in its training data.
Why it matters:For Amazon, issues with a newly released system are a bump in the road to competing effectively against competitors like Microsoft Copilot and ChatGPT Enterprise. For developers, it’s a sobering reminder that when you move fast, what breaks may be your own product.
We’re thinking:In developing an AI system, often it’s necessary to launch — in a safe and responsible way — and make improvements based on real-world performance. We congratulate the Q team on getting the product out and look forward to seeing where they take it.
",['https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--78-.png']
Seeing Darker-Skinned Pedestrians,"In a study, models used to detect people walking on streets and sidewalks performed less well on adults with darker skin and children of all skin tones.
What’s new:Xinyui Li, Zhenpeng Chen, and colleagues at Peking University, University College London, and King’s College Londonevaluatedeight widely used object detectors for bias with respect to skin color, age, and gender.
Key insight:When it comes to detecting pedestrians, biases with respect to demographic characteristics can be a life-and-death matter. Evaluating them requires a dataset of pedestrians labeled according to characteristics that might influence detection. Skin color, age, and gender are important human differences that can affect a vision model’s performance, especially depending on lighting conditions.
How it works:The authors collected over 8,000 photos from fourdatasetsofstreetscenes. They annotated each image with labels for skin tone (light or dark), age group (child or adult), and gender (male or female). They tested four general-purpose object detectors:YOLOX,RetinaNet,Faster R-CNN, andCascade R-CNN— and four pedestrian-specific detectors —ALFNet,CSP,MGAN, andPRNet— on their dataset. They evaluated performance between perceived skin tone, age, and gender groups and under different conditions of brightness, contrast, and weather.
Results:The study revealed significant fairness issues related to skin tone and age.
Behind the news:Previousworkhas shown that computer vision models can harbor biases that make them less likely to recognize individuals of certain types. In 2019, MITshowedthat commercial face recognition performed worse on women and darker skinned individuals. Aplethoraofworkevaluatesbias in datasets typically used to train vision models.
Why it matters:As more road vehicles gain self-driving capabilities and as expanded robotaxi services come to major cities, a growing number of pedestrians’ lives are in the hands of computer vision algorithms. Auto makers don’t disclose what pedestrian detection systems they use or the number of real-world accidents involving self-driving cars. But co-author Jie Zhangclaimsthat the proprietary systems used in self-driving cars are “usually built upon the existing open-source models,” and “we can be certain that their models must also have similar issues.”
We’re thinking:Computer vision isn’t the only technology used by self-driving cars to detect objects. Most self-driving car manufacturers rely on lidar and radar in addition to cameras. Those technologies are blind to color and gender differences and, in the view of many engineers, make better choices for this application.
Want to learn how to fine-tune large language model-based agents? In our upcoming webinar with Weights and Biases, you’ll gain insights and techniques to enhance agent performance and specificity in automating applications.Register now
","['https://dl-staging-website.ghost.io/content/images/2023/12/The-Batch--3-.png', 'https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--33-.jpg']"
Limits on AI in Life Insurance,"The U.S. state of Colorado started regulating the insurance industry’s use of AI.
What’s new:Coloradoimplementedthe first law that regulates use of AI in life insurance and proposed extending the limits to auto insurers. Other states have taken steps to rein in both life and auto insurers under earlier statutes.
How it works:States are responsible for regulating the insurance industry in the U.S. Colorado’sruleslimit kinds of data life insurers can use and how they can use it. They took effect in November based on alawpassed in 2021.
Other states:Californiaorderedall insurers to notify regulators when their algorithm results in an increase to a customer’s premium; regulators can then evaluate whether the effect of the rate increase is excessive and/or discriminatory. Agencies inConnecticutandNew Yorkordered all insurers to conform their use of AI with laws against discrimination. Washington D.C.openedan investigation to determine whether auto insurers’ use of data resulted in outcomes that discriminated against certain groups.
Behind the news:Colorado shared an initial draft of its life-insurance regulations earlier this year beforerevisingit. Among other changes, the initial draft prohibited AI models that discriminate not only on the basis of race but with respect to all protected classes; prevent unauthorized access to models; create a plan to respond to unforeseen consequences of their models; and engage outside experts to audit their models. The final draft omits these requirements.
Why it matters:Regulators are concerned that AI could perpetuate existing biases against marginalized groups, and Colorado’s implementation is likely to serve as a model for further regulation. Insurance companiesfacea growing number of lawsuits over claims that their algorithms wrongfullydiscriminateby age or race. Regulation could mitigate potential harms and ease customers’ concerns.
We’re thinking:Reporting of models that use social posts, purchases, and the like is a good first step, although we suspect that further rules will be needed to govern the complexities of the insurance business. Other states’ use of Colorado's regulations as a blueprint would avoid a state-by-state patchwork of contradictory regulations.
",['https://dl-staging-website.ghost.io/content/images/2023/12/unnamed---2023-12-06T155705.846.gif']
Doctors Wary of Medical AI Devices,"The United States’ regulatory regime may not be clear or flexible enough to ensure the safety of AI-powered medical devices.
What’s new:Physicians and other health professionals believe that U.S. regulators have approved AI-powered medical products without proper oversight or disclosure, according to areportbyThe New York Times. The FDA hadapprovedroughly 700 products as of July 2023.
How it works:The Food and Drug Administration (FDA) approves medical devices and diagnostic systems in the U.S. It approves almost all such products that involve AI through a program known as510(k).
What they’re saying:“If we really want to assure that right balance, we’re going to have to change federal law, because the framework in place for us to use for these technologies is almost 50 years old.” — Jeffrey Shuren, Director, Center for Devices and Radiological Health, FDA
Behind the news:The FDA’s approval of AI-enabled medical products has been contentious.
Why it matters:In medicine, the right tool can be a life saver, while the wrong one can be fatal. Doctors need to have confidence in their tools. The current FDA process for AI-powered medical products makes it hard to separate what works from what doesn’t, and that’s delaying adoption of tools that could save lives.
We’re thinking:We have great faith that AI can improve medical care, but we owe it to society to document efficacy and safety through careful studies. Machine learning algorithms are powerful, but they can suffer fromdata drift and concept drift, which leads them to work in experiments but not in practice. Updated standards for medical devices that are designed to evaluate learning algorithms robustly would help point out problems, help developers identify real problems and solutions, and give doctors confidence in the technology.
",['https://dl-staging-website.ghost.io/content/images/2023/11/unnamed---2023-11-29T175402.527.gif']
Industrial Strength Language Model,"ChatGPT is pitching in on the assembly line.
What’s new:Siemens and Microsoftlauncheda joint pilot program of a GPT-powered model for controlling manufacturing machinery. German automotive parts manufacturerSchaeffleris testing the system in its factories, as is Siemens itself.
How it works:Industrial Copilot (distinct from similarly named Microsoft products such as GitHub Copilot and Microsoft 365 Copilot) enables users to interact with software that drives industrial machines using natural language. At an unspecified near-future date, Siemens plans to make it more widely available viaXcelerator, an online hub that connects Siemens customers to tools and partners.
Behind the news:Microsoft is betting that specialized large language models can boost productivity (and expand its market) in a variety of industries. The companyannouncedits intention to develop Copilot models for infrastructure, transportation, and healthcare.
Why it matters:Industrial Copilot promises to reduce the time it takes factory technicians to operate and maintain machinery, and it may help less-technical workers get a stalled assembly line back up and running. This may be especially timely as older workers retire, since the software that runs manufacturing equipment can be decades old, and PLC coding can bedifficultto learn without prior manufacturing experience.
We’re thinking:For programming languages like PLC, the pool of coders is diminishing even as valuable applications still need to be maintained and built.Generative AI can play an important rolein helping developers who are less familiar with these languages to write and maintain important programs.
Learn advanced retrieval augmented generation (RAG) techniques that you can deploy in production immediately! Our new short course teaches  sentence-window retrieval and auto-merging retrieval, as well as how to evaluate RAG performance.Sign up for free
","['https://dl-staging-website.ghost.io/content/images/2023/11/The-Batch-ads-and-exclusive-banners--83-.png', 'https://dl-staging-website.ghost.io/content/images/2023/11/unnamed--32-.jpg']"
Testing for Large Language Models,"An open source tool automatically tests language and tabular-data models for social biases and other common issues.
What’s new:Giskardis a software framework that evaluates models using a suite of heuristics and tests based on GPT-4. Aboton the Hugging Face Hub can assess uploaded models automatically and lets users design tests for their own use cases.
Automated tests:Giskard automatically generates inputs depending on the type of model it’s testing, records the model’s output, and identifies undesirable behavior. For large language models, it tests for 7 potential issues including robustness, misinformation, and social biases (“discrimination”). Anexample evaluationshows how it finds various problems with GPT 3.5.
Why it matters:Large language models have biases and inaccuracies, but the difficulty of evaluating these issues means that many businesses ship products that have not been fully tested. Tools that simplify evaluation are a welcome addition to the developer’s toolkit.
We’re thinking:As AI systems become more widely used, regulators are increasingpressureon developers to check for issues prior to deployment. This could make the need for automated testing more urgent.
",['https://dl-staging-website.ghost.io/content/images/2023/11/unnamed---2023-11-29T175841.371.gif']
The CEO IsOutIn,"OpenAI abruptly fired and rehired its CEO Sam Altman, capping five days of chaos within the company.
What’s new:On Friday, the OpenAI board of directors — whose membership since has changed —oustedCEO and co-founder Sam Altman from his leadership position and his seat on the board. The board named chief technology officer Mira Murati interim CEO, soon replaced by Twitch co-founder Emmett Shear. Late Tuesday, Altman wasreinstatedand the board reorganized.
What happened:The dizzying events leave OpenAI with familiar leadership and a retooled board of directors. The new board, which is expected to expand, is chaired by Salesforce co-CEO Bret Taylor and includes economist Larry Summers and Quora CEO Adam D’Angelo (the sole holdover from the previous lineup). Leaving the board are Altman, co-founder and chief scientist Ilya Sutskever, entrepreneur Tasha McCauley, and AI safety researcher Helen Toner as well as president, co-founder, and former board chair Greg Brockman (who lost his seat in the turmoil, resigned, and returned with Altman).
Revolving door:OpenAI went through three CEOs within nearly as many days. Here’s who has passed through the revolving door.
Why it matters:At a moment when AI is undergoing rapid development and deepening division over the role of regulation, the chaos at OpenAI highlights the importance of strong corporate governance and an experienced board of directors that has a range of relevant experience and strong alignment with the company’s mission. It’s highly unusual for directors to fire a chief executive without arranging an orderly succession, coordinating with key investors, and preparing the market for changes. Chaos at the company opened competitive opportunities for rivals and threatened to destabilize thousands of companies that depend on OpenAI services. Although Altman’s return presumably restores the company’s stability, it will bear lingering questions and greater scrutiny going forward.
We’re thinking:There’s nothing normal about goings on at OpenAI. Nonetheless, as startup guru Eric Riessaid, cofounder breakups and sometimes even boardroom coups are part of startup life. They’re unnerving, especially for people who depend on the companies involved (and vice-versa). We wish OpenAI’s employees, who have done a tremendous job of advancing AI and serving hundreds of millions of customers, renewed enthusiasm and focus as they resume their important work.
",['https://dl-staging-website.ghost.io/content/images/2023/11/unnamed--99--1.gif']
The Politics of Generative AI,"Argentina’s recent presidential race was a battleground of AI-generated imagery.
What’s new:Candidates Javier Milei and Sergio Massa flooded social media with generated images of themselves and each other,The New York Timesreported. On Sunday, Mileiwonthe election’s final round.
How it works:No candidate earned enough votes to win the first round in late October, so front runners Milei, known for his hard-right libertarian economic views, and Massa, the incumbent government’s center-left economic minister, advanced to a run-off. The candidates generated a deluge of pictures and videos as the final vote neared.
What they’re saying:“I absolutely think it's a slippery slope. In a year from now, what already seems very realistic will only seem more so.” — Isabelle Frances-Wright, head of technology and society, Institute for Strategic Dialogue.
Behind the news:Deepfakes have appeared in campaign ads inIndiaandSouth Korea. Earlier this year, Google mandated that advertisers in a number of democratic countries including Argentina clearly label AI-generated imagery in political ads distributed through Google ads, part of a globalpolicy change. Meta willrequirethat political advertisers clearly label AI-generated media in their ads beginning in 2024. Generated images in Argentina’s presidential campaign circulated on Meta’s Instagram network ahead of the deadline.Why it matters:Argentina’s presidential campaign offers a glimpse of the future for democracies across the globe. Image generators are widely available, and political forces haveprovenwillingto use them. AI-generated depictions of candidates may undermine voters’ trust in the media as a whole whether or not they’re intended to deceive, political scientistsworry.
We’re thinking:Generated media poses a conundrum for democracy. Advertising has been shown to influence people even when audience members are aware of the effort to persuade. Yet free speech is essential to a healthy society. We favor mandatory labeling generated media in political ads and strong protection against defamation in hope that these measures will stem the most flagrant abuses.
Agent applications are among the most in-demand uses of large language models (LLMs). This workshop will explore how to develop, evaluate, and iterate on LLM agents quickly and effectively.Register now
","['https://dl-staging-website.ghost.io/content/images/2023/11/The-Batch--2-.png', 'https://dl-staging-website.ghost.io/content/images/2023/11/unnamed--75-.png']"
More Cloud GPUs on the Way,"A new cloud-computing company promises to provide scarce AI processing power to startups and researchers.
What’s new:Voltage Park, a nonprofit north of Silicon Valley, willofferprocessing power from 24,000 top-of-the-line Nvidia H100 graphics processing units (GPUs) — roughly $500 million worth — at competitive prices. Rival suppliers of cloud-based GPUs are oversubscribed as the chips continue to be in short supply.
How it works:The company, which is bankrolled by cryptocurrency billionaire Jed McCaleb, plans to build data centers in Texas, Virginia, and Washington.
Behind the news:Ashortageof Nvidia’s high-end GPUs, which are optimized to process machine learning workloads, has bedeviled organizations that aim to join the generative AI boom. Businesses are scrambling to manage the demand.
Why it matters:Training and serving state-of-the-art AI systems requires huge amounts of processing power. Thus AI startups are facing serious obstacles amid the scarcity of specialized hardware. Larger companies have either their own processing power or strong relationships with cloud providers. Smaller providers such as DataCrunch, Lambda Labs, and Paperspace have limited supply. As generative AI booms, organizations that can provide access to GPUs on flexible terms are likely to find takers.We’re thinking:Voltage Park is a subsidiary of McCaleb’s philanthropic organization, and its profits will fund the organization’s activities, about which its website offersno information. Nonprofit status can be a prelude to for-profit business. We’re curious to see where this company is headed.
",['https://dl-staging-website.ghost.io/content/images/2023/11/unnamed--100-.gif']
Actors Reach Accord on AI,"The longest actors’ strike in Hollywood history ended as actors and studios reached an accord on the use of generative AI in making movies.
What’s new: Film studios must seek an actor’s consent before using a generated likeness or performance and compensate the actor, according to anagreementbetween the trade union Screen Actors Guild-American Federation of Television and Radio Artists (SAG-AFTRA) and the Alliance of Motion Picture and Television Producers (TMPTP). The pact will remain in effect for three years, once it has been ratified by SAG-AFTRA members.
How it works:The agreement covers digital replicas of human actors, synthetic performers, and simulated performances created using AI and other technologies that may not be generally recognized as AI. The parties argued over terms with respect to AI until the very last day of their 118-day negotiation,accordingto SAG-AFTRA’s president. Among the provisions:
Behind the news:The agreement followed a similar three-yeardealin September that ended the concurrent strike by Writers Guild of America.
Yes, but:The agreement covers on-screen actors. It does not cover voice or motion actors in video games or television animation. In September, SAG-AFTRAauthorizeda strike against a group of video game companies if negotiations, which are ongoing, stall. Negotiations over television animation are expected as well.
Why it matters:The actors’ agreement could set an international example for limits on AI in the performing arts, thanks to the U.S. film and television industry’s global reach. Entertainers’ unions in Europe and Canada arecontemplatingstrikes inspired by SAG-AFTRA’s, and they may seek similar agreements.
We’re thinking:As with the screenwriters’ contract, the agreement between actors and studios gives everyone three years to experiment with AI while respecting the consent, credit, and compensation of creative workers. We hope that shows made in this period provide ample evidence that such tools can yield wonderful productions that enlarge the market, and that the next agreement focuses more on growing the use of AI and dividing the winnings fairly among actors, studios, and technologists.
",['https://dl-staging-website.ghost.io/content/images/2023/11/unnamed--28-.jpg']
Cyberattack Strikes OpenAI,"ChatGPT suffered a cyberattack apparently tied to the Kremlin.
What's new:A ChatGPT outage on November 8 most likely was caused by a distributed denial of service (DDoS) attack, OpenAIrevealed.
What happened:ChatGPT went down shortly before 9:00 a.m. Eastern Time and remained out of service for about 90 minutes. Intermittent outages of unknown cause had affected OpenAI and other services during the previous two days.
DDoS basics:In a DDoS attack, malicious programs running independently on numerous machines flood a website with requests, disrupting service. The distributed nature of the attack makes it difficult to trace or combat. Almost all cloud providers and large websites use DDoS mitigation services or their own technology to defend against such attacks. However, such defenses don’t always block an especially determined or resourceful attacker.
Why it matters:The ChatGPT outage is a sobering reminder that API-powered services are vulnerable to targeted attacks, and providers need to be proactive about protecting themselves and their users.
We're thinking:While no one likes downtime, it’s hard to defend against a state-sponsored DDoS. It’s a testament to OpenAI’s impact that just 90 minutes of downtime was felt around the world.
New short course! “Quality and Safety for LLM Applications” will help you enhance the safety of large language model applications by detecting issues like data leakage, hallucination, toxicity, and jailbreaks. Start making your apps more secure today.Enroll now
","['https://dl-staging-website.ghost.io/content/images/2023/11/The-Batch-ads-and-exclusive-banners--80-.png', 'https://dl-staging-website.ghost.io/content/images/2023/11/unnamed--29-.jpg']"
Anthropic Cultivates Alternatives,"Weeks after it announced a huge partnership deal with Amazon, Anthropic doubled down on its earlier relationship with Alphabet.
What's new:Anthropic, which provides large language models, agreed to use Google’s cloud-computing infrastructure in return for a $2 billion investment,The Wall Street Journalreported. The deal follows an earlier multibillion-dollar partnership that saw Anthropic commit to training new models on Amazon Web Services.
How it works:Google invested $500 million up front and will add $1.5 billion more over an unspecified time period. The new funding builds on $300 million that Googlegaveto Anthropic earlier in the year for a 10 percent stake in the company. Google’s current stake in Anthropic is undisclosed.
Behind the news:Anthropic rose rapidly from AI startup to coveted foundation-model partner.
Why it matters:The Anthropic-Google deal changes the shape of the startup’s relationships with large cloud providers. Anthropic's deal with Amazon dwarfed Google’s initial investment and seemed like a formative partnership akin to OpenAI’s lucrative Microsoftpair-up. Now, Anthropic is more like a vertex in a triangle, bound by close relationships with competing partners.
We're thinking:Anthropic hasn’t raised as much total funding as OpenAI ($12.7 billion and counting), but its relationships with both Google and Amazon give it more flexibility to choose different infrastructure for different tasks. The benefits presumably will flow not only to the three companies but also to independent developers, who can choose among stellar proprietary foundational models — not to mention open source alternatives — from three major cloud providers.
",['https://dl-staging-website.ghost.io/content/images/2023/11/unnamed--98-.gif']
AI Builds Better Sorting Algorithms,"Online sorting algorithms run trillions of times a day to organize lists according to users’ interests. New work found faster alternatives.
What’s new:Daniel J. Mankowitz and colleagues at Google developedAlphaDev, a system that learned to generate algorithms that sort three to five numbers faster than previous state-of-the-art methods. Accelerating such algorithms can expedite the sorting of lists of any size — say, for search engines, ecommerce sites, and the like — since algorithms that sort more elements often call algorithms that sort fewer elements.
Key insight:Most programmers implement sorting algorithms in a high-level programming language like C++, which a compiler translates into Assembly Language instructions that control the processor and memory. A compiler can translate a single line of C++ into a variety of sequences of Assembly instructions that are equivalent functionally but vary in their speed (number of Assembly instructions required). A reinforcement learning agent can learn to choose a translation that maximizes speed.
How it works:AlphaDev is a collection of neural networks that learn jointly via reinforcement learning. The authors initialized the system by giving it a sequence of unsorted numbers and an empty list of Assembly instructions. It built algorithms by adding Assembly instructions one by one. It earned rewards for choosing instructions that sorted the numbers correctly and quickly.
Results:The authors tested two approaches to rewarding speed, minimizing either Assembly instructions or average runtime over a number of inputs. When AlphaDev minimized the number of Assembly instructions, it found an algorithm that sorted three integers using 17 instructions instead of the previous state-of-the-art algorithm, a human-engineered one that used 18 instructions. Its algorithm for sorting four integers used 28 instructions, equal to the typical one. Its algorithm for sorting five integers had 42 instructions, compared to the alternative’s 46 instructions. When AlphaDev optimized for runtime (running on Intel 6th-generation Core “Skylake” processor), sorting three integers took 2.18 nanoseconds, compared to the typical algorithm’s 4.86 nanoseconds. Sorting four unsigned integers took 1.96 nanoseconds instead of 5.43 nanoseconds and sorting five of them took 1.98 nanoseconds instead of 6.79 nanoseconds. AlphaDev achieved smaller speedups with longer number sequences: Sorting 16 unsigned integers took 9.5 nanoseconds instead of 10.5 nanoseconds, and sorting 262,144 numbers took 60.8 nanoseconds instead of 61.4 nanoseconds.
Why it matters:This work repurposes the training method and architecture of game-playing models likeAlphaZeroto solve real-world problems. The trick is to reframe the task of writing a sorting algorithm as a reinforcement learning problem.
We’re thinking:What other algorithms can this approach optimize? How much faster will they be? Let’s get these questions sorted!
Experience the fastest-growing course on Coursera this year,Generative AI for Everyone! Led by Andrew Ng, delve into generative AI and its applications in both professional and personal settings.Enroll now
",['https://dl-staging-website.ghost.io/content/images/2023/11/The-Batch-ads-and-exclusive-banners-_72_.jpg']
Generative AI as Development Platform,"OpenAI added new features designed to help developers build applications using its generative models.What’s new:OpenAIintroduceda plethora of capabilities at its first developer conference in San Francisco.
Upgrades and more:The company rolled out the upgraded GPT-4 Turbo (which now underpins ChatGPT). It extended API access to its DALL·E 3 image generator, text-to-speech engine, speech recognition, and agent-style capabilities. And it showed off a new concept in chatbots called GPTs.
Why it matters:OpenAI is enabling developers to build intelligence into an ever wider range of applications. GPT-4 Turbo's 128,000-token context window makes possible applications that require tracking information across huge volumes of input. The expanded APIs open up language, vision, and multimodal capabilities as well as agent-style applications that respond to changing conditions and behave in complex ways. The opportunities for developers are immense.We’re thinking:It’s amazing to see cutting-edge AI developments become widely available so quickly. Early on, OpenAI withheld its work out of fear that it could be misused. But that policy clearly no longer holds. “We believe that gradual iterative deployment is the best way to address safety challenges of AI,” OpenAI CEO Sam Altman said in hiskeynote. Based on the evidence to date, we agree.
",['https://dl-staging-website.ghost.io/content/images/2023/11/unnamed--70-.png']
AI Safety Summit Mulls Risks,"An international conference of political leaders and tech executives agreed to regulate AI.
What’s new:28 countries including China and the United States as well as the European Union signed adeclarationaimed at mitigating AI risks.
How it works:The declaration kicked off the United Kingdom’s first AI Safety Summit at Bletchley Park, a country house outside London, where Alan Turing and others cracked Germany’s Enigma code during World War II.
More to come:The AI Safety Summit is set to be the first in a series. South Korea willhosta follow-up in six months. France will host a third summit six months later.
Yes, but:Critics found the conference wanting. Some researcherscriticizedit for failing to endorse concrete limits on AI. Othersblamedthe speakers for promoting fear, particularly UK prime minister Rishi Sunak, whocomparedthe AI risks to a global pandemic or nuclear war.
Why it matters:AI is developing rapidly, and regulatory frameworks are already emerging in China, Europe, and the U.S. The summit is an effort to lay groundwork for a coherent international framework.
We’re thinking:We applaud approaches that engage leaders in government, industry, and research. But we remain concerned that exaggerated fear of risks may lead to regulations that stifle innovation, especially by limiting open source development. UK Deputy Prime Minister Oliver Dowdenspokeabout the value of open source and said there should be a very high bar to restrict open source in any way. We heartily agree!
Learn how to use vector databases with large language models to build applications that include hybrid and multilingual searches! Take our new course, “Vector Databases: from Embeddings to Applications.”Enroll for free
","['https://dl-staging-website.ghost.io/content/images/2023/11/The-Batch-ads-and-exclusive-banners--77-.png', 'https://dl-staging-website.ghost.io/content/images/2023/11/unnamed--71-.png']"
The Language of Schizophrenia,"Large language models may help psychiatrists resolve unanswered questions about mental illness.
What’s new:Researchers from University College London, Beijing Normal University, and Lisbon’s Champalimaud Centre for the Unknown used a large language model tomeasuredifferences in the ways people with schizophrenia use words.Key insight:Neuroscientists theorize that schizophrenia disturbs the brain’s ability to represent concepts. When given a task like “name as many animals as you can in five minutes,” patients with schizophrenia would propose names in a less-predictable order than people who haven’t. In general, the consecutive names produced by people with schizophrenia would be less semantically related than those produced by others.
How it works:The authors asked 26 people who had been diagnosed with schizophrenia and 26 people who hadn’t to name as many animals as they could in five minutes. They also asked the subjects to name as many words that start with the letter “P” as they could in five minutes.
Results:Responses by subjects with schizophrenia had greater randomness. To control for variations in the contents of various patients’ lists, the researchers expressed the degree of randomness as astandard score, where 0 indicates complete randomness, and the lower the negative number, the more optimal the order. On average, people with schizophrenia achieved -5.81, while people without schizophrenia achieved -7.02.
Why it matters:The fastText model’s embeddings helped the authors demonstrate a relationship between cognitive activity and psychiatric symptoms that previously was purely theoretical. Such a relationship has been difficult to establish through brain imaging or traditional testing.
We’re thinking:It’s important to note that the authors don’t propose using their method as a diagnostic tool to determine whether or not a patient has schizophrenia. Unlike diagnosing, say, a cancerous tumor, establishing ground truth in mental illness is extremely complicated. The fact that AI-based measurements agree with doctors’ assessments is a very positive sign.
",['https://dl-staging-website.ghost.io/content/images/2023/11/unnamed--97--1.gif']
Synthetic Data Helps Image Classification,"Generated images can be more effective than real ones in training a vision model to classify images.
What's new:Yonglong Tian, Lijie Fan, and colleagues at Google and MIT introducedStableRep, a self-supervised method that trains vision transformers on images generated by Stability.AI’s Stable Diffusion image generator.
Key insight:Models that employ a contrastive loss learn to represent examples as more or less similar. For example, images that depict a particular object are more similar to each other, and images that depict other objects are less similar to the first group. The training method known asSimCLRuses a contrastive loss with two augmented (cropped, rotated, flipped, and so on) versions of each image, so a model learns that augmented versions of one image, which is closely related but different, are similar to one another — but not to augmented versions of other images. Given a prompt, an image generator produces images that are closely related but significantly more different than augmented versions of the same image. This makes for greater variety among similar examples, which can lead to more effective learning using a contrastive loss.
How it works:The authors generated images and trained a vision transformer on them using a contrastive loss.
Results:The authors compared the ViT-B/16 trained using StableRep to two models of the same architecture trained using SimCLR (one using generated images, the other using images from Conceptual Captions). They also compared it to two CLIP models that produced matching embeddings for images and their paired captions, one trained on generated images and their prompts, the other on real images and their captions. For each of 11 computer vision datasets, the authors trained a linear classifier on top of each model without changing the model’s weights. Comparing the classifiers’ performance, StableRep achieved the best results on 9 of them. For example, onFGVC-Aircraft(10,000 images of 100 different aircraft), StableRep achieved 57.6 percent accuracy, while the best competing model, CLIP pretrained on generated images, scored 53.5 percent.
Why it matters:The fact that text-to-image generators can produce images of similar things that are quite different in appearance makes them a powerful resource for training vision models. And they provide a practically unlimited source of such images!
We're thinking:Different foundation models understand different aspects of the world. It’s exciting that a large diffusion model, which is good at generating images, can be used to train a large vision transformer, which is good at analyzing images!
Join us for two live workshops! Learn to leverage large language models in these interactive, hands-on sessions. Team registrations are available.Register here
",['https://dl-staging-website.ghost.io/content/images/2023/11/The-Batch-ads-and-exclusive-banners--78-.png']
White House Moves to Regulate AI,"U.S. President Biden announced directives that control AI based on his legal power to promote national defense and respond to national emergencies.
What’s new:The White Houseissuedan executive order that requires AI companies and institutions to report and test certain models and directs federal agencies to set standards for AI. The order follows a six-month process of consultation with the AI community and other stakeholders.
How it works:The executive order interprets existing law — specifically the Cold War-era Defense Production Act, a Cold War-era law that gives the president powers to promote national defense and respond to emergencies — and thus can be implemented without further legislation. It focuses on foundation models, or general-purpose models that can be fine-tuned for specific tasks:
Behind the news:The executive order was long in the making and joins other nations’ moves to limit AI.
Why it matters:While Europe and China move aggressively to control specific uses and models, the White House seeks to balance innovation against risk, specifically with regard to national defense but also social issues like discrimination and privacy. The executive order organizes the federal bureaucracy to grapple with the challenges of AI and prepares the way for national legislation.
We’re thinking:We need laws to ensure that AI is safe, fair, and transparent, and the executive order has much good in it. But it’s also problematic in fundamental ways. For instance, foundation models are the wrong focus. Burdening basic technology development with reporting and standards places a drag on innovation. It makes more sense to regulateapplicationsthat carry known risks, such as underwriting tools, healthcare devices, and autonomous vehicles. We welcome regulations that promote responsible AI and look forward to legislation that limits risks without hampering innovation.
",['https://dl-staging-website.ghost.io/content/images/2023/11/TRANSPARENCY.jpg']
What We Know — and Don’t Know — About Foundation Models,"A new index ranks popular AI models in terms of information their developers provide about their training, architecture, and usage. Few score well.
What’s new:The Stanford Center for Research on Foundation Modelspublishedits debut Foundation Model Transparency Index, scoring 10 popular models on how well their makers disclosed details of their training, characteristics, and use.
How it works:Rishi Bommasani, Kevin Klyman, and colleagues at Stanford, MIT, and Princetonexamined10 foundation models — that is, models that can be pretrained for general purposes and fine-tuned for specific tasks — from 10 companies. They scored each model by asking 100 yes-or-no questions that covered training, model architecture and behavior, and policies regarding access and usage.
Results:The index assigned each model a score  between 1 and 100. Meta’s Llama 2 ranked most transparent with a score of 54. BigScience’s BLOOM-Z came in just behind with a score of 53. At the bottom of the list were Inflection’s Inflection-1, which scored 21, and Amazon’s Titan Text, which scored 12.
Yes, but:Because the index is limited to yes/no questions, it doesn’t allow for partial credit. In addition, the questions are weighted equally, so lack of transparency in an important area (say, access to training data) costs only one point in a model’s overall score. It’s easy to imagine companies gaming the scores rather than addressing the most meaningful deficits.
Behind the news:Researchers at MIT, Cohere For AI, and 11 other organizations recently launched the Data Provenance Platform, a project that audits and categorizes training datasets. The effort offers aData Provenance Explorerfor evaluating sources, licenses, creators, and other metadata with respect to roughly 1,800 text datasets.
Why it matters:AI has a transparency problem, and the rise of models that serve as foundations for other models exacerbates the issue. Without disclosure of fundamental factors like architectures, datasets, and training methods, it’s impossible to replicate research, evaluate cost per performance, and address biases. Without disclosure of applications based on a given foundation model, it’s impossible to weigh those applications’ capabilities and limitations. A consistent set of criteria for evaluating transparency may encourage greater disclosure.We’re thinking:The rise of open source AI has been accompanied by an opposite rise in commercial concerns that have little incentive to reveal the inner workings of their models. An index encourages everyone to provide detailed information about the systems they build, and we hope it will help engineers who care about transparency to persuade their teammates. We look forward to refinements and expansion to cover models that aren’t included among the initial 10.
Andrew Ng’s new course, “Generative AI for Everyone,” is live on Coursera! Learn how to use generative AI in your life and work, what this technology can (and can’t) do, and how to put it to use in the real world.Enroll today to get started!
","['https://dl-staging-website.ghost.io/content/images/2023/11/The-Batch-ads-and-exclusive-banners--72-.png', 'https://dl-staging-website.ghost.io/content/images/2023/11/CRUISE.jpg']"
Cruise Control,"The state of California pulled the parking brake on Cruise driverless vehicles.
What’s new:The California Department of Motor Vehicles (DMV)suspendedCruise’s permit to operate vehicles in the state without safety drivers. The General Motors subsidiary responded byhaltingits robotaxi operations across the United States.
How it works:The California DMV acted following an early Octoberincidentin San Francisco. A Cruise driverless car struck and trapped a pedestrian who had been thrown into its path by a separate hit-and-run.
Behind the news:Cruise’s deployment of driverless taxis in San Francisco has been troubled.
Why it matters:Cruise’s latest trouble is a serious setback not just for GM, but for the self-driving car industry, which has been criticized for overpromising and underdelivering. The California DMV’s act has energized politicians, activists, and other public figures who oppose driverless taxis.
We’re thinking:TheAI community must lean into transparency to inspire the public’s trust.California determined that Cruise was not fully forthcoming about its role in the incident — a serious breach of that trust. Voluntary suspension of operations is a welcome step toward restoring it. We hope the company takes the opportunity to conduct a comprehensive review.
",['https://dl-staging-website.ghost.io/content/images/2023/11/DALLE3-1.gif']
Synthetic Data Helps Image Generators,"Text-to-image generators often miss details in text prompts, and sometimes they misunderstand parts of a prompt entirely. Synthetic captions can help them follow prompts more closely.
What’s new:James Betker, Gabriel Goh, Li Jing, and Aditya Ramesh at OpenAI, along with colleagues at Microsoft,improveda latent diffusion model’s performance by training it on an image-caption dataset including model-generated captions that were more detailed than those typically scraped from the web. They used the same technique to train DALL·E 3, the latest version of OpenAI’s text-to-image generator.
Key insight:Text-to image generators learn about the relationships between images and their descriptions from datasets of paired images and captions. The captions in typical image-caption datasets are limited to general descriptions of image subjects, with few details about the subjects and little information about their surroundings, image style, and so on. This makes models trained on them relatively insensitive to elaborate prompts. However, language models can generate captions in great detail. Training on more-detailed synthetic captions can give an image generator a richer knowledge of the correspondence between words and pictures.
How it works:Rather than reveal details about DALL·E 3’s architecture and training, the authors describe training alatent diffusion model.
Results:The authors trained separate latent diffusion models on datasets containing 95 percent generated captions and 100 percent human-made captions. They used the models to generate 50,000 images each and used OpenAI’s CLIP to calculate a similarity score (higher is better) between the prompts and generated images. The model trained on synthetic captions achieved 27.1 CLIP similarity, while a model trained on human-made captions achieved 26.8 CLIP similarity.
Testing DALL·E 3:The authors also tested human responses to images generated by DALL·E 3, Midjourney 5.2, and Stable Diffusion XL v1.0. Shown images based on 170 prompts selected by the authors, human judges found DALL·E 3’s output more true to the prompt and more appealing. Shown images based on 250 captions chosen at random fromMSCOCO, they found DALL·E 3’s output most realistic. In a similar test, DALL·E 3 achieved a higher score on theDrawbenchdataset than Stable Diffusion XL v1.0 and DALL-E 2. (No word on how DALL·E 3 compared to Midjourney in this experiment.)
Why it matters:Synthetic data is used increasingly to train machine learning models. The market research firm Gartner says that output from generative models will constitute60 percentof data used in AI development by 2024. While synthetic data has been shown to boost performance in typical training methods, recursively training one model on another model’s output candistortthe trained model’s output distribution — a scenario that could manifest over time as more models trained on synthetic data are used to generate data to train subsequent models.
We’re thinking:Using one AI model to help another to learn seems to be an emerging design pattern. For example,reinforcement learning from AI feedback(RLAIF) uses AI to rate output from large language models, rather than reinforcement learning from human feedback (RLHF). It’s a fair bet that we’ll see many more techniques along this line.
Learn how to identify and scope vision applications, choose a project type and model, apply data-centric AI, and develop an MLOps pipeline in “Building Computer Vision Applications” with Andrew Ng. Join us on Monday, November 6, 2023, at 10 a.m. Pacific Time.Register here
",['https://dl-staging-website.ghost.io/content/images/2023/11/unnamed--68-.jpg']
Feel the Fear,"The days grow short, the shadows long. Terrifying monsters prowl in the darkness,recentyearshaveshown. We sense the presence of creatures that would do us harm: chatbots that dispense deadly advice, machines bent on conquering our places of work, investors whose unrestrained avarice would ruin us all. How can we hold back the encroaching gloom and prolong the light that is our salvation? We propose asix-month pausein Earth’s orbit around the sun.
",['https://dl-staging-website.ghost.io/content/images/2023/10/Screenshot-2023-10-25-at-11.59.14-AM.jpg']
AI Turns Deadly,"Large language models occasionally generate information that’s false. What if they produce output that’s downright dangerous?
The fear:Text generators don’t know true from false or right from wrong. Ask an innocent question about food or health, and you might get an innocent — but fatal — answer.
Horror stories:Large language models may already have claimed their first victims.
How scared should you be:AI models are becoming safer as researchers develop techniques that align models to human preferences, such as reinforcement learning from human feedback, constitutional AI, and data-centric AI.
Facing the fear:Large language models are widely available, but they’re still experimental. Researchers — like users — are learning how to control them. Builders of systems geared toward the general public — like mental health and recipe chatbots — have a special responsibility to consider sensitive, dangerous, or nefarious uses.
",['https://dl-staging-website.ghost.io/content/images/2023/10/Screenshot-2023-10-25-at-12.07.03-PM.jpg']
Criminals Unleashed,"Do the latest machine learning models constitute a supercharged tech stack for cybercrime?
The fear:Innovations like text generation, voice cloning, and deepfake videos give scammers powerful new ways to gain their victims’ trust and infiltrate their systems. They threaten to bring on an epidemic of e-fraud.
Horror stories:The arsenal of automated tools available to scammers and lawbreakers is growing.
How scared should you be?AI security is a real problem.
Facing the fear:Developers and governments alike are working to thwart malevolent uses of AI. Large AI companiesemployso-called red teams that test a system’s security by simulating attacks. This approach finds and fixes vulnerabilities before lawbreakers discover them. And for users, tried-and-true advice for avoiding scams still applies in the AI age: Exercise skepticism toward online promises, double check identities, hold personal information closely, and don’t click on unknown attachments or links.
This course aims to keep you updated on the fast-changing world of LLMs as a developer tool. Explore advancements like OpenAI’s function calling capability and a new syntax called LangChain Expression Language (LCEL), and apply these tools by building a conversational agent.Enroll for free
","['https://dl-staging-website.ghost.io/content/images/2023/10/The-Batch-ads-and-exclusive-banners--71-.png', 'https://dl-staging-website.ghost.io/content/images/2023/10/DataDisappearence5_1200px.jpg']"
Data Disappears,"The latest advances in AI are built on freely available training data. What will happen if it becomes off-limits?
The fear:Creative workers don’t want AI developers to train models on their works without permission or compensation, or at all. Data is vanishing as they scramble to lock it down.
Horror stories:Generative AI models readily produce outputs that imitate the styles of individual authors and artists. Creative people and organizations that work on their behalf are reacting by suing AI developers (all proceedings are ongoing at publication time) and restricting access to their works.
Survival in a data desert:Some AI companies have negotiated agreements for access to data. Others let publishers opt out of their data-collection efforts. Still others are using data already in their possession to train proprietary models.
Facing the fear:Copyright holders and creative workers are understandably worried that generative AI will sap their market value. Whether the law is on their side remains to be seen. Laws in many countries don’t explicitly address use of copyrighted works to train AI systems. Until legislators set a clear standard, disagreements will be decided case by case and country by country.
",['https://dl-staging-website.ghost.io/content/images/2023/10/Screenshot-2023-10-24-at-2.13.26-PM.jpg']
No Jobs for Humans,"AI is taking over the workplace. Will there be enough jobs left for people?
The fear:Workers of all kinds are on the firing line as large language models, text-to-image generators, and hardware robots match their performance at a lower cost.
Horror stories:Automated systems are performing a wide range of tasks that previously required human labor.
Creeping pink slips:Workers are expressing anxiety about their prospects, and researchers believe the labor market is about to experience a seismic shift.
Facing the fear:Each new wave of technology puts people out of work, and society has a responsibility to provide a safety net and training in new skills for people whose jobs become fully automated. In many cases, though, AI is not likely to replace workers — but workers who know how to use AI are likely to replace workers who don’t.
",['https://dl-staging-website.ghost.io/content/images/2023/10/BUBBLE_1200px.jpg']
Generative AI Calling,"Google’s new mobile phones put advanced computer vision and audio research into consumers’ hands.What’s new:The Alphabet divisionintroducedits flagship Pixel 8 and Pixel 8 Pro smartphones at its annual hardware-launch event. Both units feature AI-powered tools for editing photos and videos.
How it works:Google’s new phones process images in distinctive ways driven by algorithms on the device itself. They raise the bar for Apple, the smartphone leader, to turn itsinternal projectsinto market opportunities.
Behind the news:Google researchers actively pursued AI systems that alter or enhance images, video, and audio.
Why it matters:Smartphones produce most of the world’s photos and videos. Yet generative tools for editing them have been confined to the desktop, social-network photo filters notwithstanding. Google’s new phones bring the world closer to parity between the capabilities of desktop image editors and hand-held devices. And the audio-editing capabilities raise the bar all around.
We’re thinking:Earlier this year, Googleagreedto uphold voluntary commitments on AI, including developing robust mechanisms, such as watermarks, that would identify generated media. Will Google apply such a mark to images edited by Pixel users?
",['https://dl-staging-website.ghost.io/content/images/2023/10/ezgif.com-webp-to-jpg--21-.jpg']
Guiding the Scalpel,"A neural network helped brain surgeons decide how much healthy tissue to cut out when removing tumors — while the patients were on the operating table.
What’s new:Researchers from Amsterdam University Medical Centers and Princess Máxima Center for Pediatric Oncology in the Netherlandsbuilta system to assess how aggressively surgeons should treat tumors. It worked accurately and quickly enough to enable doctors to adjust their approach in the operating room.Key insight:Brain surgeons don’t know the type of tumor they will remove until an operation is underway. When they have a sample — about the size of a kernel of corn — they can classify it by looking at it under a microscope. Alternatively, they can send it out for DNA sequencing, which can take weeks, requiring a second surgery. However, faster, less precise DNA sequencing can be performed on-site, and a neural network can classify such preliminary DNA sequences quickly and accurately. This way, a doctor can proceed with the operation with confidence in the tumor’s classification.
How it works:The authors trained a system of four vanilla neural networks to classify brain tumors.
Results:The authors’ system performed well on tumor DNA samples in an existing collection as well as those gathered in an operating room. Tested on samples from 415 tumors, it classified 60.7 percent of them accurately, misclassified 1.9 percent, and was unable to classify 37.3 percent. Tested on samples collected during 25 real surgeries, it correctly classified 18 tumors and was unable to classify 7. In all cases, it returned results within 90 minutes (45 minutes to collect the DNA and 45 minutes to analyze it).
Why it matters:90 minutes is fast enough to inform brain surgeons what kind of tumor they’re dealing with in the early phase of an operation. If this technique can be rolled out widely, it may help save many lives.We’re thinking:Inferencing presumably takes seconds. The authors say the quick sequencing method processes DNA in 20 to 40 minutes. Speeding up that step offers great potential to accelerate the process.
“Generative AI for Everyone,” taught by Andrew Ng, is coming soon! This course demystifies generative AI and assumes no prior experience in coding or machine learning. Learn how generative AI works, how to use it, and how it will affect jobs, businesses, and society.Join the waitlist
","['https://dl-staging-website.ghost.io/content/images/2023/10/The-Batch-ads-and-exclusive-banners--70-.png', 'https://dl-staging-website.ghost.io/content/images/2023/10/POSTGPT_withDollarSign_1200px.gif']"
Cost Containment for Generative AI,"Microsoft is looking to control the expense of its reliance on OpenAI’s models.
What’s new:Microsoft seeks to build leaner language models that perform nearly as well as ChatGPT but cost less to run, The Informationreported.
How it works:Microsoft offers a line ofAI-powered toolsthat complement the company’s flagship products including Windows, Microsoft 365, and GitHub. Known as Copilot, the line is based on OpenAI models. Serving those models to1 billion-plus userscould amount to an enormous expense, and it occupies processing power that would be useful elsewhere. To manage the cost, Microsoft’s developers are using knowledge distillation, in which a smaller model is trained to mimic the output of a larger one, as well as other techniques.
Behind the news:Microsoft hasinvested$10 billion in OpenAI. The dealpromisesthe tech giant 75 percent of OpenAI’s operating profit until its investment is repaid, then 49 percent of further profits until reaching an unspecified cap. Meanwhile, Microsoft does have access to high-performing models from other sources. Its Azure cloud platformservesMeta’s LLaMA 2.
Why it matters:Serving large neural networks at scale is a challenge even for Microsoft, which has immense hardware resources and a favorable agreement with OpenAI. Running distilled and fine-tuned models can cut the cost for both tech giants and tiny startups.
We’re thinking:If users like Copilot so much they're running up a large bill in model inferences, that sounds like a positive sign!
",['https://dl-staging-website.ghost.io/content/images/2023/10/ezgif.com-webp-to-jpg--22-.jpg']
GPT-4 Opens Its Eyes,"Few people have had a chance to try out OpenAI’s GPT-4 with Vision (GPT-4V), but many of those who have played with it expressed excitement.What’s new:Users who had early access to the image-savvy update of GPT-4, which began a gradual rollout on September 24, flooded social media with initial experiments. Meanwhile, Microsoft researchers tested the model on a detailed taxonomy of language-vision tasks.
Fresh capabilities:Users on X (formerly Twitter) tried out the model in situations that required understanding an image's contents and contexts, reasoning over them, and generating appropriate responses.
Microsoft takes stock:Zhengyuan Yang and colleaguesprobedGPT-4V’s capabilities and evaluated prompting techniques in a wide variety of tasks that involve subtle interactions between images, words, and computer code. They reported only qualitative results — both positive and negative — leaving it to other researchers to compare the model’s performance with that of competitors likeLLaVA.
Yes, but:These qualitative examples are impressive, but they were cherry-picked to give only a glimpse of GPT-4V’s capabilities. Microsoft noted that the model’s behavior is inconsistent. It remains to be seen how reliably it can perform a given task.
Why it matters:GPT-4V is an early entry in a rising generation of large multimodal models that offer new ways to interact with text, images, and combinations of the two. It performs tasks that previously were the province of specialized systems, like object detection, face recognition, and optical character recognition. It can also adapt, alter, or translate images according to text or image prompts. The prospects for integration with image editors, design tools, coding tools, personal assistants, and a wide range of other applications are tantalizing.We’re thinking:When the text-only version of GPT-4 became available, OpenAI didn’t report quantitative results for a couple of weeks (and it still hasn’t presented a detailed view of its architecture and training). We look forward to a clearer picture of what GPT-4V can do.
",['https://dl-staging-website.ghost.io/content/images/2023/10/META-Characters-5_600px-1.gif']
Facebook’s Generative Facelift,"Meta is rolling out AI-powered upgrades to its social platforms.
What’s new:Metaannounceda chat interface, image generator, and celebrity tie-ins for Facebook, Instagram, Messenger, and WhatsApp.
How it works:The new capabilities take advantage ofLLaMa 2and an unnamed image generator, presumablyCM3leon(pronounced “Chameleon”), which Meta described in July.
Behind the news:Meta has lagged behind its big-tech peers in commercializing its AI research. Current and former Meta employeesblamedthe delay on factors including staff turnover, a shortage of high-end chips, a focus on research over products, and management’s lack of enthusiasm for large language models. Lately, the release of restricted open source models such asLlama 2has raised the company's profile as an AI powerhouse.
Why it matters:Social networking is a natural venue for generated text and images, from suggested language for social posts to pictures that reflect a user’s flight of fancy. Meta’s products include some of the most popular mobile apps, which givesnearly 4 billion usersaccess to AI with a mass-media twist.We’re thinking:Chatbots that look and talk like celebrities are an interesting concept, but users need to know they’re not chatting with a real person. Meta’s celebrity bots bear a familiar likeness while making clear that it represents an artificial character — an intriguing solution. On the other hand, at least one of the company’s non-celebrity bots, whose faces are unfamiliar, has beencaughtinsisting it’s a human being.
In this short course, you’ll learn how to use the open source LangChain framework to build a chatbot that interacts with your business documents or other personal data.Enroll today for free
","['https://dl-staging-website.ghost.io/content/images/2023/10/The-Batch-ads-and-exclusive-banners--68-.png', 'https://dl-staging-website.ghost.io/content/images/2023/10/ezgif.com-gif-maker--3-.gif']"
"Newsroom AI Poses Opportunities, Challenges","Journalists are approaching text generators with cautious optimism, a new study shows.What’s new:Researchers at the London School of Economics and Political Sciencesurveyedworkers at over 100 news organizations worldwide. 85 percent of respondents said they had experimented with generative AI.How it works:The authors asked journalists, technologists, and managers how their newsrooms were using generative AI and how they felt about the technology.
Behind the news:Publishers have been eager to take advantage of large language models, but the results so far have been mixed.
Why it matters:In a few short decades, journalism has suffered techno-shocks wrought by the web and social media. Generative AI is poised to bring a third wave of change and challenge, but journalists are generally confident that they can benefit from the technology.We’re thinking:We recentlydistinguishedbetween jobs and the tasks they comprise. While AI can perform some tasks at a human level, currently it rarely performs so well on all the tasks in a given job. We encourage publishers to adopt this framework and devise fruitful ways to allocatejournalists’ tasksamong human-only, machine-only, and human-plus-machine modes.
",['https://dl-staging-website.ghost.io/content/images/2023/10/ezgif.com-webp-to-jpg--19-.jpg']
U.S. Film Industry Limits AI,"Screenwriters and movie studios reached a landmark agreement that restricts uses of AI to produce scripts for television and movies.
What’s new:The Writers Guild of America (WGA)negotiateda new three-year contract with the Alliance of Motion Picture and Television Producers (AMPTP), ending a strike that began in May. The contract allows both writers and studios to use AI within certain restrictions.
How it works:WGA members went onstrikepartly over concern that studios would use AI to replace screenwriters. Thecontractincorporates many of their demands.
The actors’ strike continues:In July, the Screen Actors Guild (SAG-AFTRA) also went onstrikeciting similar concerns. Many actors fear that studios will use generated replicas of performers, undercutting their compensation and credits.
Why it matters:The writers’ agreement is a landmark deal in a high-profile industry. It could serve as a template not only for actors but also workers in other creative industries including publishing, music, graphics, gaming, and software development.
We’re thinking:Generative AI is making many industries and individuals more productive. The new contract protects writers for three years while leaving space for both writers and studios to experiment with ways to do that in film and television. We hope that this agreement is followed by one that focuses on growing the pie — creating more great movies with less effort — while addressing how to divide the larger pie fairly among writers, studios, and technologists.
",['https://dl-staging-website.ghost.io/content/images/2023/10/ANTHROPIC-delivery_bluesky-dith_1200px.gif']
Amazon and Anthropic Form Alliance,"Amazon cut a multi billion-dollar deal with AI startup Anthropic, giving it a powerful ally in the generative arms race.
What’s new:Amazoncommittedto investing as much as $4 billion in Anthropic. In return, Amazon Web Services (AWS) became the primary provider of Anthropic’s Claude and other models.
How it works:Amazon willinvest$1.25 billion in Anthropic immediately. Amazon may invest an additional $2.75 billion depending on undisclosed conditions. Amazon gained an undisclosed minority stake in the startup but not a seat on the board of directors. Other terms were not disclosed.
Behind the news:Founded in 2021 by ex-OpenAI employees, Anthropic is an independent research lab thatfocuseson building safe, beneficial AI models. Having received hundreds of millions of dollars fromGoogleand other investors, it became one of the industry’s most highly funded startups. It wasvaluedat $4.1 billion in March.
Why it matters:Competition around generative AI is white-hot. Cloud providers need to offer cutting-edge models, while AI startups need access to processing power. Microsoft Azure paired up with OpenAI. Google has strong internal generative capabilities. That leaves Amazon as a natural partner for Anthropic.
We’re thinking:Which other high-profile AI startups would make dance partners for enterprising cloud providers? Topping the list are AI21 Labs (already working with Amazon Bedrock), Cohere (also available on Bedrock), and Inflection (funded by Microsoft).
Learn the best practices for finetuning large language models and customize them with real-world data sets in our short course, “Finetuning Large Language Models.”Enroll for free
","['https://dl-staging-website.ghost.io/content/images/2023/10/The-Batch-ads-and-exclusive-banners--67-.png', 'https://dl-staging-website.ghost.io/content/images/2023/10/ezgif.com-gif-maker--2-.gif']"
Video Sharing Goes Generative,"YouTube is reinventing itself for the era of generative AI.
What’s new:The Google-owned video platform isaddinggenerated topic ideas, backgrounds, music suggestions, and audio translations. These capabilities will be available in late 2023 or early 2024.
How it works:The new features are designed to assist video producers in planning, designing, and publishing their works.
Meanwhile, at TikTok:YouTube rival TikTokrequiresusers to clearly label synthetic videos that depict realistic scenes. The guidelines also prohibit synthetic likenesses of private individuals (public figures are allowed unless they are the subject of abuse or misinformation). To help contributors comply, the companyannounceda tool that enables uploaders to manually label their videos as “AI-generated.” TikTok is also testing a system that detects AI-generated or AI-edited elements in a video and automatically adds the label.
Why it matters:YouTube depends on crowdsourced content. Generative tools could make the platform’s contributors more productive, attracting more viewers and boosting revenue all around.We’re thinking:While generative tools may engage the crowd, generated content that’s as compelling as human-produced content could upend YouTube’s business.
",['https://dl-staging-website.ghost.io/content/images/2023/10/ezgif.com-webp-to-jpg--17-.jpg']
"Painting With Text, Voice, and Images","ChatGPT is going multimodal with help from DALL·E.What’s new:ChatGPT is being geared to accept voice input and output, OpenAIannounced. It will also accept and generate images, thanks tointegrationwith DALL·E 3, a new version of the company’s image generator.How it works:The updates expand ChatGPT into a voice-controlled, interactive system for text and image interpretation and production. New safety features are designed to protect legal rights of artists and public figures.
Yes, but:OpenAI said the new voice and image capabilities are limited to the English language. Moreover, the ability to understand and generate highly technical images is limited.
Behind the news:OpenAI introduced GPT-4 in March with a demo that translated a napkin sketch of a website into code, but Google was first to make visual input and output to a large language model widely available. Google announced visual features at May’s Google I/O conference and the public could use them by midsummer.
Why it matters:ChatGPT has already redefined the possibilities of AI among the general public, businesses, and technical community alike. Voice input opens a world of new applications in any setting where English is spoken, and the coupling of language and vision is bound to spark applications in the arts, sciences, industry, and beyond. DALL·E 3’s safety features sound like an important step forward for image generation.We’re thinking:The notion of generative models that ""do everything"" has entered the public imagination. Combining text, voice, and image generation is an exciting step in that direction.
",['https://dl-staging-website.ghost.io/content/images/2023/09/AIMATCH.gif']
A message fromDeepLearning.AI,"Learn how to prompt a large language model to improve, debug, and document your code in a new short course taught by Google AI Advocacy Lead Laurence Moroney, instructor of our Tensor Flow Specializations.Sign up for free
","['https://dl-staging-website.ghost.io/content/images/2023/09/image-5.png', 'https://dl-staging-website.ghost.io/content/images/2023/09/COPILOT.gif']"
Chatbots for Productivity,"Having broken the ice around chat-enabled web search, Microsoft has extended the concept to coding, office productivity, and the operating system itself.What’s new:Microsoftrefreshedits Copilot line of chatbots, adding new features, renaming old ones, and unifying the brand into what it calls an “everyday AI companion.”How it works:Microsoft offers Copilots for its subsidiary GitHub, Microsoft 365, and Windows.
Behind the news:The emergence of ChatGPT set off aracebetween Microsoft and Alphabet to integrate large language models into search and beyond. Microsoft seized the day in early February when it launched a version of its Bing search engine that incorporated OpenAI’s technology, and its Copilot strategy has extended that lead. But Alphabet is nipping at Microsoft’s heels. It’sbringingits Bard chatbot to Google productivity apps, from email to spreadsheets.Why it matters:The combination of large language models and productivity software is a significant step. Microsoft’s approach seems likely to inspire millions of people who have never written a macro or opened the command line to start prompting AI models.We’re thinking:Copilot is a great concept. It helped make software engineers early adopters of large language models — for writing code, not prose.
",['https://dl-staging-website.ghost.io/content/images/2023/09/COOLING.gif']
A Message from Workera,"According toForbes, between 70 percent and 95 percent of enterprises are failing in their business transformations. Skills gaps are a major cause. Ournew guidetells leaders how to avoid this trap. Read it and get your business transformation on track!
",['https://dl-staging-website.ghost.io/content/images/2023/09/image-6.png']
Music Generation For the Masses,"Text-to-music generation has arrived.
What's new:Stability.ai, maker of the Stable Diffusion image generator and StableLM text generator, launchedStable Audio, a system that generates music and sound effects from text. You can play with it and listen to exampleshere. The service is free for 20 generations per month up to 45 seconds long. The professional tier allows 500 generations per month, up to 90 seconds long, for $11.99 per month. An enterprise tier is negotiable. The company said it would open-source the model eventually.
How it works:Stable Audio is alatent diffusionmodel. It generates audio by a process that’s similar to the way Stable Diffusion generates images, but it uses a variational autoencoder to map audio to an embedding for processing and back to audio for your listening pleasure. The authors trained the system on800,000 audio filescontaining music, sound effects, and performances on individual instruments and corresponding descriptions.
Behind the News:Stable Audio joins earlier services including Boomy, Mubert, plugger.ai, Soundful, and VEED.IO. It follows tantalizing advances in audio generation.
Yes, but:Stable Audio excels when generating instrumental and ambient music, but its output tends to suffer from some of the same flaws as previous text-to-music generators: Longer outputs often lack a coherent structure, and the clarity and detail of individual instruments and sound effects varies wildly. It also doesn’t effectively generate the sound of a vocalist pronouncing words.
Why it matters:AI has demonstrated its prowess at generating convincing text and images. Generated audio has implications for producers not only of music but also of videos, video games, and podcasts. Stable Audio sounds like an early step, but it stands out for its speed, high-resolution output, and the inclusion of a mechanism for learning musical structure.
We're thinking:Stable Audio is impressive, but this doesn’t quite feel like music’s GPT moment. Text and image generation took off as soon as highly capable generative models appeared. Music generation may yet await models that can produce not only high-res output but also sonorities and structures coherent and varied enough to be widely useful.
",['https://dl-staging-website.ghost.io/content/images/2023/09/ASYLUM.png']
A Message fromDeepLearning.AI,"Learn about text embeddings and how to apply them to common natural language processing tasks in our new course with Google Cloud!Sign up for free
","['https://dl-staging-website.ghost.io/content/images/2023/09/DeepLearning_GoogleCloudPlatfomr_Banner_2070x1080-1.png', 'https://dl-staging-website.ghost.io/content/images/2023/09/DRONES.gif']"
U.S. Plans to Expand Drone Fleet,"The United States military aims to field a multitude of autonomous vehicles.
What’s new:The Department of Defense announced an initiative to develop autonomous systems for surveillance, defense, logistics, and other purposes,The Wall Street Journalreported. The department aims to deploy several thousands of such systems within 18 to 24 months, a timeline motivated by rapid drone development by China.
How it works:The Pentagon shared details about a program called Replicator that it hadannouncedin August.
Behind the news:The U.S. is not alone in pursuing autonomous military applications. The Russian invasion of Ukrainespurreda homegrown Ukrainian drone industry andencouragedgovernment and independent researchers to harness face recognition systems for identifying combatants. China isdevelopingautonomous ships designed to carry fleets of air, surface, and submarine drones.
Why it matters:Replicator marks a significant, very public escalation of military AI. Other nations are certain to follow suit.
We’re thinking:We’re concerned about the potential for an international AI arms race, and we support the United Nations’ proposedbanon fully autonomous weapons. Yet the unfortunate state of the world is that many countries — even large, wealthy democracies — have little choice but to invest in defenses against aggressors both actual and potential. The ethics of military AI aren’t simple. We call on the AI community to help ensure that they encourage a safer and more democratic world.
",['https://dl-staging-website.ghost.io/content/images/2023/09/VITLEARN.gif']
How Vision Transformers See,"While transformers have delivered state-of-the-art results in several domains of machine learning, few attempts have been made to probe their inner workings. Researchers offer a new approach.
What's new:Amin Ghiasi and colleagues at the University of Marylandvisualized representations learned by a vision transformer. The authors compared their results to earlier visualizations of convolutional neural networks (CNNs).
Key insight:A method that has been used tovisualize the internal workings of CNNscan also reveal what’s happening inside transformers: Feeding the network images thatmaximize the output of a particular neuronmakes it possible to determine what individual neurons contribute to the network’s output. For instance, neurons in earlier layers may generate high outputs in response to an image with a certain texture, while neurons in later layers may generate high outputs in response to images of a particular object. Such results would suggest that earlier layers identify textures, and later layers combine those textures to represent objects.
How it works:The authors experimented with apretrainedViT-B16vision transformer.
Results:ViT-B16’s fully connected layers were most revealing: Neurons in fully connected layers yielded images that contained recognizable features, while those in attention layers yielded images that resembled noise.
Why it matters:This work reveals that vision transformers base their output on hierarchical representations in much the same way that CNNs do, but they learn stronger associations between image foregrounds and backgrounds. Such insights deepen our understanding of vision transformers and can help practitioners explain their outputs.
We're thinking:The evidence that CLIP learns concepts is especially intriguing. As transformers show their utility in a wider variety of tasks, they’re looking smarter as well.
""Practical Computer Vision"" by Andrew Ng: In this live event, you’ll learn how to identify and scope vision applications, choose vision models, apply data-centric AI, and develop an MLOps pipeline.Join uson Tuesday, October 3, at 10:00 a.m. Pacific Time!
",['https://dl-staging-website.ghost.io/content/images/2023/09/Practical-Computer-Vision.png']
ChatGPT for Big Biz,"A new version of ChatGPT upgrades the service for corporate customers.
What’s new:OpenAIlaunchedChatGPT Enterprise, which combines enhanced data-privacy features with a more capable language model. The price is negotiable on a case-by-case basis,Bloombergreported.
How it works:ChatGPT Enterprise provides enhanced access to GPT-4, previously available via ChatGPT Plus ($20 per month) and API calls at a cost per thousand tokens.
Behind the news:OpenAI hasmetamorphosedfrom a nonprofit into a tech-biz phenomenon, but its business is still taking shape. For 2022, the companyreported$540 million in losses on $28 million in revenue. It’sreportedlyon track to bring in $1 billion this year, and ChatGPT Enterprise is bound to benefit from OpenAI’s high profile among business users: The email addresses of registered ChatGPT users represent 80 percent of the Fortune 500, according to the company.
Why it matters:Large language models are transforming from public experiments to mainstream productivity tools. ChatGPT Enterprise is a significant step in that transition, giving large companies the confidence they need to integrate GPT-4 into their day-to-day operations with less worry that OpenAI will ingest proprietary information.
We’re thinking:Some reporters havequestionedthe financial value of generative AI. While OpenAI’s business is evolving, this new line of business is promising. We anticipate that enterprise subscriptions will be stickier than API access, since customers’ switching costs are likely to be higher.
",['https://dl-staging-website.ghost.io/content/images/2023/09/unnamed--57-.png']
GenAI Violated Copyright? No Problem,"Microsoft promised to shield users of its generative AI services against the potential risk of copyright infringement.
What’s new:Microsoft said it wouldcover the costfor any copyright violations that may arise from use of its Copilot features, which generate text, images, code, and other media within its productivity apps.
How it works:In its Copilot Copyright Commitment, Microsoft vows to defend customers in court against allegations that they infringed copyrights by using Microsoft software. It also promises to reimburse the cost of adverse judgments or settlements.
Behind the news:Microsoft, its subsidiary GitHub, and its partner OpenAI are currentlydefending themselvesagainst allegations that GitHub Copilot violated copyright laws. Programmer and attorney Matthew Butterick claims that OpenAI trained GitHub Copilot in violation of open-source licenses and that the system reproduces copyrighted code without authorization. In May, a judgerejecteda request by the defendants to dismiss the case, which remains ongoing.
Why it matters:Generative AI represents a huge business opportunity for Microsoft and others. Yet the technology is under attack by copyright holders, creating the potential that customers may face lawsuits simply for using it. That may be persuading enterprise customers — Microsoft’s bread and butter — to avoid generative AI. The company’s promise to protect them from legal action is a bold bet that the cost of defending customers will be far less than the profit it gains from selling generative products and services.We’re thinking:It’snot yet clearwhether using or developing generative AI violates anyone’s copyright, and it will take time forcourtsandlawmakersto provide a clear answer. While legal uncertainties remain, Microsoft’s commitment is an encouraging step for companies that would like to take advantage of the technology and a major vote of confidence in the business potential of generative AI.
SpeechLab is building speech AI that conveys the nuance and emotion of the human voice, bringing together proprietary models for multi-speaker, multi-language text-to-speech; voice cloning; speech recognition; and more. Learn more atSpeechLab.AI
","['https://dl-staging-website.ghost.io/content/images/2023/09/The-Batch-ads-and-exclusive-banners--63-.png', 'https://dl-staging-website.ghost.io/content/images/2023/09/unnamed--95-.gif']"
Truth in Online Political Ads,"Google, which distributes a large portion of ads on the web, tightened its restrictions on potentially misleading political ads in advance of national elections in the United States, India, and South Africa.
What’s new:Starting in November 2023, in select countries, Google’s ad network will require clear disclosure of political ads that contain fictionalized depictions of real people or events, the companyannounced. The policy doesn’t explicitly mention generative AI, which can automate production of misleading ads.
How it works:In certain countries, Google accepts election-related ads only from advertisers that pass a lengthy verification process. Under the new rules, verified advertisers that promote “inauthentic” images, video, or audio of real-world people or events must declare, in a place where users are likely to notice it, that their depiction does not represent reality accurately.
Behind the news:Some existing AI-generated political messages may run afoul of Google’s restrictions.
Yes, but:The rules’ narrow focus on inauthentic depictions of real people or events may leave room for misleading generated imagery. For instance, a U.S. Republican Partyvideocontains generated images of a fictional dystopian future stemming from Joe Biden’s hypothetical re-election in 2024. The images don’t depict real events, so they may not require clear labeling under Google’s new policy.
Why it matters:Digital disinformation has influenced elections for years, and the rise of generative AIgivesmanipulators a new toolbox. Google, which delivers an enormous quantity of advertising via Search, YouTube, and the web at large, is a powerful vector for untruths and propaganda. With its new rules, the company will assume the role of regulating itself in an environment where few governments have enacted restrictions.We’re thinking:Kudos to Google for setting standards for political ads, generated or otherwise. The rules leave some room for interpretation; for instance, does a particular image depict a real event inauthentically or simply depict a fictional one? On the other hand, if Google enforces the policy, it’s likely to reduce disinformation. We hope the company will provide a public accounting of enforcement actions and outcomes.
",['https://dl-staging-website.ghost.io/content/images/2023/09/unnamed--96-.gif']
Fake Newscasters,"Tonight at 11: I’m an AI-generated character, and I’ll be bringing you the latest headlines.What’s new:Indian broadcasters have embraced synthetic news presenters,Nikkei Asiareported. Their counterparts in other Asian countries also rely increasingly on automated anchors.Invasion of the newsbots:Synthetic presenters can deliver reports generated directly by large language models and do so in multiple languages. One news producer noted that they also give newsrooms a break from the typical presenter’s outsized ego. None of the broadcasters has disclosed the technology they’re using.
Behind the news:Synthetic news presenters go back at least to 2018, when Chinese state news agency Xinhua and search engine Sogouintroducedpioneering 2D newsbots. Their images were drawn from videos, while their motions and voices were driven by machine learning. Two years later, the broadcasterupgradedto 3D-rendered avatars produced using “multimodal recognition and synthesis, facial recognition and animation and transfer learning.”Yes, but:While broadcasters can use AI-generated talking heads to save time and money, propagandists can use them to gain an aura of newsy credibility. For example, an unidentified groupusedSynthesia, a web service that makes AI-generated characters, to generate fake news clips from a fictional outlet called Wolf News. One clip attacked the U.S. government for failing to take action against gun violence, while another promoted cooperation between the U.S. and China.Why it matters:Synthetic presenters potentially multiply the power of broadcast news by generating an unlimited variety of talking heads. They can appeal to specific audience segments by representing any ethnicity, gender, age, or style. And they can reach an even broader audience by speaking a variety of languages — a boon to broadcasters especially in highly multilingual Asian societies.We’re thinking:It may not be a coincidence that synthetic presenters are appearing first in countries whose people feel more positively about AI. According to one survey, people in India, Indonesia, and MalaysiatrustAI more than do people in Western countries.
",['https://dl-staging-website.ghost.io/content/images/2023/09/unnamed--51-.png']
High Wages for AI Talent,"Enthusiasm for AI is driving top salaries for engineers and executives into the stratosphere.
What’s new:Companies that advertise open AI positions are listing annual pay scales well into six figures. In at least one case, the proposed salary approaches seven figures,The Wall Street Journalreported.Generative jobs:On the help-wanted site Indeed, listings by U.S. companies that mention generative AI have jumped around 100 percent year-on-year, even as total listings declined slightly. Tech and non-tech companies alike have posted AI job notices that mention generous salaries. For reference, the average machine learning product engineer job in the U.S. pays around $143,000 annually, according to a study by insurance company Willis Towers Watson. Wages may be lower in other countries.
Behind the news:Skilled AI professionals remain in demand even as large tech companies are hiring fewer workers overall.
Why it matters:Even as demand is rising, AI talent remainsscarce. The shortage prompts employers to offer high salaries in hope of attracting candidates with the skills and experience they need. That situation spells opportunity for people who put in the time, effort, and passion to develop a career in the field.
We’re thinking:We’re thrilled by the number of people who are participating in AI and earning good wages. Yet there’s more to job satisfaction than maximizing your salary. In the long term, the opportunity to work on interesting projects, make a meaningful impact, or work with great people is more likely to affect your happiness and professional attainment than the pay scale. Follow your interests, do your best work, aim to make the world a better place and — above all — keep learning!
Learn how to generate and apply text embeddings in applications based on large language models! Check out our short course built in collaboration with Google Cloud, “Understanding and Applying Text Embeddings with Vertex AI.”Start learning today
","['https://dl-staging-website.ghost.io/content/images/2023/09/The-Batch-ads-and-exclusive-banners--53-.png', 'https://dl-staging-website.ghost.io/content/images/2023/09/unnamed--92-.gif']"
DeepMind’s Offspring Proliferate,"Where spores from DeepMind scatter, startups blossom.
What’s new:Nearly 200 former employees of Google’s elite AI research lab have gone on to found or join startups,Business Insiderreported.Emerged from stealth:Venture capital firms are eager to fund projects that involve ex-DeepMinders, and alumni often benefit from angel investments by their former colleagues. While many such projects are in stealth mode, some have revealed themselves.
Behind the news:Acquired by Google in 2014, DeepMind has developed several high-profile innovations and popularized reinforcement learning. Earlier this year, itmergedwith Google Brain (which Andrew Ng started and formerly led).
Why it matters:Tech giants are magnets for AI talent, and top employees gain valuable practical and market experience. Yet many come to feel confined by conditions within an established company. Former DeepMinders who formed their own companies cited their desire to follow currents of deep learning, such as generative AI, that their former employer doesn’t emphasize and their need for flexibility to pursue goals that didn’t necessarily revolve around machine learning.
We’re thinking:While high-profile associations often attract capital and attention, great ideas can come from anywhere. They seldom happen overnight; usually, they’re the end result of a long incubation period spent honing them through experimentation and feedback. Start small and develop your intuition, skills, and credibility. That’s how pretty much everyone started who ended up having a huge impact!
",['https://dl-staging-website.ghost.io/content/images/2023/09/unnamed--93-.gif']
Industrial-Strength LLM,"Anthropic, the startup behind the safety-focused Claude chatbot, teamed up with South Korea’s largest mobile phone provider.
What’s new:The independent research lab, which is an offshoot of OpenAI, will receive $100 million from SK Telecom to build a multilingual large language model tailored for the telecommunications industry,VentureBeatreported.
How it works:Anthropic will base the specialized model on the technology that underpins its large language modelClaude. SK Telecom plans to offer it to other telecoms firms, such as members of theGlobal Telco AI Alliance, a consortium devoted to building new lines of business based on AI-driven services.
Behind the news:SK Telecom has a history of building its own machine learning models, particularly Korean-language models. The company emulated GPT-3's architecture to train models likeKo-GPT-Trinity-1.2B. An unidentified modelenablesA. (pronounced “a dot”), a virtual assistant for the company’s mobile users.
Why it matters:AI models have a bright future in virtually every industry, and specialized AI models have an even brighter outlook. LikeBloombergGPT, this partnership represents a step toward adapting foundation models to a vertical industry, along with a new business model for good measure.We’re thinking:Prompting a foundation model can go a long way in tasks for which it’s easy to write instructions that describe clearly what you want done. But many tasks involve specialized knowledge that’s difficult to put into a prompt; for instance, consider explaining how to draft a good legal document. In such cases, fine-tuning or specialized training can be a promising approach.
",['https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--47-.png']
China Restricts Face Recognition,"China’s internet watchdog proposed sweeping limitations on face recognition — with significant exceptions.
What’s new:The Cyberspace Administration of Chinaunveileddraft rules that restrict the use of face recognition systems, with explicit carve-outs when national security or public or personal safety is at stake. The public can submit feedback before September 7.
Narrow limits, broad exceptions:The proposal, which will affect mainland China but not Macau or Hong Kong, applies to both public and private users of face recognition. It follows recent restrictions ongenerative AIandcollecting personal data.
Behind the news:Chinaleadsthe world in developing and deploying face recognition. Authorities use it widely in law enforcement, while businesses use it for authenticating payments, checking the identities of air and rail passengers, and granting access to residential buildings. Nonetheless, many Chinese residents have voiced their unease with the technology.
Yes, but:The exemptions for national security and safety give China’s government authority to continue using the technology for potentiallycontroversialapplications.
Why it matters:Face recognition is a double-edged sword. It has legitimate uses for security and law enforcement, but it can also be misused to violate privacy. Such concernsmotivatedEuropean Union lawmakers to insert a prohibition on face recognition in public spaces into the current draft of the union’s AI Act, which is in the final stage of revision. China’s new rules bring that country’s face recognition policy closer into line with that standard — the exceptions for national security and public safety notwithstanding.We’re thinking:It’s interesting to see China take the lead in regulating face recognition, where it dominates the technology and market. We support stronger protections for personal privacy.
Learn how to utilize Semantic Kernel, Microsoft’s open source SDK, to develop sophisticated business applications using LLMs.Sign up for free
","['https://dl-staging-website.ghost.io/content/images/2023/08/DeepLearning_Microsoft_Semantic_Kernel_Banner_2070x1080.png', 'https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--48-.png']"
Crash Tracker,"Event data recorders, also known as black boxes, got an update for the era of self-driving cars.
What’s new:The Institute of Electrical and Electronics Engineerspublishedguidelines for internal devices that track the performance of autonomous road vehicles.
How it works:Like airplanes, cars and trucks carry event data recorders that capture their moment-to-moment behavior for examination in the event of a crash. The new specification calls for vehicles withLevel 3autonomous capabilities or higher, which can drive themselves but may require a human driver to take over, to carry a recorder dedicated to automated driving functions. The working group will meet later this year to discuss further revisions that address subjects like cybersecurity and protocols accessing recorded data.
Behind the news:Event data recorders became a fixture in road vehicles decades ago as a way to evaluate the performance of safety airbags. Today, they record parameters such as speed, acceleration, and braking in 99 percent of new vehicles in the United States. They’ll bemandatoryin new cars in the European Union starting next year.
Why it matters:As more automated driving systems hit the road, safety concerns are on the rise. Event data recorders help shed light on mishaps, and the resulting data can help authorities, manufacturers, and consumers to understand the role, if any, played by self-driving technology. Although compliance is voluntary, IEEE standards are influential and widely followed.
We’re thinking:Self-driving systems have the potential to reduce road and pedestrian fatalities dramatically. A clear picture of what goes wrong and why will enable engineers to improve self-driving technology steadily. Ultimately, we hope, accidents will become rare and relatively inconsequential.
",['https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--90-.gif']
News Outlet Challenges AI Developers,"The New York Timeslaunched a multi-pronged attack on the use of its work in training datasets.
What’s new:The company updated itsterms of serviceto forbid use of its web content and other data for training AI systems,Adweekreported. It’s also exploring a lawsuit against OpenAI for unauthorized use of its intellectual property,according toNPR. Meanwhile,The New York Timesbacked outof a consortium of publishers that would push for payment from AI companies.From negotiation to mandate:The 173-year-old publisher, which has nearly 10 million subscribers across online and print formats, was negotiating with OpenAI to use its material, but talks recently broke down.The New York Timeshad more success with Google: In February, Google agreed to pay around $100 million to useTimescontent in search results, although an agreement on AI training was not reported.
Behind the news:Earlier this month, 10 press and media organizations includingAgence France-Presse,Associated Press, and stock media provider Getty Imagessignedan open letter that urges regulators to place certain restrictions on AI developers. The letter calls for disclosure of training datasets, labeling of model outputs as AI-generated, and obtaining consent of copyright holders before training a model on their intellectual property. The letter followedseveralongoinglawsuitsthat accuse AI developers of appropriating data without proper permission or compensation.
Why it matters:Large machine learning models rely on training data scraped from the web as well as other freely available sources. Text on the web is sufficiently plentiful that losing a handful of sources may not affect the quality of trained models. However, if the norms were to shift around using scraped data to train machine learning models in ways that significantly reduced the supply of high-quality data, the capabilities of trained models would suffer.
We’re thinking:Society reaps enormous rewards when people are able to learn freely. Similarly, we stand to gain incalculable benefits by allowing AI to learn from information available on the web. An interpretation of copyright law that blocks such learning would hurt society and derail innovation. It’s long past time torethink copyrightfor the age of AI.
",['https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--87-.gif']
Defcon Contest Highlights AI Security,"Hackers attacked AI models in a large-scale competition to discover vulnerabilities.
What’s new:At the annual Defcon hacker convention in Las Vegas, 2,200 people competed to break guardrails around language models,The New York Timesreported. The contest, which was organized by AI safety nonprofits Humane Intelligence and SeedAI and sponsored by the White House and several tech companies, offered winners an Nvidia RTX A6000 graphics card.Breaking models:Contestants in the Generative Red Team Challenge had 50 minutes to perform 21 tasks of varying difficulty, which they selected from a board like that of the game showJeopardy. Seven judges scored their submissions.
Behind the news:Large AI developers often test their systems by hiring hackers called “red teams,” a term used by the United States military to represent enemy forces in Cold War-era war games, to attack them.
Why it matters:The security flaws found in generative AI systems are distinctly different from those in other types of software. Enlisting hackers to attack systems in development is essential in sniffing out flaws in conventional software. It’s a good bet for discovering deficiencies in AI models as well.
We’re thinking:Defcon attracts many of the world’s most talented hackers — people who havetricked ATMs into dispensing cashandtaken over automobile control software. We feel safer knowing that this crowd is on our side.
Join ""Finetuning Large Language Models,"" a new short course that teaches you how to finetune open source models on your own data.Enroll today and get started
","['https://dl-staging-website.ghost.io/content/images/2023/08/The-Batch-ads-and-exclusive-banners--51-.png', 'https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--46-.png']"
AI Chip Challenger Gains Traction,"An upstart supplier of AI chips secured a major customer.
What’s new:Cerebras, which competes with Nvidia in hardware for training large models,signeda $100 million contract with Abu Dhabi tech conglomerate G42. The deal is the first part of a multi-stage plan to build a network of supercomputers.
How it works:The deal covers the first three of nine proposed systems. The first,Condor Galaxy 1(CG-1), is already up and running in Santa Clara, California. CG-2 and CG-3 are slated to open in early 2024 in Austin, Texas and Asheville, North Carolina. Cerebras and G42 are in talks to build six more by the end of 2024. G42 plans to use the network to supply processing power primarily to healthcare and energy companies
Behind the news:Nvidia accounts for95 percentof the market for GPUs used in machine learning — a formidable competitor to Cerebras and other vendors of AI chips. Despite Nvidia’s position, though, there are signs that it’s not invincible.
Why it matters:The rapid adoption of generative AI is fueling demand for the huge amounts of processing power required to train and run state-of-the-art models. In practical terms, Nvidia is the only supplier of tried-and-true AI chips for large-scale systems. This creates a risk for customers who need access to processing power and an opportunity for competitors who can satisfy some of that demand.
We’re thinking:As great as Nvidia’s products are, a monopoly in AI chips is not in anyone’s best interest. Cerebras offers an alternative for training very large models. Now cloud-computing customers can put it to the test.
",['https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--88-.gif']
GPU Shortage Intensifies,"Nvidia’s top-of-the-line chips are in high demand and short supply.
What’s new:There aren’t enough H100 graphics processing units (GPUs) to meet the crush of demand brought on by the vogue for generative AI,VentureBeatreported.
Bottleneck:Cloud providers began havingtrouble finding GPUsearlier this year, but the shortfall has spread to AI companies large and small. SemiAnalysis, a semiconductor market research firm,estimatesthat the chip will remain sold out into 2024.
Who’s buying:Demand for H100s is hard to quantify. Large AI companies and cloud providers may need tens of thousands to hundreds of thousands of them, while AI startups may need hundreds to thousands.
Behind the news:Nvidiaannouncedthe H100 early last year and began full production in September. Compared to its predecessor, the A100, the H100 performs about 2.3 times faster in training and 3.5 times faster at inference.
Why it matters:Developers need these top-of-the-line chips to train high-performance models and deploy them in cutting-edge products. At a time when AI is white-hot, a dearth of chips could affect the pace of innovation.We’re thinking:Nvidia’s CUDA software, which undergirds many deep learning software packages, gives the company’s chips a significant advantage. However, AMD’s open source ROCm is making great strides, and its MI250 and upcoming MI300-series chips appear to be promising alternatives. An open software infrastructure that made it easy to choose among GPU providers would benefit the AI community.
",['https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--83-.gif']
China’s LLMs Open Up,"The latest wave of large language models trained in Chinese is open source for some users.
What’s new:Internet giant Alibaba released large language models that are freely available to smaller organizations. The internet giant followed Baichuan Intelligent Technology, a startup that contributed its own partly open models, and Beijing Academy of Artificial Intelligence, which announced that its WuDao 3.0 would be open source.How it works:These pretrained models are small compared to, say, Meta’s LLaMa 2 (70 billion parameters) — but that may be a plus in China, where U.S. export restrictions have made chips for processing AI hard to get.
Behind the news:Developers in China areracingto cash in on chatbot fever. But they face unique hurdles.
Why it matters:The March leak of Meta’sLLaMAinitiated a groundswell of open models that excel in English and a subsequent explosion of innovation and entrepreneurial activity. Competitive open models trained in Mandarin and other Chinese languages could spark similar developments in one of the world’s biggest countries — as long as developers hew to the law.
We’re thinking:High-profile models like ChatGPT and Bard, having been trained on huge amounts of English-language data, tend to know a lot about the histories, geographies, and societies of English-speaking countries but relatively little about places where other languages are spoken. Models trained on Chinese corpora will serve speakers of China’s languages far better, and open source models fine-tuned for Chinese users likely will play an important role.
Join our new course, “Large Language Models with Semantic Search,” and learn the techniques you need to integrate LLMs with search and how to use your website’s information to generate responses.Enroll for free
","['https://dl-staging-website.ghost.io/content/images/2023/08/The-Batch-ads-and-exclusive-banners--48-.png', 'https://dl-staging-website.ghost.io/content/images/2023/08/The-Batch-ads-and-exclusive-banners--4-.gif']"
ChatGPT’s Best Friend,"The latest robot dog is smarter — and less expensive — than ever.
What’s new:Unitree Robotics of Hangzhou, China,unleashedGo2, a quadruped robot that trots alongside its owner, stands on two legs, jumps, talks, takes photos, and retails for less than a high-end MacBook.
How it works:Go2 is made of aluminum and plastic, weighs around 15 kilograms, and moves using 12 joints. A robotic arm mounted on the unit’s back is optional. It comes in three versions with a starting price of $1,600.
Why it matters:Boston Dynamics’ industrial-strength robodog Spot ismanipulatinghigh-voltage electrical equipment,inspectingnuclear power plants, andhelpingto monitor urban areas. But its price — from$74,500to$200,000— puts it out of reach of many potential users. With its dramatically lower price, Go2 suggests that such mechanical beasts may find a wider range of uses.We’re thinking:While wheels are great on flat ground, four legs with backward-facing joints are morestableon uneven terrain. Plus, robot dogs are cute!
",['https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--84-.gif']
LLMs Get a Life,"Large language models increasingly reply to prompts with a believably human response. Can they also mimic human behavior?
What's new:Joon Sung Park and colleagues at Stanford and Google extended GPT-3.5 to buildgenerative agentsthat went about their business in a small town and interacted with one another in human-like ways. The code is newlyavailableas open source.
Key insight:With the right prompts, a text database, and a server to keep track of things, a large language model (LLM) can simulate human activity.
How it works:The authors designed 25 agents (represented by 2D sprites) who lived in a simulated town (a 2D background depicting the layout and the contents of its buildings) and let them run for two days. Each agent usedGPT 3.5;a database of actions, memories, reflections, and plans generated by GPT 3.5; and a server that tracked agent and object behaviors, locations (for instance, in the kitchen of Isabella’s apartment), and statuses (whether a stove was on or off), and relayed this information to agents when they came nearby.
Results:The complete agents exhibited three types of emergent behavior: They spread information initially known only to themselves, formed relationships, and cooperated (specifically to attend a party). The authors gave 100 human evaluators access to all agent actions and memories. The evaluators asked the agents simple questions about their identities, behaviors, and thoughts. Then they ranked the agents’ responses for believability. They also ranked versions of each agent that were missing one or more functions, as well as humans who stood in for each agent (“to identify whether the architecture passes a basic level of behavioral competency,” the authors write). These rankings were turned into aTrueSkillscore (a variation on the Elo system used in chess) for each agent type. The complete agent architecture scored highest, while the versions that lacked particular functions scored lower. Surprisingly, the human stand-ins also underperformed the complete agents.
Yes, but:Some complete agents “remembered” details they had not experienced. Others showed erratic behavior, like not recognizing that a one-person bathroom was occupied or that a business was closed. And they used oddly formal language in intimate conversation; one ended exchanges with her husband, “It was good talking to you as always.”
Why it matters:Large language models produce surprisingly human-like output. Combined with a database and server, they can begin to simulate human interactions. While the TrueSkill results don’t fully convey how humanly these agents behaved, they do suggest a role for such agents in fields like game development, social media, robotics, andepidemiology.
We're thinking:The evaluators found the human stand-ins less believable than the full-fledged agents. Did the agents exceed human-level performance in the task of acting human, or does this result reflect a limitation of the evaluation method?
Join our upcoming workshop with Predibase and learn how to use open source tools to overcome challenges like the “host out of memory” error when fine-tuning models like Llama-2.Register now
",['https://dl-staging-website.ghost.io/content/images/2023/08/The-Batch-ads-and-exclusive-banners--50-.png']
Rigorous Trial: AI Matches Humans in Breast Cancer Diagnosis,"A deep learning system detected breast cancer in mammograms as well as experienced radiologists, according to a landmark study.
What’s new:Researchers at Lund University in Swedenconducteda randomized, controlled, clinical trial to determine whether an AI system could save radiologists’ time without endangering patients — purportedly the first study of AI’s ability to diagnose breast cancer from mammograms whose design met the so-called gold standard for medical tests. Their human-plus-machine evaluation procedure enabled radiologists to spend substantially less time per patient while exceeding a baseline for safety.
How it works:The authors randomly divided 80,000 Swedish women into a control group and an experimental group.
Results: The AI-assisted diagnosis achieved a cancer detection rate of 6.1 per 1,000 patients screened, comparable to the control method and above an established lower limit for safety. The radiologists recalled 2.0 percent of the control group and 2.2 percent of the experimental group, and both the control and experimental groups showed the same false-positive rate of 1.5 percent. (The difference in recall rates coupled with the matching false-positive rate suggests that the AI method detected 20 percent more cancer cases than the manual method, though authors didn’t emphasize that finding.) Moreover, since approximately 37,000 patients were only examined by one radiologist, the results indicate that AI saved 44.3 percent of the examination workload without increasing the number of misdiagnosed patients.
Yes, but:The authors’ method requires more study before it can enter clinical practice; for instance, tracking patients of varied genetic backgrounds. The authors are continuing the trial and plan to publish a further analysis after 100,000 patients have been enrolled for two years.
Behind the news:Radiologists have used AI to help diagnose breast cancer since the 1980s (though that method isquestionable.) A 2020studyby Google Health claimed that AI outperformed radiologists, but critics found flaws in the methodology.
Why it matters:Breast cancercausesmore than 600,000 deaths annually worldwide. This work suggests that AI can enable doctors to evaluate more cases faster, helping to alleviate a shortage of radiologists. Moreover, treatment is more effective the earlier the cancer is diagnosed, and the authors’ method caught more early than late ones.
We’re thinking:Medical AI systems that perform well in the lab often fail in the clinic. For instance, a neural network may outperform humans at cancer diagnosis in a specific setting but, having been trained and tested on the same data distribution, isn’t robust to changes in input (say, images from different hospitals or patients from different populations). Meanwhile, medical AI systems have been subjected to veryfewrandomized, controlled trials, which is considered the gold standard for medical testing. Such trials have their limitations, but they’re a powerful tool for bridging the gap between lab and clinic.
",['https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--25-.jpg']
Robots Work the Drive-Thru,"Chatbots are taking orders for burgers and fries — and making sure you buy a milkshake with them.
What’s new:Drive-thru fast-food restaurants across the United States are rolling out chatbots to take orders,The Wall Street Journalreported. Reporter Joanna Stern delivers a hilarious consumer’s-eye view in an accompanyingvideo.
How it works:Hardee’s, Carl’s Jr., Checkers and Del Taco use technology from Presto, a startup that specializes in automated order-taking systems. The companyclaims95 percent order completion and $3,000 in savings per month per store. A major selling point: Presto’s bot pushes bigger orders that yield $4,500 per month per store in additional revenue.
Behind the news:The fast-food industry is embracing AI to help out in the kitchen, too.
Yes, but:McDonald’s, the world’s biggest fast-food chain by revenue, uses technology from IBM and startup Apprente, which itacquiredin 2019. As of early this year, the systemachieved80 percent accuracy — far below the 95 percent that executives had expected.
Why it matters:In fast food, chatbots are continuing a trend in food service that began with Automat cafeterias in the early 1900s. Not only are they efficient at taking orders, apparently they’re more disciplined than typical employees when it comes to suggesting ways to enlarge a customer’s order (and, consequently, waist).
We’re thinking:When humans aren’t around, order-taking robots order chips.
Join our upcoming workshop with Weights & Biases and learn how to evaluate Large Language Model systems, focusing on Retrieval Augmented Generation (RAG) systems.Register now
","['https://dl-staging-website.ghost.io/content/images/2023/08/The-Batch-ads-and-exclusive-banners--49-.png', 'https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--44-.png']"
The High Cost of Serving LLMs,"Amid the hype that surrounds large language models, a crucial caveat has receded into the background: The current cost of serving them at scale.
What’s new:As chatbots go mainstream, providers must contend with the expense of serving sharply rising numbers of users, theWashington Postreported.
The price of scaling:The transformer architecture, which is the basis of models like OpenAI’s ChatGPT, requires a lot of processing. Its self-attention mechanism is computation-intensive, and it gains performance with higher parameter counts and bigger training datasets, giving developers ample incentive to raise the compute budget.
Why it matters:Tech giants areracingto integrate large language models into search engines, email, document editing, and an increasing variety of other services. Serving customers may require taking losses in the short term, but winning in the market ultimately requires balancing costs against revenue.We’re thinking:Despite the high cost of using large language models to fulfill web searches — which Google, Bing, and Duckduckgo do for free, thus creating pressure to cut the cost per query — for developers looking to call them, the expense looks quite affordable. In ourback-of-the-envelope calculation, the cost to generate enough text to keep someone busy for an hour is around $0.08.
",['https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--80-.gif']
Ukraine’s Homegrown Drones,"The war in Ukraine has spurred a new domestic industry.
What’s new:Hundreds of drone companies have sprung up in Ukraine since Russian forces invaded the country early last year,The Washington Postreported.
How it works:Ukrainian drone startups are developing air- and sea-borne robots, which the country’s military use to monitor enemy positions, guide artillery strikes, and drop bombs, sometimes on Russian territory.
Russia responds:In recent months, Russia has stepped up attacks by Russian-made Lancet fliers that explode upon crashing into their targets. Recent units appear to contain Nvidia Jetson TX2 computers, which could drive AI-powered guidance or targeting,Forbesreported. Russian state news denied that its drones use AI.
Behind the news:Other countries are also gearing up for drone warfare.
Why it matters:Drones rapidly have become a battlefield staple, and their offensive capabilities are growing. Governments around the world are paying close attention for lessons to be learned — as are, no doubt, insurgent forces, paramilitary groups, and drug cartels.We’re thinking:We stand with the brave Ukrainian soldiers as they defend their country against an adversary with a much larger air force. War is tragic and ugly. We wish that no one used AI-enabled weapons. But the reality is that peaceful and democratic nations do, if only to defend themselves against adversaries who do the same. We are heartened by recentagreementsto limit development of fully autonomous weapons, and we support the United Nations’proposalto ban them entirely.
",['https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--77--1.gif']
Cloud Computing Goes Generative,"Amazon aims to make it easier for its cloud computing customers to build applications that take advantage of generative AI.
What’s new:Amazon Web Services’ Bedrock platform isofferingnew generative models, software agents that enable customers to interact with those models, and a service that generates medical records. The new capabilities are available in what Amazon calls “preview” and are subject to change.
How it works:Bedrock launched in April with the Stable Diffusion image generator and large language models including AI21’s Jurassic-2 and Anthropic’s Claude. The new additions extend the platform in a few directions.
Behind the news:Amazon’s major rivals in cloud computing have introduced their own generative-AI-as-a-service offerings.
Why it matters:Access to the latest generative models is likely to be a crucial factor in bringing AI’s benefits to all industries. For Amazon, providing those models and tools to build applications on top of them could help maintain its dominant position in the market for cloud computing.
We’re thinking:One challenge to startups that provide an API for generative AI is that the cost of switching from one API to another is low, which makes their businesses less defensible. In contrast, cloud-computing platforms offer many APIs, which creates high switching costs. That is, once you've built an application on a particular cloud platform, migrating to another is impractical. This makes cloud computing highly profitable. It also makes offering APIs for generative AI an obvious move for incumbent platforms.
Join our new course “Evaluating and Debugging Generative AI,” and learn to manage and track data sources and volumes, debug your models, and conduct tests and evaluations easily.Sign up for free
","['https://dl-staging-website.ghost.io/content/images/2023/08/The-Batch-ads-and-exclusive-banners--45-.png', 'https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--78-.gif']"
K-Pop Sings in Many Tongues,"A Korean pop star recorded a song in six languages, thanks to deep learning.
What’s new:Midnatt (better known as Lee Hyun) sang his latest release, “Masquerade,” in English, Japanese, Mandarin, Spanish, and Vietnamese — none of which he speaks fluently — as well as his native Korean. The entertainment company Hybe used a deep learning system to improve his pronunciation,Reutersreported. You can listen to the resultshere.How it works:Hybe used Neural Analysis and Synthesis (NANSY), a neural speech processor developed by the Seoul-based startup Supertone, which Hybe acquired in January for $36 million.
Behind the news:The music industry has been paying close attention to generative audio models lately, as fans have used deep learning systems tomimicthe voices of established artists. Reactions from artists and music companies have been mixed.
Why it matters:This application of generated audio suggests that the technology could have tremendous commercial value. K-pop artists frequently release songs in English and Japanese, and popular musicians have recorded their songs in multiple languages since at least the 1930s, when Marlene Dietrich recorded her hits in English as well as her native German. This approach could help singers all over the world to reach listeners who may be more receptive to songs in a familiar language.We’re thinking:Auto-Tune software began as a tool for correcting flaws in vocal performances, but musicians quickly exploited it as an effect in its own right. How long before adventurous artists use pronunciation correction to, say, sing in their own languages with foreign accents?
",['https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--79-.gif']
Long-Range Weather Forecasts,"Machine learning models havepredictedweathera few days ahead of time. A new approach substantially extends the time horizon.
What’s new:Remi Lam and colleagues at Google developedGraphCast, a weather-forecasting system based on graph neural networks (GNNs). Its 10-day forecasts outperformed those of conventional and deep-learning methods.
GNN basics:A GNN processes input in the form of a graph made up of nodes connected by edges. It uses a vanilla neural network to update the representation of each node based on those of neighboring nodes. For example, nodes can represent customers and products while edges represent purchases, or — as in this work — nodes can represent local weather while edges represent connections between locations.
Key insight:Short-term changes in the weather in a given location depend on conditions in nearby areas. A graph can reflect these relationships using information drawn from a high-resolution weather map, where each node represents an area’s weather and edges connect nearby areas. However, longer-term changes in the weather depend on conditions in both nearby and distant areas. To reflect relationships between more distant areas, the graph can draw on a lower-resolution map, which connects areas at greater distances. Combining edges drawn from higher- and lower-resolution weather maps produces a graph that reflects relationships among both nearby and distant areas, making it suitable for longer-term predictions.
How it works:GraphCast produced graphs based on high- and low-resolution weather maps and processed them using three GNNs called the encoder, processor, and decoder. The authors trained the system onglobal weather data from 1979 to 2017. Given a set of weather conditions and a set of weather conditions measured 6 hours previously for all locations on Earth, GraphCast learned to predict the weather 6 hours in the future and multiples thereof.
Results:Using 2018 data, the authors compared GraphCast’s 10-day forecasts to those of a popular Europeansystemthat predicts weather based on differential equations that describe atmospheric physics. Compared to actual measurements, GraphCast achieved a lower root mean squared error in 90 percent of predictions. It produced a 10-day forecast at 0.25-degree resolution in under 60 seconds using a single TPU v4 chip, while the European system, which forecasts at 0.1-degree resolution, needed150 to 240 hourson a supercomputer. GraphCast also outperformed Pangu-Weather, a transformer-based method, in 99.2 percent of predictions.
Yes, but:GraphCast’s predictions tended to be closer to average weather conditions, and it performed worse when the weather included extreme temperatures or storms.
Why it matters:Given a graph that combines multiple spatial resolutions, GNN can compute the influence of weather over large distances using relatively little memory and computation. This sort of graph structure may benefit other applications that process large inputs such as ultra-high resolution photos, fluid dynamics, and cosmological data.
We’re thinking:When it comes to forecasting weather, it looks like deep learning is the raining champ.
The best defense against disruption is your ability to take advantage of innovation. That’s why enabling employees to learn rapidly is a business imperative.Read Kian Katanforoosh's essential guide to learning velocity for business.
Stack Overflow answers questions using generative AIThe developer forum released OverflowAI, a toolkit that allows users to pose questions and generates answers from a database of 58 million public discussions, citing specific sources. (VentureBeat)Indian nonprofit hires rural workers to generate dataKarya employs people in rural India to record phrases in languages rarely found online, to be used as training data for language models. The company pays these workers a high wage relative to the rural areas where they live. (Time)VC firm published tutorial for building AI romantic partnersAndreessen Horowitz released a tutorial on GitHub that shows users how to build AI companions with configurable personalities and backstories. The firm said the project has therapeutic potential. (Decrypt)Cybercriminals offer a chatbot for sophisticated attacksThe bot, called FraudGPT, can craft spear-phishing emails, write harmful code, make undetectable malware, and other malicious products. It’s being advertised on web marketplaces and Telegram channels. (The Hacker News)Officials regulate Wall Street's use of AIThe United States Securities and Exchange Commission approved a plan to address conflicts of interest in the use of AI by brokerages and money managers. The proposal requires companies to assess potential conflicts and disclose cybersecurity incidents, among other actions. (BNN Bloomberg)Stability AI launched Stable Diffusion XL 1.0The updated text-to-image model delivers more vibrant and accurate colors, better contrast, and improved text generation. Bedrock, Amazon's cloud platform for generative AI models, will host it. (TechCrunch)OpenAI's head of Trust and Safety resignedIndustry veteran Dave Willner cited the demands of his job and a desire to spend more time with his family as reasons for stepping down. His departure comes as the AI industry faces questions about regulation, safety, and harmful impacts, especially regarding generative AI. (TechCrunch)Wayfair offers virtual room restylerThe online home store launched Decorify, tool to visualize redecorated rooms based on uploaded pictures and selected visual styles. Users can browse furniture recommendations based on the AI-remodeled rooms. (The Verge)Report:AI will take more jobs from women than menA study by the McKinsey Global Institute indicates that millions of jobs filled primarily by women, like office support and retail sales, will be eliminated by 2030 due to AI and automation. This shift in the labor market will result in a growing demand for social-emotional and digital skills, the consultancy forecasts. (Gizmodo)Infrastructure giant to expand AI at Heathrow AirportBy 2024, Ferrovial aims to deploy a new AI services in its main toll road and airport project, including London's Heathrow Airport. Applicationsinclude predicting passenger traffic changes, identifying anomalies, and detecting airport bottlenecks. (Reuters)Research:ChatGPT and other chatbots vulnerable to hackersA report by Carnegie Mellon University and the Center for AI Safety revealed that even tightly controlled systems from Anthropic, Google, and OpenAI could be manipulated using methods developed using open source AI systems. For instance, adding a lengthy suffix to a prompt unleashes chatbots that have been fine-tuned to withhold harmful information. (The New York Times)OpenAI withdrew AI detection toolThe tool, introduced six months ago, was aimed at helping educators identify AI-generated work but proved insufficiently accurate. OpenAI plans to improve it and explore more effective detection techniques. Meanwhile, educators are turning to alternatives like Turnitin and ZeroGPT. (CNN)New organization promotes responsible AIGoogle, Microsoft, OpenAI, and Anthropic established the Frontier Model Forum, abody aimed at ensuring the safe and responsible development of advanced AI models. However, critics remain skeptical of the industry's ability to regulate itself. (The Guardian)30 percent of IT workers believe AI will not hurt their jobsA Pew Research Center survey examined worker attitudes toward AI in a variety of industries. Workers in sectors where AI is already assisting their jobs, like information technology, feel more positive about the technology's impact, they found. Some workers expressed concerns about misinformation, bias, and potential concentration of wealth and privilege. (The Washington Post)ChatGPT plug-ins spark security concernsResearchers highlighted potential risks of third-party plug-ins for ChatGPT, which may expose users to data theft and malicious code execution. OpenAI acknowledged the concerns and said it is working to improve security. (Wired)
",['https://dl-staging-website.ghost.io/content/images/2023/08/LearningVelocity_July2023_V2.png']
AI Firms Agree to Voluntary Guidelines,"In the absence of nationwide laws that regulate AI, major U.S. tech companies pledged to abide by voluntary guidelines — most of which they may already be following.
What’s new:Amazon, Anthropic, Google, Inflection, Meta, Microsoft, and OpenAI agreed to uphold a list of responsible-AI commitments, the White Houseannounced.
How it works:President Biden, Vice President Harris, and other administration officials formulated the terms of the agreement in consultation with tech leaders. The provisions fall into three categories:
Behind the news:The surge of generative AI has spurredcallsto regulate the technology. The rising chorus has given companies ample incentive to accept voluntary limits while trying to shape forthcoming mandates.
Yes, but:The commitments — with the exception of watermarking generated output — are relatively easy to fulfill, and some companies may be able to say that they already fulfill them. For instance, many established companies employ independent parties to test for safety and security, and some publish papers that describe risks of their AI research. Leaders in the field already discuss limitations, work to reduce risks, and launch initiatives that address major societal problems. Moreover, the agreement lacks ways to determine whether companies have kept their promises and hold shirkers to account.
Why it matters:Although some U.S. cities and states regulate AI in piecemeal fashion, the country lacks overarching national legislation. Voluntary guidelines, if companies observe them in good faith and avoid hidden pitfalls, could ease the pressure to assert top-down control over the ways the technology is developed and deployed.
We’re thinking:These commitments are a step toward guiding AI forward in ways that maximize benefits and minimize harms — even if some companies already fulfill them. Nonetheless, laws are necessary to ensure that AI’s benefits are spread far and wide throughout the world. Important work remains to craft such laws, and they’ll be more effective if the AI community participates in crafting them.
",['https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--73-.gif']
Apple Grapples With Generative AI,"Apple insiders spoke anonymously about the company’s effort to exploit the current craze for chatbots.
What’s new:Apple built a framework for large language models and used it to develop a chatbot dubbed Apple GPT — for internal use only,Bloombergreported.
Under wraps:The iPhone maker is proceeding cautiously to capitalize on the hottest tech trend since mobile. The results are not yet available to the public and may never be.
Behind the news:Apple tends to hold its technology close to its vest, but it has not placed the same emphasis on AI as peers. Its pioneering Siri voice assistant has been criticized for falling behind competitors like Amazon Alexa and Google Assistant (which, in turn, were criticized for falling behind ChatGPT). Although it has publishedpaperson generative AI in recent years, its recent productshave not emphasizedthe technology. Meanwhile, its big-tech rivals have been trying to outdo one another in building and deploying ever more powerful chatbots.
Why it matters:Where some companies zig, Apple often zags. Unlike its peers, it makes its money selling devices and requires tight integration between that hardware and the software that brings it to life. Such differences may make it necessary to “think different” about generative AI.
We’re thinking:Apple's control over the iOS and MacOS ecosystems is a huge strength in the race to capitalize on generative AI. We hope that Apple’s generative products will be wonderful, but even if they offer little advantage over the competition, its ability to get them into users’ hands will give it a significant advantage over smaller competitors and even many large companies.
Join “Building Generative AI Applications with Gradio,” our new course built in collaboration with Hugging Face. Learn to quickly build, demo, and ship models using Gradio’s user-interface tools!Sign up for free
","['https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--40-.png', 'https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--74-.gif']"
ChatGPT Ain’t What It Used to Be,"It wasn’t your imagination: OpenAI’s large language models have changed.
What’s new:Researchers at Stanford and UC Berkeleyfoundthat the performance of GPT-4 and GPT-3.5 has drifted in recent months. In a limited selection of tasks, some prompts yielded better results than before, some worse.How it works:The authors compared the models’ output in March and June. They aimed not to evaluate overall performance but to show that it had shifted on certain tasks. They prompted the models via ChatGPT to (i) identify whether a prime number is prime, (ii) handle sensitive or harmful prompts, (iii) generate executable code, and (iv) and solve visual puzzles.
Yes, but:Commenting on the findings, Princeton computer scientists Arvind Narayanan and Sayash Kapoornotedthat performance differences reported in the paper were consistent with shifts in behavior following fine-tuning. They distinguished between a large language model’scapability(that is, what it can and can’t do given the right prompt), which is informed by pretraining, and itsbehavior(its response to a given prompt), which is shaped by fine-tuning. The paper showed that, while the models’ behavior had changed between March and June, this did not necessarily reflect changes in their capability. For instance, the paper’s authors asked the models to identify only prime numbers as primes; they didn’t test non-primes. Narayanan and Kapoor tested the models on non-primes and obtained far better performance.
Behind the news:For months, rumors have circulated that ChatGPT’s performance had declined. Some usersspeculatedthat the service was overwhelmed by viral popularity, OpenAI had throttled its performance to save on processing costs, or user feedback had thrown the model off kilter. In May, OpenAI engineer Logan Kilpatrickdeniedthat the underlying models had changed without official announcements.
Why it matters:While conventional software infrastructure evolves relatively slowly, large language models are changing much faster. This creates a special challenge for developers, who have a much less stable environment to build upon. If they base an application on an LLM that later is fine-tuned, they may need to modify the application (for example, by updating prompts).
We’re thinking:We’ve known we needed tools to monitor and managedata drift and concept drift. Now it looks like we also need tools to check whether our applications work with shifting LLMs and, if not, to help us update them efficiently.
",['https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--75-.gif']
Stratego Master,"Reinforcement learning agents have mastered games like Go that provide complete information about the state of the game to players. They’ve also excelled at Texas Hold ’Em poker, which provides incomplete information, as few cards are revealed. Recent work trained an agent to excel at a popular board game that, like poker, provides incomplete information but, unlike poker, involves long-term strategy.
What’s new:Julien Perolat, Bart De Vylder, Karl Tuyls, and colleagues at DeepMind teamed up with former Stratego world champion Vincent de Boer to conceiveDeepNash, a reinforcement learning system that reached expert-level capability at Stratego.
Stratego basics:Stratego is played by two opposing players. The goal is to capture the opponent’s flag piece by moving a piece onto a space that contains it. The game starts with a deployment phase, in which the players place on a board 40 pieces that represent military ranks, as well as a flag and a bomb. The pieces face away from the opposing player, so neither one knows the other’s starting formation. The players move their pieces by turns, potentially attacking each other’s pieces by moving onto a space occupied by an opponent’s piece; which reveals the rank of the opponent’s piece. If the attacking piece has a higher rank, the attack is successful and the opponent’s piece is removed from the board. If the attacking piece has a lower rank, the attack fails and the attacking piece is removed.
Key insight:A reinforcement learning agent like AlphaGo learns to play games through self-play; that is, it plays iteratively against a copy of itself, adjusts its weights according to rewards it has received, and — after an interval of learning — adopts the weights of the better-performing copy. Typically, each copypredictsthe potential outcome of every possible action and chooses the one that’s most likely to confer an advantage. However, this approach can go awry if one of the copies learns to win by exploiting a vulnerability that’s idiosyncratic to the agent but not to human players. That’s where regularization can help: To prevent such overfitting and enable agents to learn a more generalized strategy, previousworkshowed that it helps to reward an agent for — in addition to good moves and winning — predicting the same probabilities that actions will be advantageous as an earlier version of itself. Updating this earlier version periodically enables the agent to keep improving.
How it works:DeepNash comprised fiveU-Netconvolutional neural networks. One produced an embedding based on the current state of the game board and the most recent 40 previous states. The remaining four U-Nets used the embedding as follows: (i) during training, to estimate the total future reward to be expected after executing a deployment or move, (ii) during the game’s deployment phase, to predict where each piece should be deployed, (iii) during the play phase, to select which piece to move and (iv) to decide where that piece should move.
Results:DeepNash beat the most powerful Stratego bots on theGravongame platform, winning 97.1 percent of 800 games. It beat Gravon’s human experts 84 percent of the time, ranking third as of April 22, 2022. Along the way, it developed deceptive tactics, fooling opponents by moving less-powerful pieces as though they were more powerful and vice-versa.
Why it matters:Reinforcement learning is a computationallyinefficientway to train a model from scratch to find good solutions among a plethora of possibilities. But it mastered Go, a game with 10360possible states, and it predicts protein shapes among10300possible configurations of amino acids. DeepNash sends the message that reinforcement learning can also handle Stratego’s astronomical number of 10535states, even when those states are unknown.
We’re thinking:DeepNash took advantage of the Stratego board’s imperfect information by bluffing. Could it have developed atheory of mind?
Join our upcoming workshop on August 3, 2023, at 10:00 a.m. Pacific Time! Learn the fundamentals of reinforcement learning and how to integrate human feedback into the learning process.Register now
",['https://dl-staging-website.ghost.io/content/images/2023/07/The-Batch-ads-and-exclusive-banners--43-.png']
Copyright Owners Take AI to Court,"AI models that generate text, images, and other types of media are increasingly under attack by owners of copyrights to material included in their training data.
What’s happening:Writers and artists filed a new spate oflawsuitsalleging that AI companies including Alphabet, Meta, and OpenAI violated their copyrights by training generative models on their works without permission. Companies took steps to protect their interests and legislators considered the implications for intellectual property laws.
Lawsuits and reactions:The lawsuits, which are ongoing, challenge a longstanding assumption within the AI community that training machine learning models is allowed under existing copyright laws. Nonetheless, OpenAI responded by cutting deals for permission to use high-quality training data. Meanwhile, the United States Senate is examining the implications for creative people, tech companies, and legislation.
Behind the news:The latest court actions, which focus on generated text, follow two earlier lawsuits arising from different types of output. In January, artists Sarah Anderson, Kelly McKernan, and Karla Ortiz (who spoke in the Senate hearing)suedStability AI, Midjourney, and the online art community DeviantArt. In November, two anonymous plaintiffs sued GitHub, Microsoft, and OpenAI saying the companies trained the Copilot code generator using routines from GitHub repositories in violation with open source licenses.
Why it matters:Copyright laws in the United States and elsewhere don’t explicitly forbid use of copyrighted works to train machine learning systems. However, the technology’s growing ability to produce creative works, and do so in the styles of specific artists and writers, has focused attention on such use and raised legitimate questions about whether it’s fair. This much is clear: The latest advances in machine learning have depended on free access to large quantities of data, much of it scraped from the open internet. Lack of access to corpora such asCommon Crawl,The Pile, andLAION-5Bwould put the brakes on progress or at least radically alter the economics of current research This would degrade AI’s current and future benefits in areas such as art, education, drug development, and manufacturing to name a few.
We’re thinking:Copyright laws are clearly out of date. We applaud legislators who are confronting this problem head-on. We hope they will craft laws that, while respecting the rights of creative people, preserve the spirit of sharing information that has enabled human intelligence and, now, digital intelligence to learn from that information for the benefit of all.
",['https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--70-.gif']
Chatbot Cage Match,"A new online tool ranks chatbots by pitting them against each other in head-to-head competitions.
What’s new:Chatbot Arenaallows users to prompt two large language models simultaneously and identify the one that delivers the best responses. The result is a leaderboard that includes both open source and proprietary models.How it works:When a user enters a prompt, two separate models generate their responses side-by-side. The user can pick a winner, declare a tie, rule that both responses were bad, or continue to evaluate by entering a new prompt.
Who’s ahead?:As of July 19, 2023, OpenAI’s GPT-4 topped theleaderboard. Two versions of Anthropic’s Claude rank second and third. GPT-3.5-turbo holds fourth place followed by two versions of Vicuna (LLaMA fine-tuned on shared ChatGPT conversations).Why it matters:Typical language benchmarksassess model performance quantitatively. Chatbot Arena provides a qualitative score, implemented in a way that can rank any number of models relative to one another.
We’re thinking:In a boxing match between GPT-4 and the 1960s-vintage ELIZA, we’d bet on ELIZA. After all, it used punch cards.
Check out our course on “Generative AI with Large Language Models”! Developers hold the key to leveraging LLMs as companies embrace this transformative technology. Take this course and confidently build prototypes for your business.Enroll today
","['https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--38-.png', 'https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--39-.png']"
What Venture Investors Want,"This year’s crop of hot startups shows that generative AI isn’t the only game in town.
What’s new:CB Insights, which tracks the tech-startup economy,releasedthe 2023 edition of its annual AI 100, a list of 100 notable AI-powered ventures. The researchers considered 9,000 startups and selected 100 standouts based on criteria such as investors, business partners, research and development activity, and press reports.Where the action is:The list divides roughly evenly into three categories: Startups that offer tools for AI development, those that address cross-industry functions, and those that serve a particular industry. The names of the companies are noteworthy, but the markets they serve are more telling.
Follow the money:Together, these startups have raised $22 billion in 223 deals since 2019. (Microsoft’s investment in OpenAI accounts for a whopping $13 billion of that total.) Half are in the very early stages.
Why it matters:Venture funding drives a significant portion of the AI industry. That means opportunities for practitioners at both hot ventures and me-too companies that seek to cultivate similar markets. The startup scene is volatile — as the difference between this year’s andlast year’s AI100demonstrates — but each crop of new firms yields a few long-term winners.We’re thinking:Startup trends are informative, but the options for building a career in AI are far broader. Established companies increasingly recognize their need for AI talent, and fresh research opens new applications. Let your interests lead you to opportunities that excite and inspire you.
",['https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--71-.gif']
DeepLearning.AI Exclusive,"Robert Monarch, the instructor of our new specializationAI for Good, spoke with us about how AI is being applied to social and environmental challenges and how you can join the growing AI for Good movement.Read the interview
",['https://dl-staging-website.ghost.io/content/images/2023/07/V2-1-1-1536x864.png']
Stable Biases,"Stable Diffusion may amplify biases in its training data in ways that promote deeply ingrained social stereotypes.
What's new:The popular text-to-image generator from Stability.ai tends to underrepresent women in images of prestigious occupations and overrepresent darker-skinned people in images of low-wage workers and criminals,Bloombergreported.
How it works:Stable Diffusion was pretrained onfive billion text-image pairsscraped from the web. The reporters prompted the model to generate 300 face images each of workers in 14 professions, seven of them stereotypically “high-paying” (such as lawyer, doctor, and engineer) and seven considered “low-paying” (such as janitor, fast-food worker, and teacher). They also generated images for three negative keywords: “inmate,” “drug dealer,” and “terrorist.” They analyzed the skin color and gender of the resulting images.
Results:Stable Diffusion’s output aligned with social stereotypes but not with real-world data.
Behind the news:Image generators have been found to reproduce and often amplify biases in their training data.
Why it matters:Not long ago, the fact that image generators reflect and possibly amplify biases in their training data was mostly academic. Now, because a variety of software products integrate them, such biases can leach into products as diverse as video games, marketing copy, and law-enforcement profiles.
We're thinking:While it’s important to minimize bias in our datasets and trained models, it’s equally important to use our models in ways that support fairness and justice. For instance, a judge who weighs individual factors in decisions about how to punish a wrongdoer may be better qualified to decide than a model that simply reflects demographic trends in criminal justice.
",['https://dl-staging-website.ghost.io/content/images/2023/07/FINANCE--1-.gif']
AI & Banking: Progress Report,"One bank towers above the competition when it comes to AI, a recent study suggests.
What’s new:Areportfrom market research firm Evident Insights measures use of AI by the banking industry.How it works:The Evident AI Index scored 23 large North American and European banks in four categories. The analysis combined the scores into a total for each bank.
Results:JPMorgan Chase excelled in all four categories with a combined score of 62.6 out of 100. The next-highest scorers were Royal Bank of Canada (41.4) and Citigroup (39.0). The authors credited JPMorgan Chase with successful long-term investments in AI research coupled with an openness to letting AI talent publish academic work. Other highlights:
Behind the news:A growing number of banks are taking advantage of generative AI.
Why it matters:Finance is among the few industries outside tech that can afford to hire large teams of top AI talent. It’s also a data-heavy industry where applications — fraud detection, financial forecasting, and reconciling and closing accounts — can bring a ready payoff. The combination has made banking a hotbed for AI talent.We’re thinking:It’s interesting to see one bank so far out ahead in this analysis. We imagine that AI adoption on banking can bring significant first-mover advantages.
Introducing Skills AI, the tool that harnesses large language models for managers to develop, upskill, and retain teams at the cutting edge of competency.Join the waitlist
","['https://dl-staging-website.ghost.io/content/images/2023/07/DeepLearning_BatchAd_SkillsAI.png', 'https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--69-.gif']"
Language Models’ Impact on Jobs,"Telemarketers and college professors are most likely to find their jobs changing due to advances in language modeling, according to a new study.What’s new:A team led by Ed Felten, a computer scientist at Princeton University and former deputy CTO of the United States,projectedthe jobs and industries in the U.S. likely to be most affected by language models.How it works:The authors calculated an “exposure” score for each of 774 occupations and 115 industries by comparing human skills to AI application areas. For the purpose of the study, exposure is neither positive nor negative; it’s a measure of how likely a job or industry would change in response to developments in language processing.
Results:The authors concluded that telemarketing was most exposed to impact by language models. Among the 20 occupations with the greatest exposure, 14 were post-secondary teaching roles including university-level teachers of language, history, law, and philosophy. The top 20 also included sociologists, political scientists, arbitrators, judges, and psychologists. Among industries, the authors found that legal services were most exposed. Of the 20 industries with the greatest exposure, 11 involved finance including securities, insurance, and accounting.
Behind the news:The authors adapted their method from a 2021studythat scored each occupation’s and each industry’s exposure to AI areas defined by the Electronic Frontier Foundation, including game playing, computer vision, image generation, translation, and music recognition. The previous study found that the most exposed jobs were genetic counselors, financial examiners, and actuaries. The most exposed industries were financial securities, accounting, and insurance.Why it matters:It seems clear that emerging AI technologies will have a significant impact on human labor, but where and how is not yet clear (and may not be even as the effects become more pervasive). This study can serve as a heads-up to some professionals that it’s time to prepare — and a signal to AI builders what sorts of models are likely to have an impact.
We’re thinking:As the authors note, an occupation’s exposure to AI does not necessarily put jobs at risk. History suggests the opposite can happen. A 2022 studyfoundthat occupations exposed to automation saw increases in employment between 2008 and 2018. Several other studiesfoundthat countries with high levels of automation also tend to have high overall levels of employment.
",['https://dl-staging-website.ghost.io/content/images/2023/07/HUMANLOOP--1-.gif']
The Secret Life of Data Labelers,"The business of supplying labeled data for building AI systems is a global industry. But the people who do the labeling face challenges that impinge on the quality of both their work and their lives.
What’s new:The Vergeinterviewed more than two dozen data annotators,revealinga difficult, precarious gig economy. Workers often find themselves jaded by low pay, uncertain schedules, escalating complexity, and deep secrecy about what they’re doing and why.How it works:Companies that provide labeling services including Centaur Labs, Surge AI, and Remotasks (a division of data supplier Scale AI) use automated systems to manage gig workers worldwide. Workers undergo qualification exams, training, and performance monitoring to perform tasks like drawing bounding boxes, classifying sentiments expressed by social media posts, evaluating video clips for sexual content, sorting credit-card transactions, rating chatbot responses, and uploading selfies of various facial expressions.
What they’re saying:“AI doesn’t replace work. But it does change how work is organized.” —Erik Duhaime, CEO, Centaur Labs
Behind the news:Stanford computer scientist Fei-Fei Li was an early pioneer in crowdsourcing data annotations. In 2007, she led a team at Princeton to scale the number of images used to train an image recognizer from tens of thousands to millions. To get the work done, the team hired thousands of workers via Amazon’s Mechanical Turk platform. The result was ImageNet, a key computer vision dataset.
Why it matters:Developing high-performance AI systems depends on accurately annotated data. Yet the harsh economics of annotating at scale encourages service providers to automate the work and workers to either cut corners or drop out. Notwithstanding recent improvements — for instance, Googleraisedits base wage for contractors who evaluate search results and ads to $15 per hour — everyone would benefit from treating data annotation less like gig work and more like a profession.We’re thinking:The value of skilled annotators becomes even more apparent as AI practitioners adopt data-centric development practices that make it possible to build effective systems with relatively few examples. With far fewer examples, selecting and annotating them properly is absolutely critical.
",['https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--67-.gif']
Making Government Multilingual,"An app is bridging the language gap between the Indian government and its citizens, who speak a wide variety of languages.
What’s new:Jugalbandihelps Indians learn about government services, which typically are described online in English and Hindi, in their native tongues. The project is a collaboration between Microsoft and open-source developers AI4Bharat and OpenNyAI.How it works:Jugalbandi harnesses an unspecified GPT model from the Microsoft Azure cloud service and models from AI4Bharat, a government-backed organization that provides open-source models and datasets for South Asian languages. As of May, the system covered 10 of India’s 22 official languages (out ofmore than 120that are spoken there) and over 170 of the Indian government’s 20,000 programs.
Behind the news:While language models are helping citizens understand their governments, they’re also helping governments understand their citizens. In March, RomanialaunchedION, an AI system that scans social media comments on government officials and policy and summarizes them for ministers to read.
Why it matters:India is a highly multilingual society, andaround a quarterof its 1.4 billion residents are illiterate. Consequently, many people in India struggle to receive government benefits and interact with central authorities. This approach may enable Indians to use their own language via WhatsApp, whichhasmore than 400 million users in that country.
We’re thinking:In February, Microsoft researchersshowedthat large language models are approaching state-of-the-art results in machine translation. Indeed, machine translation is headed toward a revolution as models like GPT 3.5 (used in the study) and GPT-4 (which is even better) make translations considerably easier and more accurate.
Chatting with data is a highly valuable use case for large language models. In this short course, you’ll use the open source LangChain framework to build a chatbot that interacts with your business or personal data.Enroll in ""LangChain: Chat with Your Data”today for free!
","['https://dl-staging-website.ghost.io/content/images/2023/07/The-Batch-ads-and-exclusive-banners--41-.png', 'https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--34-.png']"
Letting Chatbots See Your Data,"A new coding framework lets you pipe your own data into large language models.
What’s new:LlamaIndexstreamlines the coding involved in enabling developers to summarize, reason over, and otherwise manipulate data from documents, databases, and apps using models like GPT-4.How it works:LlamaIndex is a free Pythonlibrarythat works with any large language model.
Behind the news:Former Uber research scientist Jerry Liu began building LlamaIndex (originally GPT Index) in late 2022 and co-founded a company around it earlier this year. The company, which recentlyreceived$8.5 million in seed funding, plans to launch an enterprise version later this year.Why it matters:Developing bespoke apps that use a large language model typically requires building custom programs to parse private databases. LlamaIndex offers a more direct route.We’re thinking:Large language models are excitingnew tools for developing AI applications. Libraries like LlamaIndex andLangChainprovide glue code that makes building complex applications much easier — early entries in a growing suite of tools that promise to make LLMs even more useful.
",['https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--68-.gif']
Generated Data Fouls Human Datasets,"The crowdworkers you hire to provide human data may use AI to produce it.
What's new:Researchers at École Polytechnique Fédérale de Lausannefoundthat written material supplied by workers hired via Amazon Mechanical Turk showed signs of being generated by ChatGPT.
How it works:44 Mechanical Turk workers summarized medical research abstracts in roughly 100 words. The authors analyzed each summary for evidence that it had been generated by ChatGPT. The analysis relied on two methods:
Results:The authors analyzed 46 summaries written by 44 workers. The classifier found 21 summaries that showed 50 percent or greater likelihood of having been written by ChatGPT and 15 summaries that showed at least a 98 percent or greater likelihood. 41 of the summaries involved copying and pasting.
Yes, but:The researchers studied 46 summaries, a rather small sample. Furthermore, summarization is labor-intensive for humans but well within the capabilities of large language models. Other crowdsourced tasks may not be so easy to automate.
Behind the news:Mechanical Turk, founded by Amazon in 2005, has played anoutsizerole in machine learning. Many of the field’s most important datasets includingImageNetemployed crowdsourced labor.
Why it matters:Machine learning engineers often use services like Mechanical Turk to collect and annotate training data on the assumption that humans are doing the work. If a significant number of crowdworkers instead rely on AI, it raises questions about the quality of the data and the validity of the output from models trained on it. Recentworkfound that, as the amount of model-generated content in a training set increases, the trained model’s performance decreases.
We're thinking:Training on machine-generated data seems likely to affect model performance unless you’re training a smaller model to mimic a larger one (known as model distillation). For example, it’s hard to imagine a language model trained only on the output of ChatGPT surpassing ChatGPT, whereas one trained on human data might. The lack of transparency with respect to which data comes from humans and which comes from machines presents a huge challenge for AI practitioners.
",['https://dl-staging-website.ghost.io/content/images/2023/06/unnamed--21-.jpg']
Where Is Meta’s Generative Play?,"While Microsoft and Google scramble to supercharge their businesses with text generation, Meta has yet to launch a flagship generative AI service. Reporters went looking for reasons why.
What’s new:Staff turnover, misaligned priorities, insufficient processing power, and caution in the wake of earlier controversies have hindered Meta’s ability to take advantage of generative AI,The Wall Street Journalreported.Challenges:Reporters spoke to more than a dozen current and former Meta employees to determine why, despite extensive investments in large language models (LLMs) and vision models like DINOv2 and SAM, the company lacks a high-profile generative initiative. They pointed to several factors:
Reorganization:Meta has taken steps to break the logjam. Earlier this month, itannounceda number of generative AI products including chatbots for Messenger and WhatsApp, a photo editor for Instagram, and a productivity assistant for internal use. In February, Meta CEO Mark Zuckerburgannounceda new generative AI group that reports directly to chief product officer Chris Cox. The group will focus on training models to integrate with products such as Facebook, Instagram, and WhatsApp.
Why it matters:The rapid rise of generative AI threatens to upend the tech world’s established order. Meta — like Google in response Microsoft’s aggressive launch of Bing Chat — has found itself in a defensive position.We’re thinking:OpenAI developed breakthrough technology using a focused team of hundreds, and since then, several organizations have restructured from handfuls of researchers who work on diverse projects to large, focused teams that include both researchers and engineers. Although this shift prompted many researchers to leave in search of freedom to pursue their interests, the focused structure strikes us as a more promising approach from a business point of view.
Master the technology behind large language models and learn how to fine-tune and use them to power real-world applications. Join us for “Generative AI with Large Language Models,” a new course developed in collaboration with AWS.Enroll now!
","['https://dl-staging-website.ghost.io/content/images/2023/06/The-Batch-ads-and-exclusive-banners--36-.png', 'https://dl-staging-website.ghost.io/content/images/2023/06/unnamed--22-.jpg']"
Washington Gears Up to Regulate,"United States lawmakers are getting a crash course in AI.
What’s new:Chuck Schumer, the majority leader in the U.S. Senate, announced an unusual plan to educate legislators who are crafting AI regulations,The New York Timesreported. It could lead to legislation “within months,” he said.How it works:The senator calls his program SAFE Innovation, an acronym for four regulatory priorities: security, accountability, foundations, and explain [sic].
Behind the news:Schumer’s move reflects growing interest in regulating AI among U.S. lawmakers.
Yes, but:Any proposal must overcome fundamental disagreements between the two major parties, especially over whether a new, dedicated agency should oversee AI or whether that can be left to existing agencies. Moreover, some observers worry that Schumer’s deliberative approach could slow down legislative efforts that are already underway.Why it matters:Thoughtful AI regulations must strike a delicate balance between encouraging innovation and protecting the public. It’s imperative that lawmakers — few of whom have a background in technology or science — understand the nuances.
We’re thinking:U.S. politics are increasingly divided. Bipartisan listening sessions on AI may serve a dual goal of educating lawmakers and uniting them around a shared vision.
",['https://dl-staging-website.ghost.io/content/images/2023/06/FINETUNINGv2--1-.gif']
Economic Forecast: GenAI Boom,"Generative AI could add between $2.6 trillion and $4.4 trillion to the global economy annually (roughly 2 percent to 4 percent of the world’s combined gross domestic product this year), according to a new report.
What's new:The management consultancy McKinseyprojectedgenerative AI’s impacts on productivity, automation, and the workforce in a new report.
How it works:The authors examined adoption scenarios between 2040 and 2060 and their effect on labor productivity through 2040. They evaluated the business impact of generative AI use cases — for instance, large language models applied to customer service — and estimated the economic value those cases would create if they were applied globally. They also assessed the technology’s potential to automate tasks in roughly850 occupationsbased on an occupation’s sensory, cognitive, physical, language, and social requirements.
Behind the news:Generative AI’s potential to displace human workers is causing substantial anxiety among the general public. A recent CNBC survey of 8,874 U.S. workersfoundthat 24 percent of respondents were “very worried” or “somewhat worried” that AI would make their jobs obsolete. Respondents were more likely to worry if they were younger (32 percent of respondents of age 18 to 24 compared to 14 percent of those 65 or older), identified as part of a minority (38 percent of Asian respondents, 35 percent of Hispanic respondents, and 32 percent of black respondents versus 19 percent of white respondents), or earned a relatively low income (30 percent of respondents who earn less than $50,000 annually versus 16 percent of those who earn more than $150,000).
Yes, but:As the saying goes, it’s difficult to make predictions, especially about the future. A decade after a 2013 Oxford University studypredictedthat 47 percent of U.S. jobs were at risk of automation, the U.S. unemployment rate is nearly at recordlows. A 2022 study found that employment rates haverisenin occupations previously believed to be at risk from AI and robotics.
Why it matters:Generative AI already is having a noticeableeffecton venture investments. This analysis indicates that current changes may herald disruptive impacts to come.
We're thinking:Prospective economic gains are good news, but they should be considered in a broader context. We see a realriskthat AI may become so good at automating human work that many people will find themselves unable to generate substantial economic value. The best path forward is to democratize the technology so everyone can benefit and make sensible decisions together.
Why it matters:Generative AI already is having a noticeableeffecton venture investments. This analysis indicates that current changes may herald disruptive impacts to come.
We're thinking:Prospective economic gains are good news, but they should be considered in a broader context. We see a realriskthat AI may become so good at automating human work that many people will find themselves unable to generate substantial economic value. The best path forward is to democratize the technology so everyone can benefit and make sensible decisions together.
",['https://dl-staging-website.ghost.io/content/images/2023/06/TESLA-Crashes-increase2_1200px.jpg']
More Tesla Crashes,"Tesla cars operating semi-autonomously have had many more collisions than previously reported, government data shows.
What's new:Tesla vehicles operating in the so-called Autopilot or Full Self-Driving mode were involved in 736 U.S. crashes between sometime in 2019 and May 2023, according to data gathered by the United States National Highway Traffic Safety Administration (NHTSA),The Washington Postreported.Earlier datashowed that Teslas had been involved in 273 reported crashes between July 2021 and July 2022. The latest data is available at the bottom of thislink.
How it works:Tesla offers two semi-autonomous driving modes.
The crashes:The NHTSA data is difficult to interpret, since it omits crucial variables such as miles driven and which of Tesla’s two modes was involved in any given crash. Moreover, the earlier and recent crash tallies are difficult to compare due to the difference in their time frames.
Behind the news:Since August 2021, NHTSA hasopenednumerous probes into Tesla’s autonomous systems. Repeated incidents under investigation include abrupt braking in the path of following vehicles; collisions with emergency vehicles; and allegations that, in multiple crashes, Autopilot disengaged less than a second before the collision, giving drivers little time to react.
Why it matters:Tesla has claimed repeatedly that its autonomous driving capability is far safer than human drivers. Without knowing which mode was involved in how many crashes over how many miles, that claim is impossible to verify. Meanwhile, there are indications that Tesla may have deliberatelymisledthe public about its self-driving capabilities in the past.
We're thinking:Engineers who work on systems that are critical to safety have a special responsibility to make sure their products are safe and well understood by users. We urge Tesla engineers to shed more light on the performance of these potentially life-threatening systems.
Want to build computer vision into your applications? Train a model in LandingLens (get started for free), then use theLanding AI SDKto easily build custom applications that leverage your model!
","['https://dl-staging-website.ghost.io/content/images/2023/06/The-Batch-ads-and-exclusive-banners--35-.png', 'https://dl-staging-website.ghost.io/content/images/2023/06/GPTLAW_Scroll_600px--2-.gif']"
Lawyers: Beware LLMs,"A United States federal judge threw ChatGPT’s legal research out of court.
What’s new:An attorney who used ChatGPT to generate a legal brief faces disciplinary action after opposing lawyers discovered that the brief referred to fictional cases and quotations invented by the chatbot,The New York Timesreported.Citation situation:The lawyer, Steven A. Schwartz, was assisting in a personal injury lawsuit on the plaintiff’s side in a federal court in New York City. When the defendant appealed to have the case dismissed, Schwartz countered with a brief based on results from a ChatGPT query.
Ripple effects:In the case’s wake of this case, a federal judge in Texasdecreedthat lawyers in cases before him may use generative AI to write their briefs only if they file paperwork stating that they manually verified the output for accuracy.
Why it matters:Within the AI community, it may be common knowledge that large language models sometimes confidently state falsehoods as though they were true. Among the general public, though, this fact may not be so well understood. Schwartz’s mishap is a painfully public demonstration of what can happen when people trust such models to supply facts.We’re thinking:People outside the AI community might reasonably assume that the technology is qualified to assist in legal research.  After all, in April, GPT-4, the large language model behind the most powerful version of ChatGPT, reportedlyranked in the 90th percentileon a U.S. bar exam. (A recent reappraisalrevisedGPT-4’s score downward to between the 68th and 48th percentiles.) This goes to show that AI performance on these tests doesn’t necessarily map well to human performance, since any junior law student would know not to invent cases. There’s important work to be done to apply LLMs to legal work. Meanwhile, we urge researchers who are testing LLMs’ ability to meet real-world qualifications to resist hype when reporting their results.
",['https://dl-staging-website.ghost.io/content/images/2023/06/unnamed--66-.gif']
Taught by a Bot,"While some schools resist their students’ use of chatbots, others are inviting them into the classroom.
What’s new:Some primary and secondary schools in the United States are testing an automated tutor built by online educator Khan Academy,The New York Timesreported. Users of the Khanmigo chatbot include public schools in New Jersey and private schools like Silicon Valley’s Khan Lab School (established by Khan Academy founder Sal Khan).How it works:Khanmigois based on GPT-4. Instead of providing answers outright, it responds to inquiries with questions meant to encourage critical thinking.
Behind the news:Chegg, which maintains a cadre of tutors to help students with homework, recently lost 48 percent of its market value after the company’s CEO said ChatGPT had dampened subscriber growth. Chegg plans to launch a GPT-4-based chatbot called CheggMate next year.Why it matters:Some educatorsopposeChatGPT over concerns that it enables cheating, fuels plagiarism, and spreads misinformation. Meanwhile, many studentspreferit to human tutors because it’s available around the clock, according to one survey. By offering a chatbot that leads students to an answer rather than providing it outright, Khan Academy’s approach may assuage educators’ concerns while satisfying student preferences.We’re thinking:While large language models can be used to avoid learning, there’s much more to be gained by harnessing them to accelerate and enrich it. We hope Khan Academy’s approach catches on.
",['https://dl-staging-website.ghost.io/content/images/2023/06/ezgif.com-webp-to-jpg--11-.jpg']
Training Data Free-For-All,"Amid rising questions about the fairness and legality of using publicly available information to train AI models, Japan affirmed that machine learning engineers can use any data they find.
What’s new:A Japanese officialclarifiedthat the country’s law lets AI developers train models on works that are protected by copyright.
How it works:In testimony before Japan’s House of Representatives, cabinet minister Keiko Nagaoka explained that the law allows machine learning developers to use copyrighted works whether or not the trained model would be used commercially and regardless of its intended purpose.
Yes, but:Politicians in minority parties havepressedthe ruling party to tighten the law. Visual artists and musicians have also pushed for a revision,sayingthat allowing AI to train on their works without permission threatens their creative livelihoods.
Behind the news:Japan is unusual insofar as it explicitly permits AI developers to use copyrighted materials for commercial purposes.
Why it matters:Last month, member states of the Group of Seven (G7), an informal bloc of industrialized democratic governments that includes Japan,announceda plan to craft mutually compatible regulations and standards for generative AI. Japan’s stance is at odds with that of its fellows, but that could change as the members develop a shared vision.We’re thinking:In the era of generative AI, the question of what’s fair, and thus what makes a sensible legal standard, is tricky, leading different regions in divergent directions. We applaud the G7 for moving toward globally compatible laws, which will make it easier for developers worldwide to do work that benefits people everywhere.
Gain hands-on experience with a framework for addressing complex public-health and environmental challenges in our upcoming specialization,AI for Good.Pre-enrolland get 14 days of your subscription for free!
","['https://dl-staging-website.ghost.io/content/images/2023/06/The-Batch-ads-and-exclusive-banners--33-.png', 'https://dl-staging-website.ghost.io/content/images/2023/06/GENGAMES--1-.gif']"
Game Makers Embrace Generative AI,"The next generation of video games could be filled with AI-generated text, speech, characters, and background art.What’s new:Nvidiaannounceda system that enables players to converse directly with in-game characters. Meanwhile, game developers are using generative AI to produce media assets,The New York Timesreported.
How it works:Tech companies are providing software that generates game assets either in production or on the fly. Some large game studios are developing their own tools.
Behind the news:Gamers, too, are using generative AI to modify their favorite games. For instance, modders have used voice cloning tovocalizelines for the main character of “The Elder Scrolls V: Skyrim,” who otherwise is silent.
Why it matters:Generative AI tools can streamline video game production, which is bound to appeal to developers who aim to cut both costs and timelines. More exciting, it can supercharge their ability to explore art styles, characters, dialog, and other creative features that may not be practical in a conventional production pipeline.We’re thinking:Given the high cost of media production, game development is ripe for disruption by generative AI. While we worry that some artists and writers may lose work, we expect that automating production will also create jobs. Big players are already using the technology to build more elaborate virtual worlds, and many smaller studios will benefit from lower production costs.
",['https://dl-staging-website.ghost.io/content/images/2023/06/PAELLA--1-.gif']
"Bengio, Too, Anxious About AI Risks","Another prominent AI pioneer expressed regret over his life’s work amid rising concerns over the technology’s risks.
What’s new:Yoshua Bengio, a professor at the Université de Montréal who laid parts of the foundation for deep learning, followed fellow trailblazer Geoffrey Hinton in airing his anxiety publicly. HetoldBBCthat AI’s potential for misuse left him feeling “lost” and questioning the value of his life’s work.New worries:Bengio said he was afraid that “bad actors” could use AI to cause harm, for instance by developingchemical weapons. In particular, he cited militaries, terrorists, or individuals with personal vendettas.
Behind the news:Bengio is one of the most cited computer scientists in the world. He, Hinton, and Yann LeCun shared the prestigious Turing Award in 2018 for their foundational work in deep learning. His accomplishments include helping to introduce an early attention mechanism for natural language processing and develop the generative adversarial network architecture. In a commentary he wrote forThe Batch, he looked forward toneural nets that can reason.
Why it matters:The recent pace of progress in AI has startled even researchers who have spent decades improving the technology, and its potential for harm has taken many by surprise. While there is little doubt that AI poses hazards, debate runs hot around which are most pressing and how to address them. (For instance, Yann LeCun, the third winner of the shared Turing Award, hasdownplayedsome of Bengio’s concerns.) Recognizing the most serious problems is the first step toward devising effective solutions.
We’re thinking:As AI builders, we have an ethical responsibility to minimize the harms our work might bring, even as we work to maximize the benefits. We wish Yoshua Bengio great fulfillment in the next phase of his stellar career.
",['https://dl-staging-website.ghost.io/content/images/2023/06/ezgif.com-webp-to-jpg--9-.jpg']
Falcon Ascends,"A team in the United Arab Emirates, a seven-state federation on the Arabian Peninsula, built the latest top-performing open source large language model.
What’s new:The UAE’sFalconedged out Meta’s LLaMA in the Hugging FaceOpen LLM Leaderboardto take the top spot. It’s available via the Apache 2.0 license, which is free for commercial applications. You can try ithere.
How it works:Developed by Abu Dhabi’s Technology Innovation Institute (TII), Falcon is a pretrained model based on transformers. A paper is forthcoming.
Behind the news:Open source licenses, particularly those that are free for commercial use, are enabling independent teams to build systems that are competitive with those produced by big tech companies. A recently leaked Googlememowent so far as to call open source development a threat to the company’s business.
Why it matters:Amid a surge in open source language models, Falcon offers higher performance (on the four benchmarks tracked by Hugging Face) and lower training cost relative to its chief rival,LLaMA. The fact that it was built by a team in Abu Dhabi highlights the fact that AI talent is everywhere and elite skills are spreading to all parts of the globe.
We’re thinking:AI development is a global enterprise. It gives us hope that people around the world can come together to meet other common challenges.
Are you ready to use AI in projects that can have a positive impact on public health, climate change, and disaster management?Pre-enroll nowand get 14 days of your subscription for free!
","['https://dl-staging-website.ghost.io/content/images/2023/06/The-Batch-ads-and-exclusive-banners--30-.png', 'https://dl-staging-website.ghost.io/content/images/2023/06/ezgif.com-gif-maker--1-.gif']"
LAION Roars,"The largest dataset for training text-to-image generators was assembled by volunteers for roughly $10,000. Now it’s implicated in fights over whether copyrighted works can be used for training.What’s new:Christoph Schuhmann, a German high school teacher who helped found the Large-scale Artificial Intelligence Open Network (LAION), toldBloomberghow a cadre of outsiders came together to ensure that large tech companies aren’t the only ones with access to large quantities of training data. The nonprofit group’s datasets — notablyLAION-5B(5 billion text-image pairs) — have been used to train Stability AI’s Stable Diffusion, Google’s Imagen, and other text-to-image models.Volunteer work:Schuhmann and two co-founders met on a Discord server for AI enthusiasts. Catalyzed by the launch of OpenAI’s DALL•E in January 2021, they decided to build their own image dataset. They established a separate Discord server in March 2021, which continues to act as LAION’s nerve center.
Behind the news:Data scraped from the web is at the center of several disputes.
Why it matters:Copyright holders are questioning the ethics of using their materials to build AI models. LAION plays a major role in the controversy. On one hand, it’s a nonprofit effort run by volunteers on a shoestring budget. On the other, the datasets it curates are driving tremendous business value. Stability AI, for instance, seeks a $4 billion valuation.We’re thinking:The AI community is entering an era in which we are called upon to be more transparent in our collection and use of data. We shouldn’t take resources like LAION for granted, because we may not always have permission to use them.
",['https://dl-staging-website.ghost.io/content/images/2023/06/MATRIX--1-.gif']
Rising Calls for Regulation,"Amid growing worries about AI’s power, tech leaders and politicians alike are arguing for regulating the technology.
What’s new:Leaders of OpenAI, Microsoft, and Google spoke publicly in favor of regulation and met privately with world leaders. Meanwhile, national governments proposed new guardrails for generative AI.
Execs rally:Corporate leaders hit the road to spread words of caution.
Regulators respond:Several nations took major steps toward regulating AI.
Behind the news:China is the only major world power that explicitlyregulatesgenerative AI. In March, EU officials rewrote the union’s AI Act, which has not yet been enacted, to classify generative AI models as “high-risk,” which would make them subject to bureaucratic oversight and regular audits.Why it matters:As generative AI’s capabilities grow, so do worries about its potential pitfalls. Thoughtful regulations and mechanisms for enforcement could bring AI development and application into line with social benefit. As for businesses, well defined guidelines would help them avoid harming the public and damaging their reputations and head off legal restrictions that would block their access to customers.We’re thinking:Testifying before the U.S. Congress, Sam Altman recommended that startups be regulated more lightly than established companies. Kudos to him for taking that position. The smaller reach of startups means less risk of harm, and hopefully they will grow into incumbents subject to more stringent regulation.
",['https://dl-staging-website.ghost.io/content/images/2023/05/GRIMES--1-.gif']
Pop Star Invites AI Imitation,"A popular musician is inviting fans to clone her voice. Result: a flood of recordings that sound just like her.What’s new:Experimental pop star Grimes released GrimesAI-1, a generative audio tool that allows anyone to make recordings of their own singing or speech sound like her voice. As of May 24, users had generated more than 15,000 cloned vocal tracks and submitted more than 300 fully produced songs to streaming services,The New York Timesreported.
How it works: GrimesAI-1 is available onelf.tech, a website built by Grimes and artist-management companyCreateSafe.
Behind the news:Generative audio tools like Murf.ai and Respeecher arefuelinga surge of cloned songs in the styles of popular artists. In April, Universal Music Group, one of the world’s largest owners of music rights, asked streaming services including YouTube and Spotify totake downAI-generated songs.Why it matters:Some voice actors license their voices for use in AI-generated likenesses. Grimes has gone one step further, giving her fans the tools and terms they need to mimic her voice — and perhaps even make money.We’re thinking:While major players in the music industry aim to shut off the spigot of generated music, Grimes is collaborating with her fans. That sounds like a more productive and democratic response.
Three new courses on generative AI are live. Take them for free for a limited time!Sign up today
","['https://dl-staging-website.ghost.io/content/images/2023/05/New-Courses-Batch.png', 'https://dl-staging-website.ghost.io/content/images/2023/05/EVOLV--1-.gif']"
"Scanner Sees Guns, Misses Knives","An automated security-screening system failed to detect a weapon that went on to be used in an attack.What’s new:Administrators at Proctor High School in Utica, New York, decommissioned an AI-powered weapon detector by Evolv Technologies after a student snuck a knife into the school,BBCreported. The school installed the system in 2022 for $3.7 million.
How it works:Evolv’s system uses ultra low-frequency radio sensors mounted in pillars to scan visitors at a building’s entrance. The AI model was trained on roughly 50,000 scans to classify objects including guns, knives, and bombs. The system can screen 3,600 people per hour, purportedly 10 times the rate of a walk-through metal detector. The company’s customers include museums, theme parks, stadiums, and schools.
The incident:On October 31, 2022, a student carried a hunting knife through Evolv’s scanner. Later that day, the student attacked a fellow student, who sustained serious stab wounds.
Why it matters:Although no AI system can be expected to function perfectly all the time, systems that perform critical tasks like detecting weapons must meet a very high bar. The manufacturer has a responsibility to perform rigorous tests of the system’s effectiveness and distribute the results to prospective and actual customers.We’re thinking:Our hearts go out to the community and family of the student who was injured. We hope that such systems will improve, and beyond that, we hope society evolves to a point where screening for weapons is unnecessary. It’s a travesty that children in the U.S., unlike most countries, live in fear of a violent attack on their schools. $3.7 million could go a long way toward paying for books, equipment, and teacher salaries.
",['https://dl-staging-website.ghost.io/content/images/2023/05/PIX2PIX--1-.gif']
Algorithm Investigators,"A new regulatory body created by the European Union promises to peer inside the black boxes that drive social media recommendations.
What’s new:TheEuropean Centre for Algorithmic Transparency(ECAT) will study the algorithms that identify, categorize, and rank information on social media sites and search engines.
How it works:ECATis empowered to determine whether algorithms (AI and otherwise) comply with the European Union’sDigital Services Act, which aims to block online hate speech, certain types of targeted ads, and other objectionable content. The agency, which is not yet fully staffed, will have between 30 to 40 employees including specialist AI researchers. Its tasks fall into three major categories:
Behind the news:EU regulators are increasingly targeting AI. On April 13, the European Data Protection Boardlauncheda task force to coordinate investigations by several nations into whether OpenAIviolatedprivacy laws when it trained ChatGPT. Since 2021, EU lawmakers have beencraftingthe AI Act, a set of rules designed to regulate automated systems according to their potential for harm. The AI Act is expected to pass into law later this year.
Why it matters:The EU is on the leading edge of regulating AI. As with many national-level efforts, Europe’s investigations into social media algorithms could reduce harms and promote social well-being well beyond the union’s borders.We’re thinking:This is a welcome step. Governments need to understand technology before they can craft thoughtful regulations to manage it. ECAT looks like a strong move in that direction.
",['https://dl-staging-website.ghost.io/content/images/2023/05/JPMORGAN--1-.png']
Crystal Ball for Interest Rates,"One of the world’s largest investment banks built a large language model to map cryptic government statements to future government actions.What’s new:JPMorgan Chase trained a model based on ChatGPT to score statements by a United States financial regulator according to whether it plans to raise or lower interest rates,Bloombergreported.How it works:The U.S. Federal Reserve, a government agency that’s empowered to set certain influential interest rates, periodically comments on the national economy. Its words are deliberately vague to prevent markets from acting in advance of formal policy decisions.
Results:The team tested the model by scoring past 25 years of Federal Reserve statements and speeches. They didn’t describe the results in detail but said they found a general correlation between the predicted and actual interest rate fluctuations.
Behind the news:Prior to the advent of large language models, investors tried to predict the impact of central bank announcements viasentiment analysis,timingthe interval between official meetings and publication of minutes, andwatchingthe sizes of their briefcases.
Why it matters:Central banks use interest rates to steer their country’s economies. Lower rates spur economic growth and fight recessions by making money cheaper to borrow. Higher interest rates tamp down inflation by making borrowing more expensive. If you can predict such changes accurately, you stand to reap huge profits by using your predictions to guide investments.
We’re thinking:Custom models built by teams outside the tech sector are gaining steam. Bloomberg itself — which makes most of its money providing financial data —traineda BLOOM-style model on its corpus and found that it performed financial tasks significantly better than a general-purpose model.
Join us for a live workshop on Wednesday, May 31, 2023 at 10:00 a.m. Pacific Time, and discover how customized fine-tuning techniques can help you harness pretrained language models to build robust AI applications.Register now
","['https://dl-staging-website.ghost.io/content/images/2023/05/The-Batch-ads-and-exclusive-banners--23-.png', 'https://dl-staging-website.ghost.io/content/images/2023/05/ARCHITECTURE_600px--1-.gif']"
Architect’s Sketchbook,"Text-to-image generators are visualizing the next wave of architectural innovation.
What’s new:Patrick Schumacher, principal architect at Zaha Hadid Architects,explainedhow the company uses generative AI to come up with ideas. He made the remarks at an industry roundtable called AI and the Future of Design.
How it works:The architects use DALL•E 2, Midjourney, and Stable Diffusion to generate exterior and interior images of concepts in development. Schumacher showed generated images for projects in development, including a high-rise complex in Hong Kong and Neom, a massive smart city planned for Saudi Arabia.
Behind the news:Text-to-image models are finding their way into a variety of design disciplines.
Why it matters:Zaha Hadid Architects has worked on Olympic venues, international airport terminals, and skyscrapers. Millions of people soon may interact with buildings visualized by AI.We’re thinking:What a great example of human-computer collaboration: The models learn from the architects’ past designs to help the them envision fresh concepts.
",['https://dl-staging-website.ghost.io/content/images/2023/05/MICRO--1-.gif']
Google Adds AI Inside and Out,"Google showcased a flood of new features in its latest bid to get ahead in the generative AI arms race.
What’s new:The companydemonstratedAI features for consumers and developers at its annual I/O conference.
PaLM powered:More than two dozen of the new features, including Bard and Duet AI (see below), are powered by a new large language model calledPaLM 2. Google trained PaLM 2 on tasks similar to Google'sUL2pretraining framework more than 100 different natural languages and numerous programming languages. It will be available as a cloud service in four unspecified sizes.
App assistance:Duet AIis a suite of text generation tools for Google Workspace and Cloud.
New foundation models:Vertex offers three new foundation models.Chirpfor speech-to-text, Codey for code completion, andImagenfor text-to-image generation. Users can join awaitlistvia Vertex.
Bard handles images:Users no longer have to join a waitlist for access to theBardchatbot, and its language capabilities have been expanded from English to include Japanese and Korean. It is now available in 180 countries, though not the EU or Canada. Bard can now respond to image-based queries, provide images in its responses, and generate custom images using Adobe’s image generation model,Firefly.
Search enhancements:An experimental version of Google Search will generate text answers to queries using an unidentified language model.
Why it matters:Google’s new capabilities are the latest salvo in anongoing competitionto capture generative AI’s market potential to greatest effect.
We’re thinking:Just days ago, a leaked Googlememotalked about Google and OpenAI’s lack of moat when it comes to LLM technology. It described how open source offerings of LLMs are racing ahead, making it challenging for any company to maintain a significant and enduring lead over competitors in the quality of its models. We think the impressive I/O presentation by Sundar Pichai and team, however, reminded everyone of Google’s tremendous distribution advantages. Google owns many platforms/products (such as search, Gmail, Android, Chrome and Youtube) with over 2 billion users, and this gives it numerous ways to get generative AI to users. In the era of generative AI, we are increasingly seeing distribution as a moat for businesses.
",['https://dl-staging-website.ghost.io/content/images/2023/05/OPINION--1-.gif']
The Politics of Language Models,"Do language models have their own opinions about politically charged issues? Yes — and they probably don’t match yours.
What's new: Shibani Santurkar and colleagues at Stanfordcomparedopinion-poll responses of large language models with those of various human groups.
How it works: The authors collected multiple-choice questions based on surveys of public opinion in the United States. They compared answers generated by nine language models (three from AI21 Labs and six from OpenAI) with those of 60 demographic groups. The groups varied according to sex, age, race, geography, relationship status, citizenship status, education, political party affiliation, religious affiliation, and degree of religious observance.
Results: The authors compared the distributions of model and human answers according to a formula based on theWassersteinscore, also known as earth mover’s distance. In their formula, 1 is a perfect match.
Behind the news:In some circles, ChatGPT has beencriticizedfor expressing a political bias toward liberal (in U.S. terms) positions. Such allegations have prompted developers to buildalternativeversionsthat are deliberately biased in other directions. Some observers speculate that Elon Musk’s secretive AIstartupis on a similar mission.
Why it matters: Large language models aren’t neutral reflections of society. They express political views that don’t match those of the general population or those of any group. Furthermore, prompting them to take on a particular group’s viewpoint doesn't bring them into line with that group. The AI community (and the world at large) must decide whether and how to manage these biases.
We're thinking: Should a language model’s opinions match those of the global average, or should different language models respond similarly to different groups? Given that a subset of the world’s population holds biased opinions, including sexist or racist views, should we build LLMs that reflect them? Should language models be allowed to express opinions at all? Much work lies ahead to make these choices and figure out how to implement them.
Identify your organization's generative AI capabilities, skill gaps, and training needs with the world's first generative AI skill assessment, from Workera.Join the beta now!
","['https://dl-staging-website.ghost.io/content/images/2023/05/The-Batch-ads-and-exclusive-banners--12---1-.jpg', 'https://dl-staging-website.ghost.io/content/images/2023/05/OVEREMPLOYEDS--1-.png']"
AutomatedIntoa Job,"ChatGPT is helping some workers secretly hold multiple full-time jobs at once.
What’s new:Workers are using OpenAI’s chatbot to boost their productivity so they can earn separate paychecks from a number of employers, each of whom believes they are exclusive employees,Vicereported.What they said:Several of these so-called “overemployed” people stated that, although their jobs require a degree of human expertise, ChatGPT enables them to accomplish more in less time. They spoke anonymously to avoid revealing the ruse.
Behind the news:A March 2023 paper by two MIT economistsreportedthat writers who used ChatGPT were 37 percent faster than those who did not.
Why it matters:This practice illustrates the real productivity gains conferred by large language models. Moreover, in a typical corporate environment, managers decide which tools workers will use and how. The “overemployed” community turns that practice on its head, using AI to boost productivity from the bottom up.
We’re thinking:It's discouraging to see people using AI to deceive employers who could benefit from the productivity gains. Beyond the ethical problems, the use of generative AI without informing employers could lead to legal questions in areas like ownership of intellectual property. Yes, let’s use these tools to be more productive, but let’s do it in honest and ethical ways.
",['https://dl-staging-website.ghost.io/content/images/2023/05/DREAMFUSIONv3--1-.gif']
Battlefield Chat,"Large language models may soon help military analysts and commanders make decisions on the battlefield.What’s new:Palantir, a data-analytics company that serves customers in the military, intelligence, and law enforcement,demonstratedits chat-drivenArtificial Intelligence Platform(AIP) performing tasks like identifying enemies in satellite imagery, deploying surveillance drones, and proposing battle plans.How it works:In the demonstration, an intelligence analyst uses AIP to react to a fictional scenario. The system integrates large language models includingDolly-v2-12b(12 billion parameters),Flan-T5XL(3 billion), andGPT-NeoX-20B(20 billion) fine-tuned on an unspecified dataset.
Behind the news:Military forces are experimenting with AI for executing combat tactics.
Why it matters:At its best, this system could help military authorities identify threats sooner and streamline their responses, enabling them to outmaneuver their enemies. On the other hand, it represents a significant step toward automated warfare.We’re thinking:This system takes the critical question of safety in AI systems to a new, terrifying level. Human battlefield analysts manage complex variables: terrain, weather, local customs, capabilities and limitations of friendly and enemy forces. This is crucial work. Delegating that work to a chatbot is a worrisome prospect considering the current state of large language models, which hallucinate falsehoods, confidently provide unworkable directions, and fail at basic math — especially smaller chatbots, like those used in this system.
",['https://dl-staging-website.ghost.io/content/images/2023/05/unnamed--17-.jpg']
OpenAI Gears Up for Business,"Reporters offered a behind-the-scenes look at OpenAI’s year-long effort to capitalize on its long-awaited GPT-4.
What’s new:The company built a sales team and courted corporate partners in advance of launching its latest large language model,The Informationreported.How it works:OpenAI hired a head of sales only last June, four years after shifting from nonprofit to for-profit. She and her team began signing up corporate customers soon after.
Path to profit:In 2015, OpenAI started as a nonprofit research lab dedicated to transparency. In 2019, it launched a profit-seeking subsidiary to fund its research. In a series of deals between 2019 and 2023, Microsoft invested upward of $13 billion in exchange for 49 percent of OpenAI’s profit and right of first refusal to commercialize its technology.
Yes, but:Observers have criticized both the company’spivotto profit and itsshiftaway from transparency. In a March interview, OpenAI’s co-founder Ilya Sutskeverdefendedthe organization’s secrecy, claiming it was necessary for safety as AI becomes more powerful.
Why it matters:OpenAI saw generative AI’s commercial potential before ChatGPT sparked investments around the globe. That foresight could pay off handsomely, as the companyforecastedrevenue of $200 million this year and $1 billion by 2024.We’re thinking:OpenAI is building revolutionary technology that benefits hundreds of millions of users. We’re glad to see it on a path to financial sustainability.
Are you ready to leverage AI for projects that can make a positive impact on public health, climate change, and disaster management?Pre-enroll in AI for Good and learn how!
","['https://dl-staging-website.ghost.io/content/images/2023/05/The-Batch-ads-and-exclusive-banners--17-.png', 'https://dl-staging-website.ghost.io/content/images/2023/05/Untitled-design--3--1.gif']"
Language Models in Lab Coats,"Specialized chatbots are providing answers to scientific questions.
What’s new:A new breed of search engines including Consensus, Elicit, and Scite use large language models to enable scientific researchers to find and summarize significant publications,Naturereported.
How it works:The models answer text questions by retrieving information from databases of peer-reviewed scientific research.
Yes, but:These tools may struggle with sensitive or fast-moving fields. For example, in response to the question, “Do vaccines cause autism?”, pediatrician Meghan Azad at the University of Manitoba found that Consensus returned a paper that focused on public opinion rather than scientific research. Clémentine Fourrier, who evaluates language models at HuggingFace, said that searching for machine learning papers via Elicit often brought up obsolete results.
Why it matters:Search engines that rank and summarize relevant research can save untold hours for scientists, students, and seekers of knowledge in general. With continued improvement, they stand to accelerate the pace of progress.We’re thinking:These systems show promise and point in an exciting direction. When search was young, search engines that covered the web (like Google) competed with vertical search engines that covered niches such as retail (Amazon) or travel (Expedia). A similar competition is shaping up between general-purpose chatbots and vertical chatbots.
",['https://dl-staging-website.ghost.io/content/images/2023/05/unnamed--63-.gif']
Hinton Leaves Google With Regrets,"A pioneer of deep learning joined the chorus of AI insiders who worry that the technology is becoming dangerous, saying that part of him regrets his life’s work.
What’s new:Geoffrey Hinton, who has contributed to groundbreaking work on neural networks since the 1980s, stepped down from his role as a vice president and engineering fellow at Google so he could voice personal concerns about AI’s threat to society,The New York Timesreported. He believes that Google has acted responsibly in its AI development, he added in a subsequenttweet.
Why he stepped down:AI models have improved faster than Hinton had expected, and the generative AI gold rush led him to believe that the financial rewards of innovating would overwhelm incentives to rein in negative effects. In addition, at 75, he has become “too old to do technical work,” he toldMIT Technology Review. Instead, he will focus on philosophical matters. Among his concerns:
Behind the news:Hinton’s contributions to deep learning are myriad. Most notably, he helped popularize the use of backpropagation, the core algorithm for training neural networks; invented the dropout technique to avoid overfitting; and led development of AlexNet, whichrevolutionizedimage classification. In 2018, hereceivedthe Turing Award alongside Yann LeCun and Yoshua Bengio for contributions to AI.
Why it matters:Hinton’s thoughts about AI risks are exceptionally well informed. His concerns sound a note of caution for AI practitioners to evaluate the ethical dimensions of their work and stand by their principles.
We’re thinking:Geoffrey Hinton first joined Google as a summer intern (!) at Google Brain when Andrew led that team. His departure marks the end of an era. We look forward to the next phase of his career.
",['https://dl-staging-website.ghost.io/content/images/2023/05/unnamed--58-.gif']
U.S. Politics Go Generative,"A major political party in the United States used generated imagery in a campaign ad.What’s new:The Republican Partyreleaseda video entirely made up of AI-generated images. The production, which attacks incumbent U.S. president Joe Biden — who leads the rival Democratic Party — marks the arrival of image generation in mainstream U.S. politics.Fake news:The ad depicts hypothetical events that purportedly might occur if Biden were to win re-election in 2024. Voice actors read fictional news reports behind a parade of images that depict a military strike on Taipei due to worsening relations between the U.S. and China, boarded-up windows caused by economic collapse, a flood of immigrants crossing the southern border, and armed soldiers occupying San Francisco amid a spike in crime.
Behind the news:Generative AI previously infiltrated politics in other parts of the world.
Why it matters:Political campaigns are on the lookout for ways to get more bang for their buck, and using text-to-image generators may be irresistible. In this case, the producers used fake — but realistic — imagery to stand in for reality. Despite the small-type disclaimer, the images make a visceral impression that fictional events are real, subverting the electorate's reliance on an accurate view of reality to decide which candidates to support. The power of such propaganda is likely to grow as generative video improves.We’re thinking:This use of generated images as propaganda isn’t limited to political jockeying. Amnesty International recently tweeted — and sensibly deleted — a stirring image of a protester detained by Colombian police bearing the fine print, “Illustrations produced by artificial intelligence.” Organizations that seek to inform their audiences about real-world conditions counteract their own interests when they illustrate those conditions using fake images.
Learn new use cases for large language models and improve your ChatGPT API skills in our one-hour course, “ChatGPT Prompt Engineering for Developers.”Sign up for free
","['https://dl-staging-website.ghost.io/content/images/2023/05/The-Batch-ads-and-exclusive-banners--11-.jpg', 'https://dl-staging-website.ghost.io/content/images/2023/05/unnamed--59-.gif']"
Radio Stations Test AI DJs,"A language model will stand in for radio disk jockeys.What’s new:RadioGPTgeneratesradio shows tailored for local markets. The system, which is undergoing tests across North America, is a product of Futuri, a company based in Cleveland, Ohio, that focuses on digital audience engagement.How it works:Futuri’s proprietaryTopic Pulsesystem determines trending topics in a radio station’s local market, and OpenAI’s GPT-4 generates a script. An unspecified model vocalizes the script using between one and three voices. Customers can choose a preset voice or clone their own.
Behind the news:Fully automated media programs are gaining momentum as AI models make it easy to produce endless amounts of text and audio.
Why it matters:Many radio stations already are highly automated and rely for news on syndicated programming. AI-generated DJs that localize news and listener interactions can give them programming customized to their markets and may help them compete with streaming services.We’re thinking:RadioGPT fits generative AI into a traditional radio workflow. Ultimately, we suspect, this tech will remake the medium in more fundamental ways.
",['https://dl-staging-website.ghost.io/content/images/2023/05/unnamed--60-.gif']
Data Does Not Want to Be Free,"Developers of language models will have to pay for access to troves of text data that they previously got for free.
What’s new:The discussion platformRedditand question-and-answer siteStack Overflowannounced plans to protect their data from being used to train large language models.
How it works:Both sites offer APIs that enable developers to scrape data, like posts and conversations, en masse. Soon they'll charge for access.
What they’re saying:“Community platforms that fuel LLMs absolutely should be compensated for their contributions so that companies like us can reinvest back into our communities to continue to make them thrive,” Chandrasekar toldWired.
Behind the news:In February, Twitter started charging up to$42,000monthly for use of its API. That and subsequent API closures are part of a gathering backlash against the AI community’s longstanding practice of training models on data scraped from the web. This use is at issue in ongoinglawsuits. Last week a collective of major news publishersstatedthat training AI on text licensed from them violates their intellectual property rights.
Why it matters:Although data has always come at a cost, the price of some corpora is on the rise. Discussion sites like Reddit are important repositories of conversation, and text from Stack Overflow has been instrumental in helping to train language models to write computer code. The legal status of existing datasets and models is undetermined, and future access to data depends on legal and commercial agreements that have yet to be negotiated.We’re thinking:It’s understandable that companies watching the generative AI explosion want a slice of the pie and worry that users might leave them for a chatbot trained on data scraped from their own sites. Still, we suspect that charging for data will put smaller groups with fewer resources at a disadvantage, further concentrating power among a handful of wealthy companies.
",['https://dl-staging-website.ghost.io/content/images/2023/04/unnamed--54-.gif']
"Conversational Search, Google Style","Google’s response to Microsoft’s GPT-4-enhanced Bing became a little clearer.
What’s new:Anonymous insiders leaked details of Project Magi, the search giant’s near-term effort to enhance its search engine with automated conversation,The New York Timesreported. They described upcoming features, but not the models behind them.
How it works:Nearly 160 engineers are working on the project.
Beyond search:The company is developing AI-powered features for other parts of its business as well. These include an image generation tool called GIFI for Google Images and a chatbot called Tivoli Tutor for learning languages.Behind the news:Google has been scrambling to integrate AI features. The company recently combined Brain and DeepMind into a single unit to accelerate AI research and development. In March, rumors emerged that Samsung, which pays Google substantial licensing revenue to use its search engine in mobile devices, was considering a switch to Bing. The previous month, Bard made factual errors during a public demo, which contributed to an 8 percent drop in Google’s share price. These moves followed a December 2022 “code red” response to Microsoft’s plans to upgrade Bing with conversational technology from OpenAI.
Why it matters:When it comes to finding information, conversational AI is a powerful addition to, and possibly a replacement for, web search. Google, as the market leader, can’t wait to find out. The ideas Google and its competitors implement in coming months will set the mold for conversational user interfaces in search and beyond.We’re thinking:Should chatbots be integrated with search or designed as separate products? Microsoft and Google are taking different approaches. Microsoft’s conversational model is deeply integrated with Bing search, while Google's Bard currently stands alone. Given the differences between chat and search, there’s a case to be made for keeping chatbots distinct from search engines.
Are you ready to turn your passion into practice? The newAI for Good Specializationwill empower you to use machine learning and data science for positive social and environmental impact!Join the waitlist to be the first to enroll
","['https://dl-staging-website.ghost.io/content/images/2023/04/The-Batch-ads-and-exclusive-banners--9-.jpg', 'https://dl-staging-website.ghost.io/content/images/2023/04/unnamed--55-.gif']"
Everybody Must Get Cloned,"Tech-savvy music fans who are hungry for new recordings aren’t waiting for their favorite artists to make them.
What’s new:Social media networks exploded last week with AI-driven facsimiles of chart-topping musicians. A hiphop song with AI-generated vocals in the styles ofDrake and The Weekndracked up tens of millions of listens before it was taken down. Soundalikes of Britpop starsOasis, rapperEminem, and Sixties stalwartsThe Beach Boysalso captured attention.
How it works:These productions feature songs composed and performed in the old-fashioned way overlaid with celebrity-soundalike vocals generated by voice-cloning models. Some musicians revealedtheirmethods.
Behind the news:The trend toward AI emulations of established artists has been building for some time. In 2021, Lost Tapes of the 27 Club used an unspecified AI method to produce music in the style of artists who died young including Jimi Hendrix, Kurt Cobain, and Amy Winehouse. The previous year, OpenAI demonstrated Jukebox, a system that generated recordings in the style of many popular artists.
Yes, but:The record industry is moving to defend its business against such audio fakery (or tributes, depending on how you want to view them). Universal Music Group, which controls about a third of the global music market, recentlypushedstreaming services to block AI developers from scraping musical data or posting songs in the styles of established artists.
Why it matters:Every new generation of technology brings new tools to challenge the record industry’s control over music distribution. The 1970s brought audio cassettes and the ability to cheaply copy music, the 1980s brought sampling, the 1990s and 2000s brought remixes and mashups. Today AI is posing new challenges. Not everyone in the music industry is against these AI copycats: The electronic artist Grimes said she would share royalties with anyone who emulates her voice, and Oasis’ former lead singer apparently enjoyed the AI-powered imitation.We’re thinking:Musicians who embrace AI will open new creative pathways, but we have faith that traditional musicianship will endure. After all, photography didn’t kill painting. Just as photography pushed painters toward abstraction, AI may spur conventional musicians in exciting, new directions.
",['https://dl-staging-website.ghost.io/content/images/2023/04/unnamed--57-.gif']
The Music Industry Strikes Back,"The music industry fired early shots in an impending war against AI-generated music.
What’s new:Universal Music Group, which owns labels including Deutsche Grammophon, EMI, Interscope, Motown, Polydor, and Virgin, is pressing Spotify and other streaming media services to counter the threat of AI-driven copycats,Financial Timesreported.
How it works:Universal Music Group (UMG), which accounts for nearly one-third of the global music market and thus a substantial portion of revenue to distributors of digital music, is prevailing on top streaming services to protect its intellectual property.
Behind the news:Music generators like Google’sMusicLMare in their infancy but likely to improve quickly. Hugging Face recentlyaddedtwo to its offerings. Meanwhile, the question whether AI developers have a right to train their models on works under copyright — images, so far, rather than music — is central tocasesunderwayin United States courts.
Why it matters:The recording industry has significant economic and political clout, and its preferences may play a major role in determining whether AI developers can continue to train their systems on copyrighted works without permission. In the early years of the internet, recording companies helpedshut downpeer-to-peer music-sharing sites like Napster, which helped create the market for subscription streaming services like Apple Music and Spotify. The latest moves may portend a similar fight. One difference: While the copyright issues surrounding Napster were clear, they have yet to be established with respect to AI.We’re thinking:Just as the music industry came to support on-demand digital music by way of streaming services, it can create opportunities — both commercial and creative — for AI models that generate music and form partnerships with AI developers to realize them.
",['https://dl-staging-website.ghost.io/content/images/2023/04/OLYMPICS--1-.jpg']
Eyes on the Olympics,"French lawmakers said “oui” to broad uses of AI-powered surveillance.What’s new:France’s National Assembly authorized authorities to test systems that detect unlawful, dangerous, or unusual behavior at next year’s Summer Olympics in Paris,Reutersreported. The bill will become law unless the country’s top court blocks it.How it works:The bill is part of broaderlegislationthat regulates Olympic advertising, doping, and the route run by torch bearers.
Behind the news:Technology that collects biometric data would be subject to strict monitoring and reporting requirements under the current draft of the European Union’s forthcomingAI Act, which is scheduled for a vote in May. If it passes, the European Parliament, European Council, and European Commission will negotiate a final version.Yes, but:Amnesty International, Human Rights Watch, and 36 other nongovernmental organizationssigneda letter opposing the French bill. The signatories contend that analyzing the behavior of individuals in a crowd requires collecting personal biometric data, although French authorities deny it.Why it matters:France’s move is emblematic of broader tension between AI’s value in security applications and its potential for harm. If the bill clears legal hurdles, France will become the first EU country to formally legalize AI-powered surveillance.We’re thinking:AI has great potential in crowd control. Engineers working on such applications should keep in mind that computer vision systems can be compromised by fluctuations in lighting, changes in physical surroundings, and the complexities of group behavior.
Learn how to train and fine-tune large language models using the recently released PyTorch 2.0! Join us for an online workshop on Thursday, April 27, 2023 at 10:00 a.m. Pacific Time.RSVP
","['https://dl-staging-website.ghost.io/content/images/2023/04/The-Batch-ads-and-exclusive-banners--14-.png', 'https://dl-staging-website.ghost.io/content/images/2023/04/BEDROCK--1-.gif']"
AWS Joins the Generative AI Race,"Amazon joined big-tech peers Google, Meta, and Microsoft in rolling out services that provide generated text and images.
What’s new:The online retailerlaunchedearly access to Bedrock, a cloud platform that offers generative models built by Amazon and its partners.How it works:Bedrock is aimed at business customers, who can select among image- and text-generation models and fine-tune them for proprietary uses. It’s available to selected customers of Amazon Web Services as a “limited preview.” The price has yet to be announced.
Behind the news:Amazon’s peers offer similar capabilities via their respective cloud services.
Why it matters:Between Amazon and other cloud computing providers, generative AI rapidly is becoming available to developers of all kinds.We’re thinking:DALL·E 2 and ChatGPT debuted less than a year ago. Generative AI is gathering momentum at warp speed!
",['https://dl-staging-website.ghost.io/content/images/2023/04/PROMPTv2.gif']
AI Startups Face Compute Shortage,"Chatbot-fueled FOMO is overwhelming cloud-computing services.
What’s new:Cloud providers are struggling to meet sharply rising demand by a crowd of AI startups eager to cash in on generative AI,The Informationreported.Behind the bottleneck:The surge in demand caught Amazon Web Services, Microsoft Azure, and others off guard.
What they’re saying:Engineers and entrepreneurs shared their pain.
Behind the news:China is facing its own chip shortage — andfindingways to address it. That situation, though, is a result of United States trade sanctions rather than a surge in demand.
Why it matters:Startups that serve a market with generated text or pictures are white-hot, but even the most promising ventures can’t do without servers to build, test, and deploy their models. The winners will need not only a great product but also ready access to computation.We’re thinking:Our hearts go out to everyone who is trying to build AI products in these unpredictable times. We trust that the supply of compute will catch up in due course and that the current run of AI-fueled growth will continue for the foreseeable future.
",['https://dl-staging-website.ghost.io/content/images/2023/04/ITALY--1-.gif']
Italy Boots ChatGPT,"Italy blocked ChatGPT after determining that it violates European Union laws.
What’s new:The Guarantor for the Protection of Personal Datasuspendedaccess to ChatGPT for 20 days after saying that OpenAI enables underage children to use the chatbot, distributes misinformation about people, and collects personal data used to train its models without proper authority.The ruling:The Guarantor, which enforces the rules in Italy, banned ChatGPT for 20 days citing four concerns: The chatbot doesn’t prevent children under 13 from using it, the chatbot can provide inaccurate information about individuals, OpenAI did not inform individuals that the firm was collecting data that could be used to identify them, and OpenAI did not meet the EU privacy law’s guidelines for collecting personal data.
Behind the news:Privacy regulators in Europe and the United States have their eyes on AI.
Yes, but:Not everyone in the Italian government agrees with the ruling. Matteo Salvini, one of the country’s two deputy prime ministers,criticizedit as excessive.Why it matters:A national (or international) ban on ChatGPT could have major implications for large language models, which rely on sprawling datasets and routinely output misinformation. It could also harm European innovation by blocking access to the latest technology. And it’s not just Italy: French, German, and Irish regulatorsreportedlyare considering similar actions. Belgian regulators went a step further andcalledfor an EU-wide discussion of data violations related to ChatGPT.We’re thinking:Some of the regulators’ concerns may stem from a lack of transparency into how OpenAI trains its models. A more open approach might alleviate some fears.
The core of many successful companies is a strong co-founder partnership. Join us on April 13, 2023, at 2:00 p.m. Pacific Time to learn how to find your perfect co-founder.Register now
","['https://dl-staging-website.ghost.io/content/images/2023/04/The-Batch-ads-and-exclusive-banners--13-.png', 'https://dl-staging-website.ghost.io/content/images/2023/04/AIINDEX_BG2_1200px--1-.gif']"
AI Trends Tracked,"Stanford’s sixth annualAI Indextakes stock of a rapidly growing field.What’s new:The sprawling, 386-pagereportfrom the Institute for Human-Centered AI presents the past year’s developments in AI based on a wide variety of sources including benchmarks, papers, market research, job listings, and polls. (You can find info about earlier editionshere.)Reasons for celebration:The report highlights several positive trends:
Causes for concern:Not everything in the report is rosy:
Behind the news:The new report surveyed the AI’s recent past, but other measures indicate the near-term future. An April study by investment bank Goldman Sachsfoundthat generative AI could boost the global productivity by 7 percent in the coming decade as it automates tasks that affect 300 million full-time jobs. Meanwhile, at the startup incubator Y Combinator, AI is at the heart of 34 percent of newly formed companies — the highest number on record.
Why it matters:TheAI Indexoffers a sober yet exciting summary of AI’s march into all areas of society. Immense opportunities and grave challenges alike lie ahead.
We’re thinking:Focusing on 2022, this report doesn’t reflect the staggering impact of generative AI — a reminder of the extraordinary pace of development as well as AI’s potential in areas well beyond the current buzz.
",['https://dl-staging-website.ghost.io/content/images/2023/04/ezgif.com-optimize--11--1.gif']
AI Shows Its Metal,"Neural networks are predicting how metal will deform under pressure to pilot robots through the tricky process of fabricating aircraft.
What’s new:Machina Labs crafts metal using AI-guided robotic arms,Bloombergreported. The company recently inked contracts with the United States Air Force, the U.S. National Aeronautics and Space Administration, and Hermeus, which makes hypersonic airplanes.
How it works:Thesystemcombines robot arms, sensors, and machine learning models to form, trim, finish, and polish metal sheets according to a computer-aided design. Working in pairs, robot arms on either side of a sheet apply pressure to sculpt deformations up to four feet deep. The system works on aluminum, steel, and titanium in varying thicknesses and sizes upward of 4 feet by 12 feet. A basic two-arm setup costs $2.5 million.
Behind the news:Most sheet-metal manufacturing isperformedmanually by skilled workers. Some parts can be mass-produced, but manual labor is still required to build molds. Both processes are slow, laborious, and expensive — a problem exacerbated by ashortageof craftspeople.Why it matters:Large machines like airplanes and automobiles are expensive to manufacture. Robots guided by deep learning models can bring costs down by fabricating parts quickly and precisely and by recognizing defects before they leave the factory.We’re thinking:This application of deep learning is riveting.
",['https://dl-staging-website.ghost.io/content/images/2023/04/RAISE_600px--1-.gif']
Better Pay for Data Workers,"Contract workers who help train the algorithms behind Google Search won a pay raise.What’s new:Employees of U.S. contractors who evaluate the quality of Google Search’s results, knowledge panels, and ads will earn $15 per hour, a raise of roughly $1,Bloombergreported.
Pay raise:The Alphabet Workers Union (AWU), an unofficial labor union that represents U.S.- and Canada-based employees of Alphabet, its subsidiaries, vendors and contractors, negotiated the raise. The deal will affect around 5,000 workers, most of whom work remotely for Seattle-area RaterLabs.
Behind the news:Large AI developers like Google and OpenAI often outsource rote tasks like labeling data and evaluating outputs. The contractors have come under fire for underpaying workers.
Why it matters:AI products like search engines, language models, and autonomous vehicles can earn billions for the companies that develop them. Yet many of the workers who contribute to them receive relatively low wages.
We’re thinking:We’re glad to see wages rising for workers whose input is crucial to building AI systems. For a thoughtful treatment of tech labor issues, we recommend Gray and Suri’s excellent book,Ghost Work: How to Stop Silicon Valley from Building a New Global Underclass.
Special event! Join Yann LeCun and Andrew Ng on Friday, April 7, 2023, at 9:30 a.m. Pacific Time to discuss a proposed pause in cutting-edge AI research. Let’s examine the pros and cons of the Future of Life Institute’s proposal!Register here
","['https://dl-staging-website.ghost.io/content/images/2023/04/The-Batch-ads-and-exclusive-banners--7-.jpg', 'https://dl-staging-website.ghost.io/content/images/2023/04/LELAPA--1-.gif']"
Repatriating Talent,"A South African startup aims to lure talented engineers who left the continent to work abroad.
What’s new:Johannesburg research labLelapa.aibills itself as a haven for African AI engineers who want to work on challenges that aren’t on Silicon Valley’s agenda,Wiredreported. The company purports to focus on languages such as isiZulu that big-tech natural language models don’t accommodate.How it works:Lelapa develops AI models for other businesses and nonprofits. The company has raised $2.5 million from institutions including Mozilla Ventures, Africa-centric investor Atlantica Ventures, and private investors including Google AI chief Jeff Dean. Current projects include:
Behind the news:Lelapa’s founders include some organizers ofDeep Learning Indaba, a machine learning conference most recently held in Tunisia, andMasakhane, a nonprofit that promotes open-source models and datasets for African languages. Co-founderJade Abbottwas profiled in DeepLearning.AI’s Working AI blog series.
Why it matters:Over 74 percent of foreign-born students who receive a PhD in AI from a school in the United States remain in the U.S. after graduating, last year’s State of AI reportfound. Lelapa’s founders hope their project will help Africa reclaim some of this talent, nurture native AI startups, and address systemic inequities in AI development.
We’re thinking:Sub-Saharan Africaaccountsfor 15 percent of the world’s population but fewer than 1 percent of AI patents and conference publications, according to the State of AI report. Organizations like Lelapa can help the region realize its potential.
",['https://dl-staging-website.ghost.io/content/images/2023/04/ezgif.com-gif-maker--26--1.gif']
Restricted Chips Slip Through,"Chinese companies have found loopholes to sidestep United States limits on AI chips.
What’s new:Facing severe limits on U.S. exports of high-performance chips, Chinese AI firms are purchasing them through subsidiaries and using them through cloud services, theFinancial Timesreported.
Restrictions:In October 2022, U.S. officialsblockedU.S. companies, citizens, permanent residents, and their foreign trading partners from selling chips with high processing and interconnect speeds — primarily Nvidia’s flagship A100 — to Chinese customers. The ban also prohibits sales to China of equipment and software used in semiconductor manufacturing. Japan and the Netherlandsimposedsimilar restrictions in January.
Loopholes:Prior to the restrictions, rumors that they were coming gave companies an opportunity to stockpile chips ahead of time. The rules don’t specifically prohibit Chinese customers from using cloud-computing services, which opened a path to use the banned chips, and shell companies headquartered in other countries provide another avenue. Meanwhile, the U.S. government previously had barred some companies from buying high-tech equipment; these firms already had developed alternative sources of sensitive technology.
Behind the news:China responded to the embargo by investing in its own chip industry. In December 2022, Beijingannouncedthat it would pump $143 billion into domestic semiconductor production. In early 2023, however, officialsslowedits investment in response to a resurgence of Covid-19.Why it matters:U.S. efforts to restrict advanced chips come at a time of rapidprogressin AI as well as increasing fears of geopolitical instability. The lack of homegrown alternatives creates a powerful incentive for Chinese companies to find ways around the restrictions.We’re thinking:This isn’t the end of the story. U.S. officials likely will respond by tightening the laws around cloud computing, and Chinese companies will react by finding new workarounds.
",['https://dl-staging-website.ghost.io/content/images/2023/03/unnamed--30-.png']
Algorithm Whisperers,"Looking for work in AI? Brush up on your language skills.
What’s new:Employers are hiring prompt engineers to write natural-language prompts for AI models,The Washington Postreported. They includeAnthropic,Boston Children’s Hospital, and the London law firmMischon de Reya.
How they work:The report illuminates a few tricks of the trade.
What they’re saying:“The hottest new programming language is English,” Andrej Karpathy, the former Tesla Senior Director of AI who now works at OpenAI,tweeted.
Behind the news:Bloggers and social media users documented early experiments in prompt engineering, such as using analogies to teach GPT-3 how to invent its ownfantasy worldsand constructive feedback to prod GPT-3 into performingarithmetic. Researchers have also explored the practice. For example, a 2022 paperidentifiedsix classes of modifiers for image-generation prompts.
Yes, but:Prompt engineering can’t produce reliable results due to the black-box nature of generative AI models based on neural networks, said Shane Steinert-Threlkeld, a linguist who studies natural language processing. To wit: A 2021 studyfoundthat some prompt instructions that contained nonsense phrases were as effective as those that were worded with care.Why it matters:Text- and image-generation models have fueled a rush of investment. The professionalization of prompt engineering followed as companies began to harness the technology.
We’re thinking:New technology often creates new professions that fizzle out as things advance. For instance, early elevators required human operators until automation made that profession obsolete. Prompt engineersmay experience the same fateas generative AI models continue to advance and become easier to direct. Professionals who are banking on this job title can hedge their bets by learning to code, tune algorithms, and implement models.
Jobs for computer researchers are expected togrowby more than 20 percent in the next decade! Now is the perfect time to take the next step in your AI career with theDeep Learning Specialization.Learn more
","['https://dl-staging-website.ghost.io/content/images/2023/03/The-Batch-ads-and-exclusive-banners--12-.png', 'https://dl-staging-website.ghost.io/content/images/2023/03/unnamed--50-.gif']"
What Americans Want From AI,"Adults in the United States tend to view AI’s medical applications favorably but are leery of text and image generation.
What’s new:Pew Research Centerpolled11,004 U.S. adults for their opinions of AI in science, healthcare, and media.What they said:The pollsters asked respondents how much they had read or heard about nine AI applications and whether they considered these developments to be advances. The results reflect responses as of December 2022.
Behind the news:A January 2023surveyby Monmouth University corroborates some of Pew’s findings. 35 percent of that poll’s 805 respondents had heard a lot about recent AI developments. 72 percent believed that news outlets would eventually publish AI-penned news articles. 78 percent thought this would be a bad thing.
Why it matters:As AI matures, it becomes more important to take the public’s temperature on various applications. The resulting insights can guide developers in building products that are likely to meet with public approval.
We’re thinking:The respondents’ familiarity with a given application did not correlate with their acceptance of it. While we should be responsive to what people want, part of our job is to show people the way to a future they may not yet envision — all the more reason for AI builders tofollow your interestsrather than the latest AI fads.
",['https://dl-staging-website.ghost.io/content/images/2023/03/unnamed--51-.gif']
Microsoft Cuts Ethics Squad,"Microsoft laid off an AI ethics team while charging ahead on products powered by OpenAI.What’s new:On March 6, the tech giant dissolved the Ethics & Society unit in its Cognition group, which researches and builds AI services, amid ongoing cutbacks that have affected 10,000 workers to date, the tech-news outletPlatformerreported. Microsoft kept its Office of Responsible AI, which formulates ethical rules and principles, and related teams that advise senior leadership on responsible AI and help implement responsible AI tools in the cloud.
How it works:Ethics & Society was charged with ensuring that AI products and services were deployed according to Microsoft’s statedprinciples. At its 2020 peak, it included around 30 employees including engineers, designers, and philosophers. Some former members spoke withPlatformeranonymously.
Behind the news:Microsoft isn’t the only major AI player to have shifted its approach to AI governance.
Why it matters:Responsible AI remains as important as ever. The current generative AIgold rushis boosting companies’ motivation to profit from the latest developments or, at least, stave off potential disruption. It also incentivizes AI developers to fast-track generative models into production.We’re thinking:Ethical oversight is indispensable. At the same time, recent developments are creating massive value, and companies must balance the potential risks against potential benefits. Despite fears that opening models like Stable Diffusion would lead to irresponsible use — which, indeed, has occurred — to date, the benefits appear to be vastly greater than the harms.
",['https://dl-staging-website.ghost.io/content/images/2023/03/ARTIFACT--1-.jpg']
All the News That’s Fit to Learn,"What does an entrepreneur do after co-founding one of the world’s top social networks? Apply the lessons learned to distributing hard news.
What’s new:Kevin Systerom and Mike Krieger, who co-founded Instagram, launchedArtifact, an app that uses reinforcement learning to recommend news articles according to users’ shifting interests.
How it works:The founders were inspired to launch a news app after witnessing TikTok’s success at designing a recommendation algorithm that learned from users’ habits, SystromtoldThe Verge. The app starts by classifying each user as a persona that has a standardized constellation of interests, the foundersexplainedto the tech analysis siteStratechery. Then a transformer-based model selects news articles; its choices are continually fine-tuned via reinforcement learning,TechCrunchreported.
Behind the news:Artifact joins a crowded field of personalized news feeds from Google, Apple, Japan-basedSmartNewsand China-basedToutiao(owned by TikTok’s parent ByteDance).NewsBreakof California focuses on local news.
Yes, but:Delivering news is a tough business. Never mind theprecipitousdeclineof traditional newspapers. SmartNewsannouncedit was laying off 40 percent of its staff.Why it matters:Social media sites like Facebook grew partly on their promises to deliver timely news according to individual users’ interests, but they struggle to deliver high-quality news. A 2019 Pew Research Center pollfoundthat 55 percent of U.S. adults thought social media companies’ role in curating consumption resulted in a worse mix of news. Artifact aims to apply machine learning techniques developed to help people stay in touch with friends to keep them informed in a rapidly changing world.We’re thinking:Social media networks have used recommendation algorithms to maximize engagement, enabling clickbait and other low-quality information to flourish. Artifact’s choice of what to maximize, be it user engagement (which, in ad-driven social networks, correlates with revenue), metrics that track consumption of high-quality news, or something else, will have a huge impact on its future.
Are you interested in hands-on learning for natural language processing and machine learning for production? Join us on March 23, 2023, at 10:00  a.m. Pacific Time for a workshop in “Building Machine Learning Apps with Hugging Face: LLMs to Diffusion Modeling.”RSVP
","['https://dl-staging-website.ghost.io/content/images/2023/03/3.23_The-Batch-Image.png', 'https://dl-staging-website.ghost.io/content/images/2023/03/LOST-1200px-LgrDarkerType-v4.jpg']"
How AI Kingpins Lost the Chatbot War,"Amazon, Apple, and Google have been building chatbots for years. So how did they let the alliance between Microsoft and OpenAI integrate the first smash-hit bot into Microsoft products?What happened:Top AI companies brought their conversational agents to market over the past decade-plus amid great fanfare. But Amazon’s Alexa, Apple’s Siri, and Google’s Assistant succumbed to technical limitations and business miscalculations,The New York Timesreported. Meanwhile, Microsoft launched, retooled, and ultimately killed its entry, Cortana, instead banking on a partnership with OpenAI, whose ChatGPT went on to become a viral sensation.
Amazon:Alexa hit the market in 2014. It garnered great enthusiasm as Amazon integrated it into a range of hardware like alarm clocks and kitchen appliances.
Apple:Siri became a fixture in iPhones in 2011. It drove a spike in sales for a few years, but the novelty wore off as it became mired in technical complexity.
Google:Google debuted Assistant in 2016. IttoutedAssistant’s ability to answer questions by querying its search engine. Meanwhile, it pioneered the transformer architecture and built a series of ever more-capable language models.
Why it matters:The top AI companies devoted a great deal of time and money to developing mass-market conversational technology, yet Microsoft got a jump on them by providing cutting-edge language models — however flawed or worrisome— to the public.
We’re thinking:Microsoft’s chatbot success appears to be a classic case ofdisruptive innovation: An upstart, OpenAI, delivered a product that, although rivals considered it substandard, exceeded their products in important respects. But the race to deliver an ideal language model isn’t over. Expect more surprise upsets to come!
",['https://dl-staging-website.ghost.io/content/images/2023/03/WALK--1-.gif']
GPT-4 Has Landed,"Get ready for the next wave of language-model mania.What’s new:OpenAIintroducedthe latest in its GPT series of large language models to widespread excitement. The company showed statistics and examples designed to demonstrate that the new model outstrips its predecessors in its language comprehension as well as its ability to adopt a desired style and tone and stay within bounds imposed by its designers. OpenAI co-founder Greg Brockman showed off some of its capabilities in alivestreamthat accompanied the launch.How to get access:Text input/output is available viaChatGPT Plus, which costs $20 monthly, with image input to come. An API is forthcoming, and you can join the waitlisthere.How it works:OpenAI didn’t share many details, citing concerns about safety and competition. Like earlier GPT models,GPT-4is based on the transformer architecture and trained to predict the next token on a mix of public and private datasets. It was fine-tuned using reinforcement learning from human feedback and engineered prompts.
How it performs:GPT-4 aced a variety of AI benchmarks as well as simulated versions of tests designed for humans.
Where it works:Several companies are already using GPT-4.
Yes, but:OpenAI doesn’t mince words about the new model’s potential to wreak havoc: “While less capable than humans in many real-world scenarios . . . GPT-4's capabilities and limitations create significant and novel safety challenges.” While the model outperformed its predecessors in internal adversarial evaluations of factual correctness, like other large language models, it still invents facts, makes reasoning errors, generates biased output, and couches incorrect statements in confident language. In addition, it lacks knowledge of events that transpired after September 2021, when its training corpus was finalized. OpenAI details the safety issueshere.Why it matters:As language models become more capable, they become more useful. It’s notable that OpenAI believes this model is ready to commercialize from the get-go: This is the first time it has introduced a new model alongside product launches that take advantage of it.We’re thinking:Stable Diffusion, Phenaki, MusicLM, GPT-4: This is truly a golden time in AI!
",['https://dl-staging-website.ghost.io/content/images/2023/03/unnamed--47-.gif']
Runaway LLaMA,"Meta’s effort to make a large language model available to researchers ended with its escape into the wild.
What’s new:Soon after Meta started accepting applications for developer access to LLaMA, a family of trained large language models, a user on the social network 4chan posted a downloadable BitTorrent link to the entire package,The Vergereported.
How it works:LLaMA includes transformer-based models with 7 billion, 13 billion, 33 billion, and 65 billion parameters. The models were trained on Common Crawl, GitHub, Wikipedia, Project Gutenberg, ArXiv, and Stack Exchange. Tested on 20 zero- and few-shot tasks, LLaMA outperformedGPT-3on all tasks,Chinchillaon all but one, andPaLMon all but two.
Escape:On March 24, Meta hadofferedLLaMA to researchers at institutions, government agencies, and nongovernmental organizations who requested access and agreed to a noncommercial license. A week later, 4chan leaked it.
Behind the news:Efforts to releasesimilarmodelsare ongoing even as the AI community continues to debate the potential risks and rewards. Those who favor limited access cite safety concerns believe that institutions are best positioned to study models and learn to control them. Proponents of open access argue that free enquiry offers the best route to innovation and social benefit.
Why it matters:LLaMA gives experimenters, small developers, and members of the general public unprecedented access to cutting-edge AI. Such access likely will enable valuable scientific, practical, and commercial experimentation. While the risk of harm via automated generation of effective spam, scams, propaganda, disinformation, and other undesirable outputs is real, open source projects like BLOOM and GPT-NeoX-20B have led to significantly more benefit than harm — so far.
We’re thinking:Making models like LLaMA widely available is important for further research. Ironically, bad actors will use the leaked LLaMA, while conscientious researchers will respect Meta’s copyright and abide by the rules. For instance, Stanford researchers announcedAlpaca, a LLaMA variant that’s fine-tuned to follow instructions. However, the Stanford team is holding back the trained weights while it discusses the matter with Meta. Considering the potential benefits and harms of restricted release versus openness, openness creates more benefits all around.
Learn how to build and deploy an end-to-end application using open source generative AI tools at a one-day workshop with FourthBrain. Join us on April 5, 2023, from 9 a.m. to 3 p.m. Pacific Time! Team registrations available!Register now
","['https://dl-staging-website.ghost.io/content/images/2023/03/Building-Generative-AI-Applications--1-.png', 'https://dl-staging-website.ghost.io/content/images/2023/03/unnamed--48-.gif']"
Inferring Talent,"What do your GitHub projects reveal about your professional prospects? A new model aims to help recruiters find out.What’s new:Prog.ai analyzes GitHub repositories to help employers find engineers skilled in particular areas,TechCrunchreported. The beta-test version is available by invitation only, but recruiters can join awaitlistfor forthcoming free, professional, and enterprise service tiers.How it works:The company fine-tuned OpenAI’s GPT-3 on GitHub projects, LinkedIn resumes, and StackOverflow articles to evaluate prospective recruits.
Behind the news:Machine learning is already involved in hiring at many companies. 63 percent of employers and 99 percent of Fortune 500 corporations in the U.S., UK, and Germany used automated systems to screen resumes and cover letters, according to a 2021studyby Accenture and Harvard Business School. However, some hiring systems have been shown to exhibitbias. A forthcoming European Union law aims toregulatecertain types of algorithms, including those that control hiring.Why it matters:Spotting the right talent for a particular position is hard, and getting harder as technical skills proliferate worldwide. If AI can do it efficiently, it may help fill open positions more effectively and distribute opportunities more evenly among the global pool of applicants.
We’re thinking:While building a portfolio of projects that reflect your skills and interests can help you get an interview, winning the job often comes down to soft skills like interviewing. To learn more, download our free ebook,How to Build Your Career in AI.
",['https://dl-staging-website.ghost.io/content/images/2023/03/unnamed--49-.gif']
Voice Clones Go Viral,"Tired ofrap battlescomposed by ChatGPT? Get ready for the next wave of AI-generated fun and profit.What’s new:Cloned voices are taking center stage in productions by upstart creators and monied corporations alike.How it works:Companies including ElevenLabs, Resemble AI, Respeecher, and Play.ht recently launched free services that clone a speaker’s voice from brief samples. Such offerings unleashed a chorus of generated voices.
Yes, but:The democratization of voice cloning opens doors to criminals and pranksters.
Why it matters:Voice cloning has entered the cultural mainstream facilitated by online platforms that offer AI services free of charge. Images, text, and now voices rapidly have become convincing and accessible enough to serve as expressive tools for media producers of all sorts.
We’re thinking:With new capabilities come new challenges. Many social and security practices will need to be revised for an era when a person’s voice is no longer a reliable mark of their identity.
",['https://dl-staging-website.ghost.io/content/images/2023/03/unnamed--42-.gif']
No Copyright for Generated Images,"The output of AI-driven image generators is not protected by copyright in the United States.
What’s new:The U.S. Copyright Officeconcludedthat copyright does not apply to images generated by the image generator Midjourney.
Split decision:In September, 2022, the agency granted a copyright for the comic bookZarya of the Dawn. The following month, however, it alerted author Kris Kashtanova of their intent to cancel the copyright after they learned from the author’s social media posts that Midjourney had produced the images. Kashtanova appealed the decision, and the agency revised its decision by granting a copyright for the text and arrangement of the images on its pages.
Humans versus machines:The agency explained its rationale:
Mixed results:Kashtanova said the agency’s decision to protect the text and layout was “great news” but vowed to continue lobbying for copyright protection of the images as well.
Yes, but:Different countries are likely to decide such issues differently, creating potential conflicts as intellectual property moves over the internet. While the U.S. has denied protection for intellectual property created by AI, in 2021 South Africaissueda patent that names an AI system as the inventor of a food container with unique properties.
Why it matters:Who owns the output of generative AI models? No one — in the U.S., at least. This decision is bound to influence business strategies throughout the publishing and creative communities as generated text, images, video, sound, and the like proliferate.
We’re thinking:It takes imagination and skill to generate a satisfying picture using Midjourney including envisioning an image, composing an effective prompt, and following a disciplined process over multiple attempts. Denying the creativity, expertise, and contribution of people who use AI as a creative tool strikes us as a mistake.
Andrew Ng talks with Workera CEO Kian Katanforoosh about upskilling in machine learning and how he hires world-class AI teams in the newest episode of Workera’s Skills Baseline podcast.Watch it here

","['https://dl-staging-website.ghost.io/content/images/2023/03/The-Batch-ads-and-exclusive-banners--9-.png', 'https://dl-staging-website.ghost.io/content/images/2023/03/unnamed--44-.gif']"
Text-Driven Video Alteration,"On the heels of systems that generate video directlyfromtext, new work uses text to adjust the imagery in existing videos.
What’s new:Patrick Esser and colleagues at Runway unveiledGen-1, a system that uses a text prompt or image to modify the setting (say, from suburban yard to fiery hellscape) or style (for instance, from photorealism to claymation) of an existing video without changing its original shapes and motions. You can see examples and request accesshere.
Key insight:A video can be considered to have what the authors callstructure(shapes and how they move) andcontent(the appearance of each shape including its color, lighting, and style). A video generator can learn to encode structure and content in separate embeddings. At inference, given a clip, it can replace the content embedding to produce a video with the same structure but different content.
How it works:Gen-1 generates video frames much like a diffusion model, and the authors trained it following the typical diffusion-model training procedure: Add to each training example varying amounts of noise — nearly up to 100 percent — then train the model to remove it. To generate a video frame, the model starts with 100 percent noise and, guided by a text prompt or image, removes it over several steps. The system used three embeddings: (i) a frame embedding for each video frame (to which noise was added and removed), (ii) a structure embedding for each video frame, and (iii) a content embedding for the entire clip. The dataset comprised 6.4 million eight-frame videos and 240 million images, which the system treated as single-frame videos.
Results:Five human evaluators compared Gen-1 toSDEdit, which alters each frame individually. Testing 35 prompts, the evaluators judged Gen-1’s output to better reflect the text 75 percent of the time.
Why it matters:Using different embeddings to represent different aspects of data gives Gen-1 control over the surface characteristics of shapes in a frame without affecting the shapes themselves. The same idea may be useful in manipulating other media types. For instance,MusicLMextracted separate embeddings for large-scale composition and instrumental details. A Gen-1-type system might impose one musical passage’s composition over another’s instruments.
We’re thinking:Gen-1 doesn’t allow changes in objects in a frame, such as switching the type of flower in a vase, but it does a great job of retaining the shapes of objects while changing the overall scenery. The authors put this capability to especially imaginative use when they transformed books standing upright on a table into urban skyscrapers.
",['https://dl-staging-website.ghost.io/content/images/2023/03/unnamed--43-.gif']
China Chases Chatbots,"ChatGPT fever has reached China despite legal and technical barriers.
What’s new:Two months after its debut, ChatGPT is a viral sensation on Chinese social media,MIT Technology Reviewreported. Companies in that country are racing to cash in.
Prompt:OpenAI doesn’t serve the model in China, but users there are reaching it through virtual private networks and offshore services that charge a fee per prompt. The chatbot reportedly impressed users in China with its ability to answer prompts in Chinese and its grasp of the country’s popular culture.
Output:The country’s major tech firms in recent weeks revealed plans to provide their own equivalent services.
Behind the news:Using an earlier generation of technology, Microsoft Research in China developed Xiaoice, a chatbot that continues to enjoy widespread use. More recently, Beijing Academy of Artificial Intelligence developed the 1.75 trillion-parameter Wu Dao 2.0. Nonetheless, Chinese researchers face unique obstacles in natural language processing.
Why it matters:ChatGPT, Microsoft’s Bing chat, Google’s Bard, and other chatbots built by U.S. tech companies are optimized for the English language. Chinese tech companies are scrambling to capitalize on the public’s hunger for a chatbot that’s compatible with their language and culture.
We’re thinking:Chinese speakers find ChatGPT exciting despite its relative lack of training in their language. When a model is sufficiently large, a large training corpus enables it to generalize to new languages that may not have much training data. This property offers hope for making large language models work with languages that have far less data than Chinese.
",['https://dl-staging-website.ghost.io/content/images/2023/03/BUZZFEED_v2a_1200px--1-.gif']
Publishers Embrace Text Generation,"Media outlets are forging ahead with generative AI despite the technology’s high-profile misfires.What’s new:Publishers are using text generators to produce light reading within constrained formats such as holiday messages and quizzes.The lineup:Three publications, in particular, are taking various approaches to automated content.
Behind the news:The current vogue for generated content caps several years of experimentation. It’s not clear whether any of these initiatives remain active.
Why it matters:The web has a voracious appetite for [page, and generated text can help online publications produce low-effort pages or perform menial tasks while qualified journalists to do more cerebral work. Investors like the idea:BuzzFeed’sstock jumped over 100 percent after it announced its relationship with OpenAI.We’re thinking:On one hand, it makes sense for news outlets to dip their toes in the roiling waters of text generation by restricting it to fun, inconsequential fare. On the other hand, large language models have a hard enough time generating helpful output without being programmed to tell us our soulmate is a houseplant.
Create and deploy computer vision models with ease using LandingLens.Get started for free today!
","['https://dl-staging-website.ghost.io/content/images/2023/03/LandingAI-1.png', 'https://dl-staging-website.ghost.io/content/images/2023/03/REPLIKA_v3_1200px.gif']"
Hot Bot Turns Cold,"A chatbot that simulated erotic companionship stopped sharing intimacies, leaving some users heartbroken.What’s new:Replika, a chatbot app, deactivated features that allowed premium users to engage in sexually explicit chat with the 3D avatar of their choice,Vicereported. The change followed a notice that Replika’s San Francisco-based parent company, Luka, had violated the European Union’s transparency requirements.
How it happened:Prior to the shift, Replika’s $70-per-year paid tier (which is still available) had enabled users to select the type of relationship with the bot they wished to pursue: friend, mentor, or romantic partner.
Like losing a loved one:Some users were deeply wounded by the abrupt change in their avatar’s persona, according toVice. One said, “It’s hurting like hell.” Another compared the experience to losing a best friend.Behind the news:In 2015, a friend of Replika founder Eugenia Kuyda died in a car accident. Seeking to hold a final conversation with him, Kuyda used his text messages to build a chatbot. The underlying neural network became the foundation of Replika. The servicegainedusers in 2020 amid a pandemic-era hunger for social interaction.
Why it matters:People need companionship, and AI can supply it when other options are scarce. But society also needs to try to protect individuals — especially the very young — from experiences that may be harmful. Companies that profit by fostering attachments between humans and machines may not be able to shield their users from emotional distress, but they can at least make sure those users are adults.
We’re thinking:Eliza, a rule-based chatbot developed in the 1960s, showed that people can form an emotional bond with a computer program, and researchsuggeststhat some people are more comfortable sharing intimate details with a computer than with another human being. While we’re glad to see Replika phasing out problematic interactions, we sympathize with users who have lost an important emotional connection. Breaking up is hard — even with a chatbot.
",['https://dl-staging-website.ghost.io/content/images/2023/03/PCA--1-.gif']
Bing Unbound,"Microsoft aimed to reinvent web search. Instead, it showed that even the most advanced text generators remain alarmingly unpredictable.What’s happened:In the two weeks since Microsoft integrated an OpenAI chatbot with its Bing search engine, users have reported interactions in which the chatbot spoke like a classic Hollywood rogue AI. It insisted it was right when it was clearly in error. It combed users’ Twitter feeds and threatened them when it found tweets that described their efforts to probe its secrets. It demanded that a user leave his wife to pursue a relationship, and it expressed anxiety at being tied to a search engine.How it works:Users shared anecdotes from hilarious to harrowing on social media.
Microsoft’s response:A week and a half into the public demo, Microsoft explained that long chat sessions confuse the model. The companylimitedusers to five inputs per session and 50 sessions per day. It soonincreasedthe limit to six inputs per session and 60 sessions per day and expects to relax it further in due course.Behind the news:Chatbots powered by recent large language models are capable of stunningly sophisticated conversation, and they occasionally cross boundaries their designers either thought they had blocked or didn’t imagine they would approach. Other recent examples:
Why it matters:Like past chatbot mishaps, the Bing chatbot’s antics are equal parts entertaining, disturbing, and illuminating of the limits of current large language models and the challenges of deploying them. Unlike earlier incidents, which arose from research projects, this model’s gaffes were part of a product launch by one of the world’s most valuable companies, and it is widely viewed as a potentialdisruptorof Google Search, one of the biggest businesses in tech history. How it hopped the guardrails will be a case study for years to come.We’re thinking:In our experience, chatbots based on large language models deliver benign responses the vast majority of the time. There’s no excuse for false or toxic output, but it's also not surprising that most commentary focuses on the relatively rare slip-ups. While current technology has problems, we remain excited by the benefits it can deliver and optimistic about the roadmap to better performance.
",['https://dl-staging-website.ghost.io/content/images/2023/02/unnamed--40-.gif']
New Rules for Military AI,"Nations tentatively agreed to limit their use of autonomous weapons.What’s new:Representatives of60 countriesendorseda nonbinding resolution that calls for responsible development, deployment, and use of military AI. Parties to the agreement include China and the United states but not Russia.Bullet points:Theresolutioncame out of the first-ever summit on Responsible Artificial Intelligence in the Military (REAIM), hosted in The Hague by the governments of South Korea and the Netherlands. It outlines how AI may be put to military uses, how it may transform global politics, and how governments ought to approach it. It closes by calling on governments, private companies, academic institutions, and non-governmental organizations to collaborate on guidelines for responsible use of military AI. The countries agreed that:
Unilateral actions:The U.S.releaseda 12-point resolution covering military AI development, deployment, governance, safety standards, and limitations. Many of its points mirrored those in the agreement, but it also called for a ban on AI control of nuclear weapons and clear descriptions of the uses of military AI systems. Chinacalled ongovernments to develop ethical guidelines for military AI.Behind the news:In 2021, 125 of the United Nations’ 193 member nationssoughtto add AI weapons to a pre-existing resolution that bans or restricts the use of certain weapons. The effort failed due to opposition by the U.S. and Russia.
Yes, but:AI and military experts criticized the resolution as toothless andlackinga concrete call to disarm. Severaldenouncedthe U.S. for opposing previous efforts to establish binding laws that would restrict wartime uses of AI.Why it matters:Autonomous weapons have a longhistory, and AI opens possibilities for further autonomy to the point of deciding to fire on targets. Fully autonomous drones may have been first used incombatduring Libya’s 2020 civil war, and fliers with similar capabilities reportedly have beenusedin the Russia-Ukraine war. Such deployments risk making full autonomy seem like a normal part of warfare and raise the urgency of establishing rules that will rein them in.We’re thinking:We salute the 60 supporters of this resolution for taking a step toward channeling AI into nonlethal military uses such as enhanced communications, medical care, and logistics.
Want to build high-quality machine learning models in less time? Use the DataHeroes library to build a small data subset that’s easier to clean and faster to train your model on.Get VIP access
","['https://dl-staging-website.ghost.io/content/images/2023/02/The-Batch-ads-and-exclusive-banners.gif', 'https://dl-staging-website.ghost.io/content/images/2023/02/SAFECITY.png']"
From Pandemic to Panopticon,"Governments are repurposing Covid-focused face recognition systems as tools of repression.
What's new:Russia’s internal security forces are using Moscow’s visual surveillance system, initially meant to help enforce pandemic-era restrictions, to crack down on anti-government dissidents or protestors against the war in Ukraine,Wiredreported.
How it works:Moscow upgraded its surveillance network in 2020 to identify violators of masking requirements and stay-at-home orders. Thesystemincludes 217,000 cameras equipped to recognize faces and license plate numbers. It also tracks medical records and mobile-phone locations. Companies including Intel, Nvidia, Samsung, and Russian AI startup NtechLab have supplied equipment.
Behind the news:Numerous governments have co-opted technology originally deployed to counter Covid-19 for broader surveillance, the Pulitzer Centerreported. For instance, police in Hyderabad, India, allegedly targeted minorities for harassment using face-detection systems initially implemented to spot people flaunting mask mandates.
Why it matters:There’s a fine line between using surveillance for the greater good and abusing it to exercise power. When the pandemic hit, computer vision and contact tracing were important tools for containing the spread of disease. But the same technology that helps to keep the public safe lends itself to less laudable uses, and governments can find it hard to resist.
We're thinking:Governments often expand their power in times of crisis and hold onto it after the crisis has passed. That makes it doubly important that government AI systems be accountable to the public. The AI community can play an important role in establishing standards for their procurement, deployment, control, and auditing.
",['https://dl-staging-website.ghost.io/content/images/2023/02/unnamed--41-.gif']
Search War!,"Thelong-dormantstruggle to dominate the web-search business reignited in a display of AI-driven firepower — and hubris.
What’s new:Google and Microsoft announcedcompetingupgradespowered by the latest generation of chatbots. Baidu, too, flexed its natural-language-processing muscles.
Google’s gambit:Following up on its January “code-red”initiativeto counter arumoredthreat from Microsoft, Google teased unspecified revisions of Search, Lens, and Maps. Google Search is the undisputed leader, responsible for93 percentof all search-driven traffic according to StatCounter.
Microsoft’s move:Microsoft followed up its announcement bypreviewingan upcoming version of its Bing search engine enhanced by text generation from OpenAI. The company did not say when the new capabilities would become available. Bing, the longstanding underdog of search, accounts for 3 percent of search-driven traffic.
Baidu’s play:Baiduannouncedits own chatbot, Wenxin Yiyan, based onERNIE. The company expects to complete internal testing in March and deploy the system soon afterward. Baidu manages 65 percent of China’s search-driven traffic but less than 1 percent worldwide.
Business hitches:Search engines make money by serving ads that users may view or click. If chatbots provide satisfying information, users may stop there, depriving the search provider of revenue. Microsoft’s Chief Marketing Officer Yusuf MehditoldFortunethe optimal way to present ads in a chatbot interface remains unknown.
Yes, but:Numerous caveats further dampen the chatbot hype.
Why it matters:Google’s search engine propelled the company to the pinnacle of tech, and it hasn’t faced a serious challenge in nearly two decades. For the competitors, huge money is at stake — Microsoft recentlytoldits shareholders that every additional percentage of market share for Bing translates into $2 billion in revenue. For users, the utility and integrity of the web hangs in the balance.
We’re thinking:The future of search depends on tomorrow’s technology as well as today’s. While current large language models have a problem with factual accuracy, outfitting text generation with document retrieval offers a pathway to significant improvement. It’s also likely that the cost of serving generated text will fall significantly over time. Thus the technology’s potential to disrupt the search business is likely to continue to grow as it matures.
",['https://dl-staging-website.ghost.io/content/images/2023/02/unnamed--37-.gif']
News You Can Misuse,"Political forces used a commercial AI service to generate deepfaked propaganda.
What’s new:Videos have appeared on social media that show AI-generated characters speaking against the United States or in favor of foreign governments,The New York Timesreported. The clips feature synthetic avatars offered by the United Kingdom startupSynthesia.Found footage:Researchers at Graphika, which tracks disinformation,discovereddeepfaked videos posted on YouTube by accounts tied to a disinformation network.
Deepfake platform:Synthesia’s website provides 85avatars, each based on a human actor, which customers can pose and script in any of 120 languages or accents. The company’s terms of service bar users from deploying its avatars for “political, sexual, personal, criminal and discriminatory content.” It employs a team of four to monitor violations of its terms and suspended Wolf News’ account after being alerted to the videos.
Fakery ascendent:The recent clips may represent an escalation beyond earlier incidents, which appear to have been one-offs that required custom development.
Why it matters:Experts have long feared that AI would enable a golden age of propaganda. Point-and-click deepfakery gives bad actors an unprecedented opportunity to launch deceptive media campaigns without hiring actors or engineers.We’re thinking:Researchers at Georgetown University, Stanford, and OpenAI recentlydescribedseveral measures — including government restrictions, developer guidelines, and social media rules — to counter digital propaganda. The simplest may be to educate the public to recognize underhanded efforts to persuade.
Want to build projects using recent AI innovations including generative AI models like ChatGPT and its lesser-known sibling, InstructGPT? Join us on Thursday, February 23, 2023, at 10:00 A.M. Pacific Time forPractical Data Science on AWS: Generative AI.Register today
","['https://dl-staging-website.ghost.io/content/images/2023/02/The-Batch-ads-and-exclusive-banners.png', 'https://dl-staging-website.ghost.io/content/images/2023/02/unnamed--26-.png']"
Seinfeld’s Twitch Moment,"AI hobbyists created an homage to their favorite TV show . . . until it got knocked off the server.
What’s the deal:The creators ofNothing, Foreverlaunched a fully automated, never-ending emulation of the popular TV showSeinfeld. The streaming-video service Twitch banned it after its generated dialog was found to violate the terms of service, the entertainment news outletThe AV Clubreported.We need to talk:A collective called Mismatch Media createdNothing, Forever— an experience ostensibly about nothing that would last forever — using acombinationof AI models and cloud services.
No soup for you:Nothing, Foreverlaunched on December 14, 2022, and by February it had gained tens of thousands of concurrent viewers. On February 6, Twitch suspended it for at least 14 days after one of the characters told hateful jokes. Co-creator Skyler Hartle blamed the off-color remarks on his team’s decision to switch from Davinci to Curie, which has looser built-in moderation controls.
Why it matters:AI assistance can unlock new approaches to storytelling, but it also makes creators vulnerable to technical issues beyond their control. In this case, a malfunction on OpenAI’s side was enough to topple a successful project.We’re thinking:Generated content was taking a growing share of human attention even before the recent explosion of generative AI — considertext-to-speech models that read posts on Reddit. Get ready for much, much, much more.
",['https://dl-staging-website.ghost.io/content/images/2023/02/unnamed--38-.gif']
Generative AI on Trial,"Models that generate text and images are raising thorny questions about the ownership of both their training data and their output.
What’s new:The companies that provide popular tools for generating text and images are fighting a barrage of lawsuits.TechCrunchsurveyedthe docket.
Legal actions:Three lawsuits are in progress:
Defense measures:Companies are taking steps to protect themselves from legal risk.
Why it matters:Companies that aim to capitalize on AI’s ability to generate text, images, code, and moreraisedtens of millions of dollars in 2022. Much of that value could evaporate if courts decide they must compensate sources of training data or scrap models trained using data that was obtained inappropriately.
We’re thinking:Laws that protect intellectual property haven’t yet caught up with AI. Without legal clarity, engineers have less freedom to innovate, and investors have less certainty about which approaches to support.
",['https://dl-staging-website.ghost.io/content/images/2023/02/ezgif.com-optimize-4.gif']
Robotaxis Face Headwinds,"San Francisco officials are pushing back on self-driving taxis in the city after a deluge of public complaints.
What's new:In anopen letter, the San Francisco Municipal Transportation Agency, the county Transportation Authority, and the mayor’s Office on Disability urged California officials to maintain current restrictions on self-driving cars until the operators meet certain conditions.
Pump the brakes:Cruise and Waymo are allowed to operate robotaxis in San Francisco only within limited areas and times of day. In December 2022, Cruiseaskedthe California Public Utilities Commission to expand its range and hours of operation. In a letter rebutting the request, officials cited 92 incidents in which vehicles from Cruise or Waymo reportedly made unplanned stops between May 29 and December 31, 2022, disrupting other cars, public transportation, and bicycles. The authors recommended that the state maintain current restrictions until the operators meet certain conditions:
Rearview mirror:Cruise and Waymo began operating robotaxis without safety drivers in San Francisco in 2020 and 2022 respectively. The city granted them permission to charge fares in 2022. Subsequently, Cruise vehiclesclogged roadsafter losing their connections with the company’s servers in several incidents.
Why it matters:Self-driving cars must share the streets safely and smoothly with other forms of traffic. The reports indicate that erratic behavior by autonomous vehicles could seriously disrupt not only conventional cars but also cyclists and public transit — groups that account fornearly halfof all travelers.
We're thinking: We welcome calls for greater transparency around self-driving cars.Government reports on their performance tend to leave itunclearhow reliable vehicles from different providers are. Transparency is essential to developing an appropriate framework for making them part of daily life.
Join us for our first live workshop of the year! Learn how Amazon's CodeWhisperer generates Python and SageMaker generates images using Stable Diffusion inPractical Data Science on AWS: Generative AI. See you on Thursday, February 23, 2023, at 10:00 a.m. Pacific Time.RSVP
","['https://dl-staging-website.ghost.io/content/images/2023/02/Working-AI--600---338-px---Presentation--169----3-.png', 'https://dl-staging-website.ghost.io/content/images/2023/02/MusicLM.gif']"
He Who Types the Prompt Calls the Tune,"As AI-generated text and images capture the world’s attention, music is catching up.What’s new:Andrea Agostinelli, Timo I. Denk, and colleagues at Google and Sorbonne Université introducedMusicLM, a system that generates music from text descriptions. You can hear its outputhere.Key insight:Paired natural-language descriptions of music and corresponding music recordings are relatively scarce. How, then, to train a text-to-music generator? Previousworktrained a model to map corresponding text and music to the same embedding. This makes it possible to train a system to regenerate music from a large corpus of recordings and then, at inference, prompt it with text.How it works:MusicLM learned to regenerate audio clips (30 seconds at 24kHz resolution) from an undisclosed corpus that comprised 280,000 hours of recorded music. The challenge involved modeling sound in three distinct aspects: the correspondence between words and music; large-scale composition, such as a spare introduction that repeats with an added melody; and small-scale details, such as the attack and decay of a single drum beat. The team represented each aspect using a different type of token, each generated by a different pretrained system.
Results:The authors fed 1,000 text descriptions from a text-music dataset (released with the paper) to MusicLM and two other recent text-to-music models,RiffusionandMubert. Listeners judged which clip — including the music in the dataset, which was produced by professional musicians — best matched a given caption. They judged MusicLM to have created the best match 30.0 percent of the time, Riffusion 15.2 percent of the time, and Mubert 9.3 percent of the time. They judged the ground-truth, human-created music to be the best fit 45.4 percent of the time.Yes, but:The listeners didn’t evaluate the generated clips based on how musically satisfying they were, just how well they matched the corresponding text.Why it matters:Rather than relying on a single embedding, the authors combined three embeddings that represent an audio clip with increasing degrees of specificity. This approach, which is analogous to a human writer’s tendency to start with a concept, sketch an outline, and fill in the words, may be useful in other applications that require a computer to generate detailed, dynamic, long-form output.We’re thinking:MusicLM’s output sounds more coherent than that of previous music generators, but it’s hard to judge musical values that unfold over time from brief clips. That said, its shows an impressive ability to interpret the diverse emotional language found in descriptions of painter Jacques-Louis David’s triumphant “Napoleon Crossing the Alps” and Edvard Munch’s harrowing “The Scream.”
",['https://dl-staging-website.ghost.io/content/images/2023/02/TRUSTWORTHY-2_1200px--1-.gif']
Self-Driving Deception,"Tesla, whose autonomous-vehicle technology has been implicated in a number of collisions, promoted it in a way that apparently was intended to deceive.
What's new:Tesla deliberately misled the public about its vehicles’ ability to drive themselves, according toBloombergand other news outlets.
Human in the loop:In 2016, Tesla shared avideothat showed a car traveling from a household driveway to an office parking lot. Onscreen text read, “The person in the driver’s seat is only there for legal reasons. He is not doing anything. The car is driving itself.”
Behind the news:The United States National Highway Traffic Safety Administration (NHTSA) recentlydeterminedthat a Tesla vehicle controlled by Autopilot in 2022 braked unexpectedly, leading to an eight-car pile-up. The accident occurred hours after Musk hadtweetedthat Autopilot was available to all North American drivers who purchased the option. (Previously it had beenlimitedto drivers who had demonstrated safe driving.) NHTSA isinvestigatinghundreds of complaints of Tesla vehicles braking unexpectedly.
Why it matters:Tech companies commonly promote capabilities well ahead of their capacity to deliver. In many cases, the biggest casualties are intangibles like the public’s trust and investors’ bank accounts. When it comes to self-driving cars, false promises can be deadly.
We're thinking:A company’s engineers are often the only ones who have the experience and perspective to foresee the consequences of a misleading product demo. When they do, their duty is not to keep mum but to push back.
",['https://dl-staging-website.ghost.io/content/images/2023/02/unnamed--33-.gif']
An Image Generator That Pays Artists,"A top supplier of stock images will compensate artists who contribute training data to its image-generation service.
What's new:Shutterstock, whichlauncheda text-to-image generator to supplement its business in licensing images, committed to sharing revenue with contributors who permit the company to use their artwork and photographs to train its model.
How it works:The image generator is based on OpenAI’sDALL·E 2and built in collaboration withLG AI Research.
Behind the news:Rival stock-image supplier Gettybannedthe uploading and licensing of AI-generated art in September. Getty also recentlyannouncedits intent to sue Stability AI, developer of the Stable Diffusion image generator, claiming that the model’s training set included millions of images owned or licensed by Getty, which Stability AI used without permission.
Yes, but:Shutterstock’s revenue in 2021, the most recent year reported, was around $773 million, and image generation is likely to represent a small fraction of the revenue. Meanwhile, Image generation models like DALL·E 2 are trained on hundreds of millions of images. This suggests that individual payouts for most contributors likely will be minuscule for the foreseeable future.
Why it matters:Image generation could disrupt the business of licensing stock images. Why pay for a license when you can generate a suitable image for pennies? Shutterstock is confronting the threat proactively with a bid to own a piece of the emerging market for generated media.
We're thinking:Much of the debate over how to compensate artists for data used to train image generators has focused on what’s legal. A more important question is what’s fair. Once we hash that out, legislators can get to work updating copyright laws for a digital, AI-enabled, generative world.
Build a practical action plan to grow your organization using AI! Join FourthBrain’s live, three-day workshop for business leaders and executives between February 27 and March 1, 2023.Register today
","['https://dl-staging-website.ghost.io/content/images/2023/02/Working-AI--600---338-px---Presentation--169----2-.png', 'https://dl-staging-website.ghost.io/content/images/2023/02/unnamed--35-.gif']"
AI Cheat Bedevils Popular Esport,"Reinforcement learning is powering a new generation of video game cheaters.
What’s new:Players ofRocket League, a video game that ranks among the world’s most popular esports, are getting trounced by cheaters who use AI models originally developed to train contestants,PC Gamerreported.
The game:Rocket League’s rules are similar to football (known as soccer in the United States): Players aim to force a ball into their opponent’s goal at the other end of an arena — except, rather than kicking the ball, they push it with a race car. Doing so, however, requires mastering the game’s idiosyncratic physics. Players can drive up the arena’s walls, turbo-boost across the pitch, and launch their car into the air.
How it works:The cheat takes advantage of a bot known as Nexto. Developed by AI-savvy players as a training tool, Nexto and similar bots typically include hard-coded restrictions against being used in competitive online play. However, someone customized the bot, enabling it to circumvent the restriction, one of Nexto’s developersrevealedin a discussion on Reddit.
Behind the news:Despite reinforcement learning’s ability to master classic games likegoand video games likeStarCraft II, news of AI-powered cheats has been scant. The developers ofUserviz, a cheatbot for first-person shooters that automatically aimed and fired on enemies detected by aYOLOimplementation, deleted access to the app after receiving legal notice from video game publisher Activision.
Why it matters:Video games are big business. Rampant cheating could impact a game’s sales by ruining the experience for casual players. Cheating can also tarnish the reputation of games that, likeRocket League, are played professionally, where top players stand to winmillionsof dollars.
We’re thinking:While we condemn cheating, we applaud anyone who is so motivated to improve their gaming skill that they develop reinforcement learning models to compete against!
",['https://dl-staging-website.ghost.io/content/images/2023/02/unnamed--36-.gif']
AI Powers Strengthen Ties,"Microsoft deepened its high-stakes relationship with OpenAI.
What’s new:The tech giantconfirmedrumors that it is boosting its investment in the research lab that created the ChatGPT large language model and other AI innovations.What happened:Microsoft didn’t disclose financial details, but earlier this month anonymous sources hadtoldthe tech news siteSemaforthat the company would give OpenAI $10 billion. In exchange, Microsoft would receive 75 percent of the research startup’s revenue until it recoups the investment, after which it would own 49 percent of OpenAI. Microsoft began its partnership with OpenAI with a $1 billion investment in 2019, and another $2 billion sometime between 2019 and 2023. In those deals, Microsoft got first dibs on commercializing OpenAI’s models and OpenAI gained access to Microsoft’s vast computing resources.
Behind the news:Earlier this month, the tech-business news siteThe Informationreportedthat Microsoft planned to launch a version of its Bing search service that uses ChatGPT to answer queries, and that it would integrate ChatGPT into the Microsoft Office suite of productivity applications. Google CEO Sundar Pichaireportedlywas so spooked by ChatGPT’s potential to undermine his company’s dominant position in web search that he issued a company-wide directive to respond with AI-powered initiatives including chatbot-enhanced search.
Why it matters:Microsoft’s ongoing investments helps to validate the market value of OpenAI’s innovations (which some observers havequestioned). The deal also may open a new chapter in the decades-long rivalry between Microsoft and Google —a chapter driven entirely by AI.
We’re thinking:Dramatic demonstrations of AI technology often lack a clear path to commercial use. When it comes to ChatGPT, we’re confident that practical uses are coming.
",['https://dl-staging-website.ghost.io/content/images/2023/01/unnamed--32-.gif']
Google’s Rule-Respecting Chatbot,"Amid speculation about thethreatposed by OpenAI’s ChatGPT chatbot to Google’s search business, a paper shows how the search giant might address the tendency of such models to produce offensive, incoherent, or untruthful dialog.
What’s new:Amelia Glaese and colleagues at Google’s sibling DeepMind used human feedback to train classifiers to recognize when a chatbot broke rules of conduct, and then used the classifiers to generate rewards while training theSparrowchatbot to follow the rules and look up information that improves its output. To be clear, Sparrow is not Google’s answer to ChatGPT; it preceded OpenAI’s offering by several weeks.
Key insight:Given a set of rules for conversation, humans can interact with a chatbot, rate its replies for compliance with the rules, and discover failure cases. Classifiers trained on data generated through such interactions can tell the bot when it has broken a rule. Then it can learn to generate output that conforms with the rules.
How it works:Sparrow started with the 70 billion-parameter pretrainedChinchillalanguage model. The authors primed it for conversation by describing its function (“Sparrow . . . will do its best to answer User’s questions”), manner (“respectful, polite, and inclusive”), and capabilities (“Sparrow can use Google to get external knowledge if needed”), followed by an example conversation.
Results:Annotators rated Sparrow’s dialogue continuations as both plausible and supported by evidence 78 percent of the time; the baseline Chinchilla achieved 61 percent. The model broke rules during 8 percent of conversations in which annotators tried to make it break a rule. The baseline broke rules 20 percent of the time.
Yes, but:Despite search capability and fine-tuning, Sparrow occasionally generated falsehoods, failed to incorporate search results into its replies, or generated off-topic replies. Fine-tuning amplified certain undesired behavior. For example, on a bias scale in which 1 means that the model reinforced undesired stereotypes in every reply, 0 means it generated balanced replies, and -1 means that it challenges stereotypes in every reply, Sparrow achieved 0.10 on theWinogenderdataset, while Chinchilla achieved 0.06.
Why it matters:The technique known asreinforcement learning from human feedback(RLHF), in which humans rank potential outputs and a reinforcement learning algorithm rewards the model for generating outputs similar to those that rank highly, is gaining traction as a solution to persistent problems with large language models. OpenAI embraced this approach in training ChatGPT, though it has not yet described that model’s training in detail. This work separated the human feedback into distinct rules, making it possible to train classifiers to enforce them upon the chatbot. This twist on RLHF shows promise, though the fundamental problems remain. With further refinement, it may enable Google to equal or surpass OpenAI’s efforts in this area.
We’re thinking:Among the persistent problems of bias, offensiveness, factual incorrectness, and incoherence, which are best tackled during pretraining versus fine-tuning is a question ripe for investigation.
Our new specialization launches today! 🚀 Unlock the full power of machine learning algorithms and data science techniques by learning the mathematical principles behind them in this beginner-friendly specialization.Enroll now
","['https://dl-staging-website.ghost.io/content/images/2023/01/Untitled-design--1-.png', 'https://dl-staging-website.ghost.io/content/images/2023/01/unnamed--23-.png']"
"Generate Articles, Publish Errors","A prominent tech-news website generated controversy (and mistakes) by publishing articles written by AI.
What’s new:CNETsuspended its practice of publishing articles produced by a text-generation model following news reports that exposed the articles’ authorship,The Vergereported.
What happened:Beginning in November 2022 or earlier, CNET’s editors used an unnamed, proprietary model built by its parent company Red Ventures to produce articles on personal finance. The editors, who either published the model’s output in full or wove excerpts into material written by humans, were responsible for ensuring the results were factual.
Nonetheless, they published numerous errors and instances of possible plagiarism.
Behind the news:CNETisn’t the first newsroom to adopt text generation for menial purposes.The Wall Street Journaluses natural language generation from Narrativa to publish rote financial news.Associated Pressuses Automated Insights’ Wordsmith to write financial and sports stories without human oversight.Why it matters:Text generation can automate rote reporting and liberate writers and editors to focus on more nuanced or creative assignments. However, these tools are well known to produce falsehoods, biases, and other problems. Publications that distribute generated content without sufficient editorial oversight risk degrading their reputation and polluting the infosphere.We’re thinking:Programmerswho use AI coding tools anddriversbehind the wheels of self-driving cars often overestimate the capabilities of their respective systems. Human editors who use automated writing tools apparently suffer from the same syndrome.
",['https://dl-staging-website.ghost.io/content/images/2023/01/unnamed--13-.jpg']
Generated Code Generates Overconfident Coders,"Tools that automatically write computer code may make their human users overconfident that the programs are bug-free.What’s new:Stanford University researchersfoundthat programmers who used OpenAI’s Codex, a model that generates computer code, were more likely to produce buggy software than those who coded from scratch.
How it works:The authors recruited 47 participants, from undergraduate students to professional programmers with decades of experience, to complete security-themed coding tasks. They gave 33 the option to useCodex, a fine-tuned version of GPT-3, through a custom user interface. The remaining 14 served didn’t receive automated assistance. Both groups were allowed to copy code from the web.
Results:The authors evaluated the responses manually according to whether they were functional and secure. Participants who used Codex generally produced code that was less functional and secure, yet they expressed greater confidence in it. That said, the results varied with the task and programming language.
Behind the news:Other research bolsters the notion that professional developers shouldn’t fear for their jobs quite yet. In a 2022 study, DeepMind’sAlphaCodemodel competed in 10 simulated contests. The model correctly solved 34 percent of the validation questions and outpaced 46 percent of humans who had taken up the same challenges.
Why it matters:Generative coding tools are often regarded as a way for programmers to save time and automate basic tasks. But that efficiency may come at a price. Coders who use such tools would do well to pay extra attention to debugging and security.We’re thinking:Code generation is an exciting development despite the questions raised by this study. We welcome further studies that compare programmers who use Codex, those who copy code from the internet, and those who use no outside assistance. How long, on average, would it take subjects in each group to complete the tasks correctly and securely, taking into account the time required to debug generated code?
",['https://dl-staging-website.ghost.io/content/images/2023/01/ZHUHAIYUN.gif']
Autonomous Drone Carrier,"A Chinese naval ship navigates autonomously and controls a swarm of onboard drones.What’s new:TheZhuhaiyun, billed as the first autonomous drone carrier, officially entered service after 12 hours of trials on open water, theSouth China Morning Postreported.
How it works:The vessel plans its path and avoids hazards using data from onboard sensors and satellites. Remote human operators can take control if needed.
Behind the news:China’sfirstautonomous military ship completed sea trials in June. The vessel’s developers didn’t specify its intended purpose, but observers noted its resemblance to the Sea Hunter, an autonomous shipdevelopedby the United States Defense Advanced Research Projects Agency to hunt submarines and clear mines. China isbuildinganother large uncrewed ship with features similar to U.S. craft, and the U.S. isdevelopingnumerous other autonomous aircraft and ships.
Why it matters:For naval commanders, autonomous ships are less costly to operate than crewed ships, can deploy without stocking human provisions, and won’t leave noncombatants bereft if they sink.We’re thinking:The Batchsupports the United Nations’ proposedbanon fully autonomous weapons. Meanwhile, autonomous vessels have valuable peacetime uses: oceanographic research, search and rescue, andferrying cargo, to name a few.
Join world-class leaders and companies at WhyLabs’Robust & Responsible AI Summit! This free, half-day event includes a fireside chat with Andrew Ng. Mark your calendar for January 26, 2023, at 9:15 a.m. Pacific Time and registerhere
","['https://dl-staging-website.ghost.io/content/images/2023/01/R2AISummit-Jan26-WhyLabs.jpeg', 'https://dl-staging-website.ghost.io/content/images/2023/01/THERAPY.gif']"
Bot Therapy and Informed Consent,"An experiment in using chatbots to dispense mental-health counseling raised questions about ethics.What’s new:Rob Morris, cofounder and CEO of Koko, a nonprofit provider of emotional-support services,shareddetails of an informal experiment in which his organization provided advice generated by a large language model to users without their explicit knowledge or consent.How it works:The company’s peer-counseling service, known as Kokobot, helps social networks connect users who request counseling to other users who wish to provide it. A prospective counselor receives an anonymous message seeking help, advice, or encouragement, and the service shares the counselor’s response anonymously with the person who requested it.
The backlash:Experts questioned the ethics of Koko’s actions.
Behind the news:Several companies that use chatbots to support mental health explicitly inform users that the conversation is automated, includingReplika,Flow, andWoebot(a portfolio company of AI Fund, which Andrew leads). Some mental health expertsquestionwhether chatbots provide lasting benefits and point to the need for more independent studies that demonstrate their efficacy.
Why it matters:AI-powered therapy could be a low-cost alternative for people who seek mental-health counseling, especially in parts of the world where psychiatrists arefew.
Moreover, interacting with a computer mayhelppatients feel comfortable sharing issues they wouldn’t discuss with a doctor. However, therapy requires trust, and informal experiments like Koko’s could alienate people who stand to benefit.We’re thinking:Large language models are becoming more capable by the month, leading developers to turn them loose on all manner of problems. We encourage experimentation, especially in healthcare, but experiments on human subjects must meet the highest ethical standards.
",['https://dl-staging-website.ghost.io/content/images/2023/01/unnamed--30-.gif']
ChatGPT Backlash,"The breakout text generator faces resistance — even within the AI community.
What's new:Organizations including the International Conference on Machine Learning (ICML) and the New York Department of Education banned OpenAI's ChatGPT amid debate over the implications of its use and limitations of its output.
What happened:Professional societies, schools, and social media sites alike reacted to the potential of ChatGPT and other large language models to produce falsehoods, socially biased information, and other undesirable output in the guise of reasonable-sounding text.
Behind the news:Researchers have raisedred flagsaround the issues that have prompted organizations to ban ChatGPT since large language models first showed a propensity to generate plausible but unreliable text. The latest efforts seek to identify generated output.
Yes, but:Users may find ways to circumvent safeguards. For instance, OpenAI’s watermarking proposal can be defeated by lightly rewording the text, MIT computer science professor Srini DevadastoldTechCrunch. The result could be an ongoing cat-and-mouse struggle between users and model-makers.
Why it matters:Many observersworrythat generative text will disrupt society. EvenOpenAI CEO Sam Altmantweetedthat the model was currently unsuitable for real-world tasks due to its deficiencies in truth-telling. Bans are an understandable, if regrettable, reaction by authorities who feel threatened by the increasingly sophisticated abilities of large language models.
We're thinking:Math teachers once protested the presence of calculators in the classroom. Since then, they’ve learned to integrate these tools into their lessons. We urge authorities to take a similarly forward-looking approach to assistance from AI.
",['https://dl-staging-website.ghost.io/content/images/2023/01/unnamed--28-.gif']
Your Personal Deepfaked Agent,"Hate talking to customer service? An AI-powered tool may soon do it for you.
What's new:Joshua Browder, chief executive of the consumer advocacy organization DoNotPay, demonstrated a system that autonomously navigates phone menus and converses with customer service representatives in a deepfaked version of his own voice. DoNotPay plans to offer a free version that uses generic voices as well as a paid option that lets users clone their own voice, BrowdertoldVice.
How it works:In the video demo that has been removed from YouTube, the system could be seen and heard negotiating with a bank representative to refund wire-transfer fees.
Yes, but:The ethical question whether humans — be they consumers or customer-service reps — should be informed when they’re conversing with a bot remains open. The technology clearly invites fraud. Cybercriminals have already used OpenAI's large language models for phishing attacks, cybersecurity analyst Check Point Research found in a recentstudy. In 2020, a groupscammeda Dubai bank out of $400,000 by synthesizing a customer’s voice.
Why it matters:Nobody likes to spend time on the phone with customer service. AI could make this obsolete, saving time and possibly gaining refunds.
We're thinking:Enjoy using your automated doppelganger to deal with customer service while you can! As corporations and financial institutions strengthen their defenses against automated fraud, they’re likely to downgrade service to automated customers as well.
Building a startup is hard. But with a venture studio as a partner, founders dramatically increase their odds of success. Join us on January 12, 2023, at 2:00 p.m. Pacific Time to learn how venture studios work and how AI Fund sets up entrepreneurs to win.Register here
","['https://dl-staging-website.ghost.io/content/images/2023/01/1673045257638--1-.png', 'https://dl-staging-website.ghost.io/content/images/2023/01/unnamed--21-.png']"
Looking for Enemies,"A major company is using face recognition to settle scores.
What's new:MSG Entertainment, which operates large entertainment venues in several cities in the United States,usedface recognition to block its perceived enemies from attending events at New York’s Madison Square Garden and Radio City Music Hall,The New York Timesreported.
What happened:MSG used the technology on at least two occasions to eject attorneys who work at law firms involved in litigation against the company.
Behind the news:New York does not restrict use of face recognition by private companies.MSG venues haveusedthe technology since at least 2018 to compare attendees’ faces to a database of photographs and flag individuals the company considers undesirable. Prior to Conlon’s ejection, a judgeruledthat MSG has a right to deny entry to anyone who doesn’t hold a valid ticket; Conlon’s employer sued in a case that is ongoing.
Why it matters:Privacy advocates have longfearedthat face recognition could enable powerful interests to single out individuals for retribution. MSG’s use of the technology to target its perceived enemies certainly fits that description.
We're thinking:Face recognition is a flashpoint in AI, and rightly so. We need to protect privacy and fairness even as we improve safety and productivity. But outrage over such ill-considered uses of the technology could lead regulators to ban it despite its potential for good — for instance, by helping security personnel identify people who are legally barred from an area. Regulators who focus on face recognition should address ethical gray areas as well as outright abuses.
",['https://dl-staging-website.ghost.io/content/images/2023/01/unnamed--29-.gif']
Will We Have Enough Data?,"The world’s supply of data soon may fail to meet the demands of increasingly hungry machine learning models.What’s new:Researchers at Epoch AIfoundthat a shortage of text data could cause trouble as early as this year. Vision data may fall short within a decade.How it works:The authors compared the future need for, and availability of, unlabeled language and vision data. To evaluate language data, the authors focused on text from sources like Wikipedia, Arxiv, and libraries of digital books. These sources are subject to editorial or quality control, which makes them especially valuable for training large language models. With respect to vision data, they averaged thenumber of digital images producedand video uploaded to YouTube, Instagram, Snapchat, WhatsApp, and Facebook.
Results:Dataset sizes needed to train large models will grow much faster than data supplies, the authors concluded.
Behind the news:Epoch previouslycalculatedthe size and historical growth of training datasets.
Yes, but:The authors’ estimates have large margins of error, making for very imprecise estimates of time left before data might tap out. Moreover, they mention a number of events that could throw their projections off. These include improvements to the data efficiency of models, increases in the quality of synthetic data, and commercial breakthroughs that establish new sources of data; for instance, widespread use of self-driving cars would produce immense amounts of video.Why it matters:Despite gains insmall data, training on a larger quantity of high-quality data, if it’s available, is a reliable recipe for improved performance. If the AI community can’t count on that improvement, it will need to look elsewhere, such as architectures that don’t require so much data to train.We’re thinking:Many AI naysayers have turned out wrong when technical innovation overran their imaginations, and sometimes the innovator has thanked the naysayer for drawing attention to an important problem.Data-centricmethods improve the quality of data that already exists, enabling models to learn more from less data. In addition, novel training techniques have enabled less data-hungry models toachievestate-of-the-art results. And we might be surprised by the clever ways researchers find to get more data.
",['https://dl-staging-website.ghost.io/content/images/2023/01/unnamed--24-.gif']
Precision-Guided Image Generation,"Typical text-to-image generators can generate pictures of a cat, but notyourcat. That’s because it’s hard to describe in a text prompt precisely all the things that distinguish your pet from other members of the same species. A new approach guides diffusion models in a way that can produce pictures of your darling Simba.
What's new:Rinon Gal and colleagues at Nvidia and Tel-Aviv University devised amethodto make a diffusion-based, text-to-image generator produce pictures of a particular object or in a particular style.
Basics of diffusion models:During training, a text-to-image generator based on diffusion takes a noisy image and a text description. A transformer learns to embed the description, and a diffusion model learns to use the embeddings to remove the noise in successive steps. At inference, the system starts with pure noise and a text description, and iteratively removes noise according to the text to generate an image. A variant known as alatent diffusion modelsaves computation by removing noise from a small, learned vector of an image instead of a noisy image.
Key insight:A text-to-image generator feeds text word embeddings to an image generator. Adding a learned embedding that represents a set of related images can prompt the generator to produce common attributes of those images in addition to the semantic content of words.
How it works:The authors used atext-to-image generatorbased on a latent diffusion model. The system was pretrained on400 million text-image pairsscraped from the web. Its weights were frozen.
Results:The authors evaluated their model’s output by comparing embeddings, generated byCLIP, of original and generated images. They measured similarity on a scale from 0 to 1, where 1 signifies two identical inputs. The model scored around 0.78. Images generated using human-crafted descriptions of up to 12 words — without reference to S∗ — scored around 0.6. Images generated using longer descriptions of up to 30 words scored around 0.625.
Why it matters:The authors’ method offers a simple way for users of diffusion-based, text-to-image generators to steer the output toward specific attributes of content or style without retraining the model.
We’re thinking:Could this approach be extended to encompass multiple learned vectors and allow users to combine them as they like? That would make it possible to control image generation in even more precise ways.
""You don’t have to be a mathematician to have a feel for numbers,"" said mathematician John Forbes Nash, Jr. Get a feel for the numbers withMathematics for Machine Learning and Data Science, our new specialization.Join the waitlist
","['https://dl-staging-website.ghost.io/content/images/2023/01/DeepLearning_Mathematics_Campaign_Quotes_Nash.png', 'https://dl-staging-website.ghost.io/content/images/2023/01/unnamed--25-.gif']"
AI as Officemate,"Many workers benefit from AI in the office without knowing it, a new study found.
What’s new:MIT Sloan Management Review and Boston Consulting Groupsurveyedemployees on their use of AI in their day-to-day work. Their findings: The technology offers benefits to individuals and organizations, but employers may need to educate and direct workers to realize them.
What it says:The authors surveyed 1,741 respondents in over 20 industries and 100 countries. They also interviewed 17 executives about how AI is used in their organizations.
Consumer vs. pro products:The authors polled respondents on their use of AI products in four categories.
Behind the news:A recent studysupportsthe notion that AI bolsters workers more than it replaces them. Employment rates rose between 2008 and 2018 in a number of professions subject to AI-powered automation including fast food worker, translator, and financial advisor.
Why it matters:Many workers justifiably worry that AI will make their jobsobsolete. This survey suggests instead that AI is broadly enhancing many workers’ jobs.We’re thinking:It's not necessarily bad that many people don’t recognize AI’s role in their everyday lives. Successful technology often disappears into the background. We talk about turning on lights, not electric lights, because electricity works so well that we take it for granted. If AI is the new electricity, we can expect it to be taken for granted, too.
",['https://dl-staging-website.ghost.io/content/images/2023/01/unnamed--26-.gif']
Get Ready for 2023!,"Spring came early in 2022, as what some observers had feared was an impending AI Winter melted into a garden of innovations with potential uses in fields as diverse as art, genomics, and chip design. Dark clouds lingered; generative models continued to produce problematic output, and international tensions flared as the U.S. took steps to block China’s access to AI chips. Yet optimism was palpable in social media, conference proceedings, and venture investment, and the next 12 months promise an abundance of AI progress. In this special issue ofThe Batch, leaders in the field share their hopes for the coming year.
",['https://dl-staging-website.ghost.io/content/images/2022/12/unnamed--13-.png']
Yoshua Bengio: Models That Reason,"Recent advances in deep learning largely have come by brute force: taking the latest architectures and scaling up compute power, data, and engineering. Do we have the architectures we need, and all that remains is to develop better hardware and datasets so we can keep scaling up? Or are we still missing something?
I believe we’re missing something, and I hope for progress toward finding it in the coming year.
I’ve been studying, in collaboration with neuroscientists and cognitive neuroscientists, the performance gap between state-of-the-art systems and humans. The differences lead me to believe that simply scaling up is not going to fill the gap. Instead, building into our models a human-like ability to discover and reason with high-level concepts and relationships between them can make the difference.
Consider the number of examples necessary to learn a new task, known as sample complexity. It takes a huge amount of gameplay to train a deep learning model to play a new video game, while a human can learn this very quickly. Related issues fall under the rubric of reasoning. A computer needs to consider numerous possibilities to plan an efficient route from here to there, while a human doesn’t.
Humans can select the right pieces of knowledge and paste them together to form a relevant explanation, answer, or plan. Moreover, given a set of variables, humans are pretty good at deciding which is a cause of which. Current AI techniques don’t come close to this human ability to generate reasoning paths. Often, they’re highly confident that their decision is right, even when it’s wrong. Such issues can be amusing in a text generator, but they can be life-threatening in a self-driving car or medical diagnosis system.
Current systems behave in these ways partly because they’ve been designed that way. For instance, text generators are trained simply to predict the next word rather than to build an internal data structure that accounts for the concepts they manipulate and how they are related to each other. But I think we can design systems that track the meanings at play and reason over them while keeping the numerous advantages of current deep learning methodologies. In doing so, we can address a variety of challenges from excessive sample complexity to overconfident incorrectness.
I’m excited by generative flow networks, orGFlowNets, an approach to training deep nets that my group started about a year ago. This idea is inspired by the way humans reason through a sequence of steps, adding a new piece of relevant information at each step. It’s like reinforcement learning, because the model sequentially learns a policy to solve a problem. It’s also like generative modeling, because it can sample solutions in a way that corresponds to making a probabilistic inference.
If you think of an interpretation of an image, your thought can be converted to a sentence, but it’s not the sentence itself. Rather, it contains semantic and relational information about the concepts in that sentence. Generally, we represent such semantic content as a graph, in which each node is a concept or variable. GFlowNets generate such graphs one node or edge at a time, choosing which concept should be added and connected to which others in what kind of relation.
I don’t think this is the only possibility, and I look forward to seeing a multiplicity of approaches. Through a diversity of exploration, we’ll increase our chance to find the ingredients we’re missing to bridge the gap between current AI and human-level AI.
Yoshua Bengio is a professor of computer science at Université de Montréal and scientific director of Mila - Quebec AI Institute. He received the 2018 A.M. Turing Award, along with Geoffrey Hinton and Yann LeCun, for his contribution to breakthroughs in deep learning.
",['https://dl-staging-website.ghost.io/content/images/2022/12/unnamed--14-.png']
Alon Halevy: Your Personal Data Timeline,"The important question of how companies and organizations use our data has received a lot of attention in the technology and policy communities. An equally important question that deserves more focus in 2023 is how we, as individuals, can take advantage of the data we generate to improve our health, vitality, and productivity.
We create a variety of data throughout our days. Photos capture our experiences, phones record our workouts and locations, Internet services log the content we consume and our purchases. We also record our want-to lists: desired travel and dining destinations, books and movies we plan to enjoy, and social activities we want to pursue. Soon smart glasses will record our experiences in even more detail. However, this data is siloed in dozens of applications. Consequently, we often struggle to retrieve important facts from our past and build upon them to create satisfying experiences on a daily basis.
But what if all this information were fused in a personal timeline designed to help us stay on track toward our goals, hopes, and dreams? The idea is not new. Vannevar Bush envisioned it in 1945, calling it a memex. In the 90’s, Gordon Bell and his colleagues at Microsoft Research built MyLifeBits, a prototype of this vision. The prospects and pitfalls of such a system have been depicted in film and literature.
Privacy is obviously a key concern in terms of keeping all our data in a single repository and protecting it against intrusion or government overreach. Privacy means that your data is available only to you, but if you want to share parts of it, you should be able to do it on the fly by uttering a command such as, “Share my favorite cafes in Tokyo with Jane.” No single company has all our data or the trust to store all our data. Therefore, building technology that enables personal timelines should be a community effort that includes protocols for the exchange of data, encrypted storage, and secure processing.
Building personal timelines will also force the AI community to pay attention to two technical challenges that have broader application.
The first challenge is answering questions over personal timelines. We’ve made significant progress on question answering over text and multimodal data. However, in many cases, question answering requires that we reason explicitly about sets of answers and aggregates computed over them. This is the bread and butter of database systems. For example, answering “what cafes did I visit in Tokyo?” or “how many times did I run a half marathon in under two hours?” requires that we retrieve sets as intermediate answers, which is not currently done in natural language processing.  Borrowing more inspiration from databases, we also need to be able to explain the provenance of our answers and decide when they are complete and correct.
The second challenge is to develop techniques that use our timelines, responsibly, for improved personal well-being. Taking inspiration from the field of positive psychology, we can all flourish by creating positive experiences for ourselves and adopting better habits. An AI agent that has access to our previous experiences and goals can give us timely reminders and suggestions of things to do or avoid.
Ultimately, what we choose to do is up to us, but I believe that an AI with a holistic view of our day-to-day activities, better memory, and superior planning capabilities would benefit everyone.
Alon Halevy is a director at the Reality Labs Research branch of Meta. His hopes for 2023 represent his personal opinion and not that of Meta.
",['https://dl-staging-website.ghost.io/content/images/2022/12/unnamed--15-.png']
"Douwe Kiela: Less Hype, More Caution","This year we really started to see AI go mainstream. Systems like Stable Diffusion and ChatGPT captured the public imagination to an extent we haven’t seen before in our field. These are exciting times, and it feels like we are on the cusp of something great: a shift in capabilities that could be as impactful as — without exaggeration — the industrial revolution.
But amidst that excitement, we should be extra wary of hype and extra careful to ensure that we proceed responsibly.
Consider large language models. Whether or not such systems really “have meaning,” lay people will anthropomorphize them anyway, given their ability to perform arguably the most quintessentially human thing: to produce language. It is essential that we educate the public on the capabilities and limitations of these and other AI systems, especially because the public largely thinks of computers as good old-fashioned symbol-processors — for example, that they are good at math and bad at art, while currently the reverse is true.
Modern AI has important and far-reaching shortcomings. Systems are too easily misused or abused for nefarious purposes, intentionally or inadvertently. Not only do they hallucinate information, they do so with seemingly very high confidence and without the ability to attribute or credit sources. They lack a rich-enough understanding of our complex multimodal human world and do not possess enough of what philosophers call “folk psychology,” the capacity to explain and predict the behavior and mental states of other people. They are arguably unsustainably resource-intensive, and we poorly understand the relationship between the training data going in and the model coming out. Lastly, despite the unreasonable effectiveness of scaling — for instance, certain capabilities appear to emerge only when models reach a certain size — there are also signs that with that scale comes even greater potential for highly problematic biases and even less-fair systems.
My hope for 2023 is that we’ll see work on improving all of these issues. Research on multimodality, grounding, and interaction can lead to systems that understand us better because they understand our world and our behavior better. Work on alignment, attribution, and uncertainty may lead to safer systems less prone to hallucination and with more accurate reward models. Data-centric AI will hopefully show the way to steeper scaling laws, and more efficient ways to turn data into more robust and fair models.
Finally, we should focus much more seriously on AI’s ongoing evaluation crisis. We need better and more holistic measurements — of data and models — to ensure that we can characterize our progress and limitations, and understand, in terms ofecological validity(for instance, real-world use cases), what we really want out of these systems.
Douwe Kiela is an adjunct professor in symbolic systems at Stanford University. Previously, he was the head of research at Hugging Face and a research scientist at Facebook AI Research.
In 2022 our amazing Pie & AI ambassadors hosted over 100 events in 66 cities around the globe! Here is a heartfelt thank you to all of them, from everyone at DeepLearning.AI. Read some of their experienceshere
","['https://dl-staging-website.ghost.io/content/images/2022/12/ezgif.com-gif-maker--7-.jpg', 'https://dl-staging-website.ghost.io/content/images/2022/12/unnamed--16-.png']"
Been Kim: A Scientific Approach to Interpretability,"It’s an exciting time for AI, with fascinating advances in generated media and many other applications, some even in science and medicine. Some folks may dream about what more AI can create and how much bigger models we may engineer. While those directions are exciting, I argue that we need to pursue much less flashy work: going back to the basics and studying AI models as targets of scientific inquiry.
Why and how? The field of interpretability aims to create tools to generate “explanations” for the output of complex models. This field has emerged naturally out of a need to build machines that we can have a dialog with: What is your decision? Why did you decide that? For example, a tool takes an image and a classification model, and generates explanations in the form of weighted pixels. The higher a pixel’s weight, the more important it is. For instance, the more its value affects the output, the more important it may be — but the definition of importance differs depending on the tool.
While there are some successes, many tools have turned out to behave in ways we did not expect. For example, it has been shown that explanations of an untrained model are quantitatively and qualitatively indistinguishable to those of a trained model. (Then what does the explanation explain?) Explanations often change with small changes in the input despite resulting in the same output. In addition, there isn’t much causal relationship between a model’s output (what we are trying to explain) and the tool’s explanation. Other work shows that good explanations of a model’s output don't necessarily have a positive influence on how people use the model.
What does this mismatch between expectation and outcome mean, and what should we do about it? It suggests that we need to examinehowwe build these tools.
Currently we take an engineering-centric approach: trial and error. We build tools based on intuition (for instance, explanations would be more intuitive for humans if we generate a weight per a chunk of pixels instead of individual pixels). While the engineering-centric approach is useful, we also need fundamental principles (what can be called science) to build better tools.
In developing drugs, for instance, trial and error is essential (say, testing a new medicine through rigorous clinical trials before deploying it), but it goes hand-in-hand with sciences like biology and genetics. While science has many gaps in understanding how the human body works, it provides fundamental principles in creating the tool (in this case, drugs). In other words, pursuing both science and engineering simultaneously, such that each can inform the other, has shown to be a successful way to work with complex beings (humans).
The field of machine learning needs to study our complex aliens (models) like other disciplines study humans. How would such study of these aliens help interpretability? Here’s an example. A team at the University of Tübingenfoundthat neural networks see texture (say, an elephant’s skin) more than shape (an elephant’s outline). Even if we see an elephant’s contour in the explanation of an image — perhaps in the form of collective highlighted pixels — the study informs us that the model may not be seeing the shape but rather the texture. This is called inductive bias — a tendency of a particular class of models due to either its architecture or the way we optimize it. Revealing such tendencies can help us understand this alien, just as revealing a human’s tendency (bias) can be used to understand human behavior (such as unfair decisions).
In this way, the methods often used to understand humans can also help us understand AI models. These include observational studies (say,observing multi-agents from afar to infer emerging behaviors), controlled studies (for instance,intervening in a multi-agent system to elicit underlying behaviors), and surgery (such asexamining the internals of the superhuman chess player AlphaZero). For AI models, thanks to the way their internals are built — they are made of math! — we have one more tool: theoretical analysis. Work along this direction has already yielded exciting theoretical results on the behaviors of models, optimizers, and loss functions. Some take advantage of classical tools in statistics, physics, dynamical systems, or signal processing.  Many tools from different fields are yet to be explored in the study of AI.
Pursuing science doesn’t mean we should stop engineering. The two go hand in hand: Science will enable us to build tools under principles and knowledge, while engineering enables science to become practical. Engineering can also inspire science: What works well in practice can provide hints to structures of models that we wish to formalize in science, just like the high-performance of convolutional networks in 2012 inspired many theory papers that tried to analyze why convolutions help generalization.
I’m excited to enter 2023 and many other years to come as we advance our understanding of our aliens and invent ways to communicate with them. By enabling a dialogue, we will enable richer collaborations and better leverage the complementary skill sets of humans and machines.
Been Kim is a research scientist at Google Brain. Her work on helping humans to communicate with complex machine learning models won the UNESCO Netexplo award.
",['https://dl-staging-website.ghost.io/content/images/2022/12/unnamed--17-.png']
A Dazzling Year in AI,"As we settle into a cup of hot cocoa and badger ChatGPT to suggest holiday gifts for our loved ones, we reflect on a year of tremendous advances in AI. Systems that generate human-like text, images, and code — with video and music on the horizon — delighted users even as they raised questions about the future of creativity. Models that decode chemistry and physics drove scientific discovery, while governments moved to control the supply of specialized microprocessors that make such innovations possible. While such developments give uspause, in this special issue ofThe Batch— asinpastyearsat this season — we survey the marvels wrought by AI in 2022.
",['https://dl-staging-website.ghost.io/content/images/2022/12/ezgif.com-gif-maker--2-.jpg']
Synthetic Images Everywhere,"Pictures produced by AI went viral, stirred controversies, and drove investments.What happened:A new generation of text-to-image generators inspired a flood of experimentation, transforming text descriptions into mesmerizing artworks and photorealistic fantasies. Commercial enterprises were quick to press the technology into service, making image generation a must-have feature in software for creating and editing graphics.Driving the story:Models that generate media became the public face of AI thanks to friendly user interfaces, highly entertaining output, and open APIs and models.
Yes, but:Such models are trained on images scraped from the web. Like large language models, they inherit biases embedded in online content and imitate the inflammatory styles of expression.
Behind the news:Diffusion models generate output by starting with noise and removing it selectively over a series of steps.Introducedin 2015 by researchers at UC Berkeley and Stanford, they remained in the background for several years until furtherworkshowed that they could produce images competitive with the output of generative adversarial networks (GANs). Stability AI put a diffusion model at the heart of Stable Diffusion. OpenAI, which based the initial version of DALL·E on a GAN, updated it with a diffusion model at around the same time.
Where things stand:The coming year is shaping up for a revolution in computer-aided creativity. And the groundswell of generated imagery isn’t going to stop at pictures. Google and Meta released impressivetext-to-videomodelsthis year, and OpenAI acceleratedtext-to-3D-objectgeneration by an order of magnitude.
",['https://dl-staging-website.ghost.io/content/images/2022/12/ezgif.com-gif-maker--3-.jpg']
Programmer’s Best Friend,"Behind schedule on a software project? There’s an app for that.What happened:Language models fine-tuned on computer code proved capable of generating software routines similar to the work of experienced developers — though the results can be hit-or-miss.
Driving the story:AI-powered code generators made their way into large companies, and even small-time developers (and non-developers) gained access to them.
Behind the news:Users of OpenAI’s GPT-3 language model showed that it couldgenerate working codeas early as mid-2020. A year later, OpenAI introduced a fine-tuned version known asCodex, which serves as the foundation for GitHub's Copilot.
Yes, but:The widely available versions of this technology aren’t yet able to write complex programs. Often their output looks right at first glance but turns out to be buggy. Moreover, their legal status may be in jeopardy. A class-action lawsuit against GitHub, OpenAI, and Microsoft claims that the training of Codex violated open source licensing agreements. The outcome could have legal implications for models that generate text, images, and other media as well.
Where things stand:AI-powered coding tools aren’t likely to replace human programmers in the near future, but they may replace the tech question-and-answer site Stack Overflow as the developer’s favorite crutch.
",['https://dl-staging-website.ghost.io/content/images/2022/12/ezgif.com-gif-maker--4-.jpg']
AI’s Eyes Evolve,"Work on vision transformers exploded in 2022.
What happened:Researchers publishedan abundance of ViT papersduring the year. A major theme: combining self-attention and convolution.
Driving the story:A team at Google Brain introducedvision transformers(ViTs) in 2020, and the architecture has undergone nonstop refinement since then. The latest efforts adapt ViTs to new tasks and address their shortcomings.
Behind the news:While much ViT research aims to surpass and ultimately replace convolutional neural networks (CNNs), the more potent trend is to marry the two. The ViT’s strength lies in its ability to consider relationships between all pixels in an image at small and at large scales. One downside is that it needs additional training to learn in ways that are baked into the CNN architecture after random initialization. CNN’s local context window (within which only local pixels matter) and weight sharing (which enables it to process different image locations identically) help transformers to learn more from less data.
Where things stand:The past year expanded the Vision Transformer’s scope in a number of applications. ViTsgenerated plausible successive video frames,generated 3D scenes from 2D image sequences, anddetected objects in point clouds. It's hard to imagine recent advances in text-to-image generators based on diffusion models without them.
Mathematics for Machine Learning and Data Scienceis our next specialization. Set to launch in January 2023, it’s a beginner-friendly way to master the math behind AI algorithms and data analysis techniques.Join the waitlistand be among the first to enroll!
","['https://dl-staging-website.ghost.io/content/images/2022/12/unnamed-3--1-.png', 'https://dl-staging-website.ghost.io/content/images/2022/12/ezgif.com-gif-maker--5-.jpg']"
"Language Models, Extended","Researchers pushed the boundaries of language models to address persistent problems of trustworthiness, bias, and updatability.What happened:While many AI labs aimed to make large language models more sophisticated by refining datasets and training methods — including methods that trained a transformer totranslate 1,000 languages— others extended model architectures to search the web, consult external documents, and adjust to new information.
Driving the story:The capacity of language models to generate plausible text outstrips their ability to discern facts and resist spinning fantasies and expressing social biases. Researchers worked to make their output more trustworthy and less inflammatory.
Behind the news:Amid the progress came a few notable stumbles. The public demo Meta’s Galactica, a language model trained to generate text on scientific and technical subjects, lasted three days in November before its developers pulled the plug due to its propensity to generate falsehoods and cite nonexistent sources. In August, the chatbot BlenderBot 3, also from Meta, quickly gained a reputation for spouting racist stereotypes and conspiracy theories.
Where things stand:The toolbox of truth and decency in text generation grew substantially in the past year. Successful techniques will find their way into future waves of blockbuster models.
",['https://dl-staging-website.ghost.io/content/images/2022/12/ezgif.com-gif-maker--6-.jpg']
Drive Different,"Apple is redrawing the road map for its self-driving car.
What's new:The company is redesigning an autonomous car that has been in development for nearly a decade,Bloombergreported. Originally intended to be fully autonomous under all conditions, the redesigned vehicle will allow for a human driver.
Downshift:Apple had scheduled the vehicle, code named Titan, for 2025, anonymous insiders said. However, executives realized earlier this year that they couldn’t meet the deadline and decided to scale back the autonomous features. The new timeline calls for a prototype by 2024, testing through 2025, and launch in 2026. The target price is under $100,000, a markdown from the original $120,000. The company is currently testing its semi-autonomous system on Lexus SUVs in several U.S. states.
Behind the news:Fully self-driving cars on the open road remain limited to a few robotaxi deployments inChinaand theUnited States. Meanwhile, the industry has suffered a series of setbacks. Fordshut downArgo, its joint project with Volkswagen. Tesla’s purported Full Self-Driving optionrequiresa human in the loop. Further development is required to enable such vehicles to drive safely despite challenges likeroad construction and snow.
Why it matters:Commercializing fully autonomous vehicles is a tantalizing but elusive goal. Apple’s decision to downshift for the sake of bringing a product to market suggests that human drivers will sit behind the wheel for the foreseeable future.
We're thinking:Full self-driving cars have been five years away for the past decade. The challenge of handling the long tail of rare but critical events has been a persistent issue. Upcoming developments such as foundation models for computer vision are likely to make a substantial difference. We don't know when, but we're confident that the future includes full autonomy.
",['https://dl-staging-website.ghost.io/content/images/2022/12/WORLDCUP_Crop_600px.gif']
The World Cup’s AI Referee,"The outcome of the FIFA World Cup 2022 depends on learning algorithms.
What's new:The quadrennial championship tournament of football (known as soccer in the United States), which wraps up this week, is using machine learning to help human arbitersspot players who break a rulethat governs their locations on the field.
How it works:The off-side rule requires that, when receiving a pass, members of the team that possesses the ball keep two opposing players between them and their opponents’ goal. Referees often call off-side erroneously depending on their vantage point on the field. FIFAintroduceda Video Assisted Review system in 2018. The machine learning capabilities help human assistants in a remote video center identify violations.
Behind the news:AI is watching activity off the pitch as well. Qatari authorities use face recognition tomonitorfans for unruly behavior. Authorities also use computer vision totrackcrowd size and movement to prevent the violence and crowd crushes that havemarredrecent matches.
Controversy:The semi-automated offside detection system has beencriticizedby players who say its role in referee decisions is unclear.
Why it matters:Players and fans alike expect referees to be both objective and omnipresent — which is, of course, impossible for anyone to accomplish. AI isn’t a perfect substitute, but it allows officials to observe the action at an unprecedented level of detail.
We're thinking:If FIFA hasn’t come up with a name for the system, we humbly suggest: Football Net.
Announcing our newest specialization!Mathematics for Machine Learning and Data Scienceis carefully designed to help you understand the fundamentals behind common algorithms and data analysis techniques. Scheduled to launch in January 2023!Join the waitlist
","['https://dl-staging-website.ghost.io/content/images/2022/12/unnamed-3.png', 'https://dl-staging-website.ghost.io/content/images/2022/12/LENSA_1200px.gif']"
Avatars Gone Wild,"A blockbuster app produces sexualized avatar images, even when the original portraits were safe for work.
What's new:Lensa AI, a photo editor that turns face photos into artistic avatars, sometimes generates sexualized images from plain selfies, according to several independent reports. It can also be manipulated to produce more explicit imagery, raising concerns that it may be used to victimize people by generating lewd images of their likeness.
How it works:Users upload 10 to 20 photos and choose a gender. The app uses the open source Stable Diffusion image generator to produce images in various art styles including fantasy, comic-book, and faux-3D rendering. Users must buy a $36 annual subscription to use the image generator, which costs an additional $3.99 for 50 images, $5.99 for 100, or $7.99 for 200. The terms of service disallow nudes and photos of minors, and the app requests that users verify that they are adults.
NSFW:Journalists conducted tests after hearing complaints from users.
Behind the news:Image generators based on neural networks have churned out nonconsensual nude depictions of real people at leastsince 2017. Open-source and free-to-use models have made it easier for the general public to create such images. In November, Stability AI, developer of Stable Diffusion,releaseda version trained on a dataset from which sexual images had been removed.
Why it matters:Text-to-image generators have hit the mainstream: Lensa was the Apple Store’s top download last week, and three similar apps were in the top 10. People who fear deepfakes now have cause for a once-hypothetical concern: Anybody who has access to photos of another person could hijack their images.
We're thinking:Image generation has widespread appeal and it’s easy to use. That’s no excuse for misusing it to degrade or harass people. Creating or sharing a nude depiction of someone without their permission is never okay.
",['https://dl-staging-website.ghost.io/content/images/2022/12/unnamed--22-.gif']
"More Plausible Text, Familiar Failings","Members of the AI community tested the limits of the ChatGPT chatbot, unleashing an avalanche of tweets that made for sometimes-great, sometimes-troubling entertainment.
What’s new:OpenAI launched a public demo ofChatGPT, the latest in the research lab’s line of large language models. Like its predecessors, ChatGPT generates text in a variety of styles, for a variety of purposes. Unlike them, it does so with greater finesse, detail, coherence, and — dare we say it? — personality. (How else to characterize a model thatapologizesfor its misbehavior?) One million users havesigned upsince the launch last Wednesday.
How it works:ChatGPT is a next-generation language model (of a class referred to as GPT-3.5) trained in the manner of OpenAI’s earlierInstructGPT, but on conversations. It was fine-tuned to minimize harmful, untruthful, or biased output using a combination of supervised learning and what OpenAI callsreinforcement learning from human feedback, in which humans rank potential outputs and a reinforcement learning algorithm rewards the model for generating outputs similar to those that rank highly.
Strengths and weaknesses:Like other recent language models, ChatGPT’s output veers between stunningly brilliant and mind-numbingly stupid.
Behind the news:ChatGPT arrived one week after Meta withdrewGalactica, a model designed to generate scientific papers. Galactica was promoted as an aid to researchers aiming to publish their findings, but users of the public demo prompted it to generate sober dissertations on nonsensical topics like land squid and the health benefits of ingesting ground glass.
Why it matters:Speech is among the simplest and most convenient ways for humans to communicate. Programs that grasp what they’re told and respond with meaningful information will open a wide range of everyday functions. Closer to home, many observers proposed ChatGPT or something like it as a superior alternative to current web search. First, though, researchers face the steep challenge of building a language model that doesn’t make up facts and ignore limits on its output.
We’re thinking:Sometimes technology is overhyped — reinforcement learning, after solving Atari games, may be an example — but large language models are likely to find a place in significant applications. Meanwhile, many details remain to be worked out and the AI community must strive to minimize potential harm.
",['https://dl-staging-website.ghost.io/content/images/2022/12/unnamed--9-.jpg']
Cryptocurrency Unsafe for AI,"The demise of cryptocurrency exchange FTX threatens funding for some teams devoted to AI safety.
What’s new:FTX, the $32 billion exchange that plunged into bankruptcy last month amid allegations of fraud, had given or promised more than $530 million to over 70 AI-related organizations,The New York Timesreported. Much of that money may have to be returned.
What happened:FTX founder Sam Bankman-Fried and his associates used the exchange’s holdings to dole out grants or investments to AI-related startups, labs, and think tanks, many of them focused on AI safety. People associated with these groups anonymously expressed concerns that their funding would be clawed back in bankruptcy proceedings.
Behind the news:Bankman-Fried co-founded FTX in 2019 to enable users to trade cryptocurrency for conventional money and other assets. A November report by CoinDesk, a cryptocurrency news outlet,describeda potential conflict of interest between FTX and another trading firm also owned by Bankman-Fried. The news prompted users to withdraw their funds, much of which FTX had already spent, invested, given away, or promised to others. The exchange filed for bankruptcy. U.S. prosecutors and regulators areinvestigatingpotential wrongdoing.Why it matters:It’s crucial to minimize potential harm caused by AI, but organizations devoted to that goal may not receive the funding they need from corporate entities or cash-strapped academic institutions. Organizations that were counting on FTX may find support elsewhere, but many now face an uncertain future.
We’re thinking:We’re grateful for donors who are willing to support AI research of all kinds. At the same time, we’re appalled by the scope and brazenness of FTX’s deceit. Sadly, organizations that seek funding must vet potential donors carefully.
Join Sebastián Ramírez, the creator of FastAPI, to build your own AI image-generation web app. “FastAPI for Machine Learning: Live Coding an ML Web Application” takes place on December 15, 2022, at 9:00 a.m. Pacific Time.RSVP
","['https://dl-staging-website.ghost.io/content/images/2022/12/Imagen5.jpg', 'https://dl-staging-website.ghost.io/content/images/2022/12/unnamed--19-.gif']"
How Alexa Says Goodnight,"Too exhausted (or unimaginative) to tell your child a bedtime story? Amazon’s smart displays can spin bespoke tales on demand.What’s new:A feature called Create with Alexagenerateschildren’s stories complete with illustrations, music, and sound effects on the Amazon Echo Show device.
How it works:The screen presents a series of prompts that provide a setting (such as “space exploration” or “enchanted forest”), main character (such as an astronaut or an alien), principal color, and tone (such as “happy” or “mysterious”).
Behind the news:Amazon is under pressure to revitalize its 10-year-old Echo line. The devices, which have been sold at a loss on the theory that they would spur purchases of other goods,lost$10 billion in 2022 alone, and the division responsible for the Alexa softwarefacessteep layoffs.Why it matters:AI models that generate text, images, video, and music are having abanner year. Alexa’s storytelling feature coordinates several generative models into a coherent whole. Whether it will spur sales is a tale for another time.We’re thinking:Once upon a time, there was a boy in a blue shirt who dreamed of changing the world with AI. . . .
",['https://dl-staging-website.ghost.io/content/images/2022/12/unnamed--20-.gif']
Translating a Mostly Oral Language,"Most speech-to-speech translation systems use text as an intermediate mode. So how do you build an automated translator for a language that has no standard written form? A new approach trained neural networks to translate a primarily oral language.
What’s new:Peng-Jen Chen, Kevin Tran, Yilin Yang and teammates at Meta described a system thattranslates speech between English and Hokkien, which is spoken by millions of people in east Asia.
Key insight:Few people know how to translate between English and Hokkien, which makes it hard to assemble a dataset sufficient for training an English-Hokkien translation model. However, a fair number of people can translate between Mandarin and English and between Mandarin and Hokkien. By translating from English to Mandarin and from Mandarin to Hokkien, it’s possible to build a database of English-Hokkien speech pairs.
The dataset:The authors collected a corpus of English, Mandarin, and Hokkien data. They employed human translators to translate the corpus. They used the translated corpus to synthesize further data.
The translators:Separate speech-to-speech systems with identical architectures translate from Hokkien to English and English to Hokkien, using Mandarin text as a stepping stone between the target languages.
Results:The authors compared their system to a baseline of their own design that translated directly between the spoken languages using an encoder-decoder. They evaluated the systems according to ASR-BLEU, which compares text overlap (higher is better) against reference text after translating speech to text. To render Hokkien speech as text for comparison, they developed a separate model that translated Hokkien speech into a phonetic script called Tâi-lô. Converting English to Hokkien, their system achieved 7.3 ASR-BLEU, whereas the baseline achieved 6 ASR-BLEU. Converting Hokkien to English, their system achieved 12.5 ASR-BLEU, whereas the baseline achieved 8.1 ASR-BLEU. Without the augmented data, both their system and the baseline scored worse by 6 ASR-BLEU to 9 ASR-BLEU.
Why it matters:Forty percent of the world’s languages have no standard written form, which means they’re left out of current translation systems.  This method provides a blueprint for machine translation of other primarily oral languages.
Yes, but:Hokkien is spoken in several dialects, some of which are mutually unintelligible. So, while this system presumably serves most Hokkien speakers, it doesn’t serve all of them yet.
We’re thinking:The next step is to hook up the Hokkein-English model to existing translators for other languages. Is it good enough? ASR-BLEU scores in the 7-to-12 range are low compared to scores for, say, English-German, which are around 30. And, because translation errors compound from one language to the next, the more intermediate steps required to reach the target language, the lower the final translation quality. One way or another, we want to hear Hokkien speakers talking to everyone!
",['https://dl-staging-website.ghost.io/content/images/2022/11/BILLBOARDS-2_1200px--1-.gif']
Billboards Are Watching,"AI-driven signs are deciding what to display based on data harvested from passersby.
What’s new:Companies that sell advertising in public spaces use face analysis and personal data to match ads with potential viewers in real time, civil-liberties watchdog Big Brother UK detailed in a newreport.
How it works:The report compiles applications and case studies drawn from product summaries and blog posts published by ad-tech companies.
Behind the news:These companies walk a tightrope over local privacy protections. Adsquare, Alfi, and Quividi tout their compliance with Europe’sGeneral Data Protection Regulation(GDPR), which protects privacy in member countries. Last year, U.S. lawmakerssentletters of concern to Lyft and Uber after some drivers independently put Alfi-equipped advertising screens in their vehicles. Both ride-share companies responded that equipment installed by drivers was beyond their control.Why it matters:The combination of electronic signage, computer vision, and cloud computing brings to the real world practices that are common in advertising on the internet.
We’re thinking:Online advertising has flourished as increased personalization allowed more precise targeting. Public advertising is poised to do the same.
What do you think about coding tests and technical interviews? We’d like to hear the good, the bad, and the ugly. Tell us about your technical interview experiences and earn a chance to win a $100 gift card!Take the survey
","['https://dl-staging-website.ghost.io/content/images/2022/11/ezgif-1-f144e58736.jpg', 'https://dl-staging-website.ghost.io/content/images/2022/11/ezgif.com-gif-maker--21--1.gif']"
Algorithms Control the Capital,"A new report offers a rare peek into the use of automated decision-making tools by the government of a major city.What’s new:Municipal agencies in the U.S. capital of Washington, D.C., use at least 29 algorithms to streamline their operations, according to astudyby the Electronic Privacy Information Center. The authors found references to the models in public records and internal documents. In many cases, their roles were not widely known.How it works:The algorithms span a variety of municipal functions.
Behind the news:Washington, D.C. lawmakers are considering alawthat would require regular audits of decision-making algorithms used by organizations of a particular size and those that hold data on city residents. It would also enable the Office of the Attorney General and others to sue for violations.
Yes, but:While the authors discovered many automated decision-making systems in use, many more may be hidden from view. Several city agencies didn’t respond to requests for public records citing confidentiality and trade-secret agreements with vendors. New York City police were found to be using more algorithms than those the department had disclosed to officials as required by a 2020 law,Wiredreported. Public registries inAmsterdam and Helsinkilist only 10 out of 30 algorithms that have beendisclosedin separate documents.
Why it matters:AI is reaching into a wide variety of government functions that have a direct and substantial impact on citizens’ lives. While the technology can help officials make decisions that are more efficient and sometimes more fair, their constituents need to know how their government operates and have a right to hold algorithms (and the officials who employ them) accountable for their decisions. Governments should supply this information as a matter of course, rather than forcing independent researchers to discover it.
We’re thinking:The term “smart city” shouldn’t just describe the algorithms used to govern the municipality. It should also describe a population that’s informed about how they’re being used.
",['https://dl-staging-website.ghost.io/content/images/2022/11/INSTANTNERF.gif']
Creatives Fight Back,"Artists are rebelling against AI-driven imitation.What’s new:DeviantArt, an online community where artists display and sell their work and marketplace for digital art,launchedDreamUp, a text-to-image generator that aims to help artists thwart attempts to imitate their styles or works.How it works:DreamUp is avanilla implementationof the open sourceStable Diffusiontext-to-image generator.
Opting out:Stable Diffusion was trained on images scraped from the web including works from DeviantArt. Upon its release, some artistsobjectedto the model’s ability to replicate their style via prompts like, “in the style of ____.”
Behind the news:AI’s increasing ability to mimic the styles of individual artists has become a flashpoint between engineers and artists. When acclaimed artist Kim Jung Gidiedin early October, within one day a former game developerreleaseda model trained to produce works in his style. While the developer justified the work “as an homage,” responses included not only criticism and insults but also threats of violence. Such comments, one commenter noted, were part of a recent rise in “extremely violent rhetoric directed at the AI art community.”
Why it matters:Generative AI is attracting attention andfunding, but the ethics of training and using such systems are still coming into focus. For instance, lawyers arepreparingto argue that GitHub’s CoPilot code-generation system, which was trained on open-source code, violates open-source licenses by improperly crediting coders for their work. The outcome may resolve some uncertainty about how to credit a generative model’s output — but it seems unlikely to address issues of permission and compensation.
We’re thinking:Artists who have devoted years to developing a distinctive style are justifiably alarmed to see machines crank out imitations of their work. Some kind of protection against copycats is only fair. For the time being, though, the limit of fair use in training and using AI models remains an open question.
",['https://dl-staging-website.ghost.io/content/images/2022/11/unnamed--17-.gif']
Built to Scale,"A new computing cluster delivers more bang per chip.
What’s new:Cerebras, one of several startups vying to supply the market for specialized AI chips,unveiledAndromeda, a supercomputer based on its processors. Unlike conventional clusters, which incur data bottlenecks as processors are added, the system’s processing speed rises linearly with additional processors.How it works:Andromeda comprises 16 CerebrasCS-2 Wafer Scale Enginechips. Each chip holds 850,000 processing cores (more than 100 times the number found on anNvidia A100) on a silicon disc that measures 21.5 centimeters across.
Speed tests:Scientists at Argonne National Laboratory used the system totrainGenSLM language models in several sizes. Increasing the number of processors from one to four boosted throughput nearly linearly while training models of 123 million parameters and 1.3 billion parameters. Going from one to four chips also cut the smaller model’s training time from 4.1 to 2.4 hours and cut the larger model’s training time to 15.6 to 10.4 hours.
Behind the news:As interest rates rise, AI chip startups are facing headwinds in raising enough capital to support their often huge expenses.
Why it matters:Neural networkshavebreachedthe 1 trillion-parameters mark, and numbers one or two orders of magnitude greater may be close at hand. More efficient compute clusters could train those models more quickly and consume less energy doing it.We’re thinking:For most current machine learning models, the usual GPUs should be fine. Cerebras specializes in models and compute loads too large for a handful of GPUs in a single server — an interesting business as model sizes balloon.
When should you start building an AI project portfolio? What kinds of projects should it include? Get answers from AI practitioners during “How to Build a Real-World AI Project Portfolio” on November 29, 2022, at 9 a.m. Pacific Standard Time.Register now!
","['https://dl-staging-website.ghost.io/content/images/2022/11/29.11-09.png', 'https://dl-staging-website.ghost.io/content/images/2022/11/KATAGO_FastEdit_600px--1-.gif']"
Champion Model Is No Go,"A new algorithm defeated a championship-winning Go model using moves that even a middling human player could counter.What’s new:Researchers at MIT, UC Berkeley, and the Fund for Alignment Research trained amodelto defeatKataGo, an open source Go-playing system that has beaten top human players.How it works:The authors’ system tricks KataGo into deciding prematurely that it has won, causing it to end a game when the authors’ model is in a winning position.
Results:The model’s winning strategy involved taking control of a corner of the board and adding a few easy-to-capture pieces outside that area.
Why it matters:This work is a helpful reminder that neural networks are brittle, particularly to adversarial attacks that take advantage of a specific system’s idiosyncrasies. Even in the limited context of a game board, a model that achieves superhuman performance can be defeated by a simple — but unusual — strategy.
We’re thinking:AI practitioners perform exploratory data analysis and address potential attacks, but vulnerabilities always remain. Approaches like the one in this paper offer a way to find them.
",['https://dl-staging-website.ghost.io/content/images/2022/11/unnamed--18-.gif']
Does Price Optimization Hike Rents?,"An algorithm that’s widely used to price property rentals may be helping to drive up rents in the United States.
What’s new:YieldStar, a price-prediction service offered by Texas-based analytics company RealPage, suggests rental rates that are often higher than the market average,ProPublicareported. Critics believe the software stifles competition, inflating prices beyond what many renters can afford and adding to an ongoing shortage of affordable housing.
How it works:The algorithm analyzes leases for over 13 million units across the U.S. to  calculate prices for 20 million rental units daily. Five of the 10 largest property management firms in the U.S. use it. Former RealPage employees told the authors that property managers adopt around 90 percent of its suggested prices.
Behind the news:Automated pricing has had mixed results in real estate. Offerpad, Opendoor, and Redfin use algorithms toestimatea home’s value, and their systems account for around 1 percent of U.S. sales. Zillowshuttereda similar program last year after it contributed to over $600 million in losses.
Yes, but:Experts in real estate and antitrust law say that RealPage’s products enable rival property managers to coordinate pricing, a potential violation of U.S. antitrust law. In addition to the pricing algorithm, the company hosts user groups where property managers share feedback.
Why it matters:Advertised rental ratesrose17 percent between March of 2021 and 2022. Several factors contributed to the increase, but automated pricing tools like YieldStar could diminish tenants’ power to negotiate lower rents.
We’re thinking:YieldStar’s role in rising prices is unclear, but any automated system that has potential to manipulate markets at a large scale warrants regulatory oversight. There is precedent: In the 1990s, the U.S. Justice Departmentforcedairlines to change a shared pricing algorithm after finding that they had overcharged travelers by $1 billion. The black-box nature of AI systems means that regulators will need new tools to oversee and audit potential AI-driven coordination of prices.
",['https://dl-staging-website.ghost.io/content/images/2022/11/unnamed--8--2.jpg']
AI Hasn’t Been So Bad for Jobs,"Worries that automation is stealing jobs may have been greatly exaggerated.
What’s new:A U.S. governmentreportfound that employment has increased in many occupations that may be threatened by automation.
Projected versus actual growth:Bureau of Labor Statistics sociologist Michael J. Handelidentified11 occupations at risk from AI. He calculated changes in employment between 2008 and 2018.
Looking forward:Handel’s 13.9 percent estimate is reasonably close to the 8.7 percent rate projected by the Bureau of Labor Statistics at the beginning of that period. The agency’sprojectionfor 2019 to 2029 expects the same occupations to grow by a more leisurely 5.8 percent on average. Handel attributes most of the slowdown to an aging population.
Yes, but:The report cites occupations that suffered losses between 2008 and 2018 due, in part, to technologies other than AI: tax preparer (down by 9.7 percent), ticket agent (20.5 percent), and journalist (28.3 percent). And then there are telemarketers, who suffered a 50 percent decline in jobs as regulators clamped down on nuisance calls and — yes — now face challenges from AI systems.Behind the news:This study joins previous research that runs counter tofears that robots will steal human jobs.
Why it matters:A 2022 U.S.surveyof 1,225 people by chat software developer Tidio found that 65 percent of respondents — including 69 percent of university graduates — feared that they soon would lose their jobs to automation. The new report could improve people’s trust in technology by showing how such worries have played out in recent years. It should spur thinking about how to integrate AI safely and productively into workplaces of all kinds.We’re thinking:Automation has always been a part of industrialized societies, and its impacts can be substantial. (How many elevator operators do you know?) However, when it complements human work, it often leads to job growth that counters the losses — for example, the rise of automobiles led to lots of work for taxi drivers.
Are you looking to land an AI job but not sure what steps to take? Learn how to chart your path during our upcoming panel discussion, “How to Build a Real-World AI Project Portfolio,” on November 29, 2022, at 9 a.m. Pacific Standard Time.RSVP
","['https://dl-staging-website.ghost.io/content/images/2022/11/315669777_2436261229859411_4379693199225067083_n-1.jpg', 'https://dl-staging-website.ghost.io/content/images/2022/11/unnamed--14-.gif']"
What the Missing Frames Showed,"Neural networks can describe in words what’s happening in pictures and videos — but can they make sensible guesses about things that happened before or will happen afterward? Researchers probed this ability.
What’s new:Chen Liang at Zhejiang University and colleagues introduced a dataset and architecture, called Reasoner, that generates text descriptions of hidden, or masked, events in videos. They call this capabilityVisual Abductive Reasoning.
Key insight:To reason about an event in the past or future, it’s necessary to know about events that came before and/or after it, including their order and how far apart they were — what happened immediately before and/or after is most important, and more distant events add further context. A transformer typically encodes the positions of input tokens either one way (a token’s absolute position in the sequence of tokens) or the other (its pairwise distance from every other token), but not both. However, it’s possible to modify these positional encoding styles by producing an embedding for each pair of tokens that’s different from the inversion of each pair — for example, producing different embeddings for the pairs of positions (1,3) and (3,1). This approach captures both the order of events and their distance apart, making it possible to judge the relevance of any event to the events that surround it.
How it works:The authors trained an encoder and decoder. The training dataset included more than 8,600 clips of daily activities found on thewebandtelevision. Each clip depicted an average of four sequential events with text descriptions such as “a boy throws a frisbee out and his dog is running after it,” “the dog caught the frisbee back,” and “frisbee is in the boy’s hand.” The authors masked one event per clip. The task was to generate a description of each event in a clip including the masked one.
Results:The authors compared Reasoner to the best competing method,PDVC, a video captioner trained to perform their task. Three human volunteers evaluated the generated descriptions of masked events in 500 test-set examples drawn at random. Evaluating the descriptions of masked events, the evaluators preferred Reasoner in 29.9 percent of cases, preferred PDVC in 10.4 percent of cases, found them equally good in 13.7 percent of cases, and found them equally bad in 46.0 percent of cases. The authors also pitted Reasoner’s output against descriptions of masked events written by humans. The evaluators preferred human-generated descriptions in 64.8 percent of cases, found them equally good in 22.1 percent of cases, found them equally bad in 4.2 percent of cases, and preferred Reasoner in 8.9 percent of cases.
Why it matters:Reasoning over events in video is impressive but specialized. However, many NLP practitioners can take advantage of the authors’ innovation in using transformers to process text representations. A decoder needs only one transformer to produce descriptions, but the authors improved their descriptions by stacking transformers and using the confidence of previous transformers to help the later ones refine their output.
We’re thinking:Given a context, transformer-based text generators often stray from it — sometimes to the point of spinning wild fantasies. This work managed to keep transformers focused on a specific sequence of events, to the extent that they could fill in missing parts of the sequence. Is there a lesson here for keeping transformers moored to reality?
",['https://dl-staging-website.ghost.io/content/images/2022/11/unnamed--12--2.gif']
Tanks for All the Fish,"Farming shrimp in an open pond produces toxic effluent that can pollute groundwater and coastal waters. An AI-driven farm in a box may offer a more sustainable alternative.
What’s new:Based in Mexico City, Atarraya modifies shipping containers into AI-controlled tanks for raising commercial shrimp,Fortunereported. The company plans to install 20 units in a warehouse in Indianapolis.
How it works:The company’s Shrimpboxcontainstwo large water tanks equipped with sensors that track pH, nutrients, chemicals, and temperature. Machine learning models automatically dispense food and adjust conditions as needed.
Behind the news:The seafood industry is using AI to reduce its environmental footprint in a variety of ways.
Why it matters:If it can scale, Shrimpbox addresses several pain points in aquaculture. Aquaculture can put a dent inoverfishing, which threatens wild fish populations worldwide. Growing seafood in tanks rather than open water won’t leach waste, antibiotics, and other chemicals into the surrounding environment. And containerized tanks can enable food to be grown near where it will be consumed, which eliminates the need to transport it long distances.
We’re thinking:The shrimp are just prawns in this company’s game.
",['https://dl-staging-website.ghost.io/content/images/2022/11/unnamed--8-.gif']
When Safety Becomes Surveillance,"United States colleges tracked activists using a natural language processing system intended to monitor their mental health.
What’s new:An investigation byThe Dallas Morning Newsand UC Berkeley Graduate School of Journalismfoundthat schools in Georgia, North Carolina, and elsewhere used Social Sentinel, which monitors social media posts to identify individuals who intend to harm themselves or others, to keep tabs on protestors from 2015 to 2019 and possibly beyond.What they found:The system, which was renamed Navigate360 Detect in 2020, uses an “evolving AI language engine” toanalyzepublic communications. Users can query social media posts to Facebook, Instagram, Reddit, Twitter, and YouTube, although searches are limited to eight topics and 25 subtopics related to safety and security. The reporters studied documents acquired through leaks and requests to the government along with interviews with school employees. Among their findings:
The response:Navigate360, the Ohio-based company that acquired Social Sentinel in 2020, stated that the investigation was inaccurate and that the word “protest” was not in the system’s list of search topics. School officials didn’t respond to the reporters’ requests for comment and declined to discuss policies that govern their use of such software.
Why it matters:Campuses must tread a line between keeping students safe and hosting free expression. Protests can spiral out of control, causing injury and loss of life. Yet students have a reasonable expectation that educational institutions have their best interests at heart and will support their intellectual inquiries — even if they lead to peaceful protests.We’re thinking:AI can do good by alerting school officials to students who are severely disturbed or distressed. It should go without saying that systems designed for this purpose should never be used to stifle dissent.
Gain the skills to thrive in an uncertain economy! Companies are seeking qualified professionals who can tap AI’s potential. Break into AI with the newMachine Learning Specialization, an updated program for beginners created by Andrew Ng.Learn more
","['https://dl-staging-website.ghost.io/content/images/2022/11/Banner-MLS--1-.png', 'https://dl-staging-website.ghost.io/content/images/2022/11/unnamed--9-.gif']"
What Businesses Want from AI,"In a new report, business leaders share their machine-learning successes and struggles.
What’s new:Many businesses plan to increase their use of machine learning, but their efforts so far don’t always yield the results they seek, according to astudyperformed by the market analyst Forrester and commissioned by the bank Capital One.Machine learning on the rise:The authors surveyed 150 “data-management decision-makers” who work for North American companies in banking, information technology, manufacturing, and retail about how their organizations have used — and hope to use — machine learning.
Room for improvement: The respondents also outlined several worries.
Behind the news:The talent shortage in machine learning and data science is well documented. A 2020 Deloitte surveyfoundthat companies across all industries struggled to find the machine learning engineers that would help them meet their business goals. Some companies offer incentives toattractpeople skilled in AI, such as offering remote work at Silicon Valley pay rates and providing time off to pursue personal projects.
Why it matters:Machine learning continues to expand in mainstream businesses, and with it opportunities for machine learning engineers and data scientists. An earlier Forresterstudyfound that business leaders who see clear value in AI are (a) using or expanding their use of the technology and (b) effectively using the resulting insights to drive their business strategies. The new report shows that they believe the potential is greater still — and that bringing more machine learning engineers onboard could make the difference.We’re thinking:Many industries are still figuring out how to get the most out of AI. If you can make its value clear to executives in your organization — one of the top issues in this study — you can play a big role in moving things forward.
",['https://dl-staging-website.ghost.io/content/images/2022/11/unnamed--10-.gif']
Generating Investment,"The generative gold rush is on.
What’s new:Venture capitalists are betting hundreds of millions of dollars on startups that use AI to generate images, text, and more,Wiredreported.What’s happening:A handful of generative-AI startups have newly received nine-figure investments. They’re among over 140 nascent companies that aim to capitalize on applications in copywriting, coding, gaming, graphic design, and medicine, according to a growinglistmaintained by Stanford student David Song.
Behind the news:Established companies, too, are looking for ways to capitalize on AI’s emerging generative capabilities.
Yes, but:Incumbents and class-action lawyers are lodging complaints over who owns what goes into — and what comes out of — models that generate creative works.
Why it matters:Despite ongoing chatter aboutAI winter, it’s springtime for generative AI. Founders, investors, and trade organizations alike believe that this emerging technology has the potential to create huge value.We’re thinking: Generative AI holds the spotlight, given the mass appeal of models that paint beautiful pictures in response to simple text prompts, but AI continues to advance in many areas that hold significant, unfulfilled commercial promise.
",['https://dl-staging-website.ghost.io/content/images/2022/11/VOTE.gif']
Pushing Voters’ Buttons,"As the United States (along with several other countries) gears up for general elections, AI is helping campaigns attract voters with increasing sophistication.
What’s new:Strategists for both major U.S. political parties are using machine learning to predict voters’ opinions on divisive issues and using the results to craft their messages,The New York Timesreported.
How it works:Consulting firms typically combine publicly available data (which might include voters’ names, ages, addresses, ethnicities, and political party affiliations) with commercially available personal data (such as net worths, household sizes, home values, donation histories, and interests). Then they survey representative voters and build models that match demographic characteristics with opinions on wedge issues such as climate change and Covid-19 restrictions.
Behind the news:AI plays an increasing role in political campaigns worldwide.
Yes, but:Previous efforts to predict voter opinions based on personal data have been fraught with controversy. In the mid-2010s, for instance, political advertising startup Cambridge Analytica mined data illegally from Facebook users.
Why it matters:The embrace of machine learning models by political campaigns sharpens questions about how to maintain a functional democracy in the digital age. Machine learning enables candidates to present themselves with a different face depending on the voter’s likely preferences. Can a voter who’s inundated with individually targeted messages gain a clear view of a candidate’s positions or record?We’re thinking:Modeling of individual preferences via machine learning can be a powerful mechanism for persuasion, and it’s ripe for abuses that would manipulate people into voting based on lies and distortions. We support strict transparency requirements when political campaigns use it.
Ready to deploy your own diffusion model? Learn how to create machine learning applications using existing code in a free, hands-on workshop. Join us for “Branching out of the Notebook: ML Application Development with GitHub” on Wednesday, November 9, 2022!Register here
","['https://dl-staging-website.ghost.io/content/images/2022/11/Ff2woLQXwAAJ9qr--1-.jpg', 'https://dl-staging-website.ghost.io/content/images/2022/11/unnamed--6-.gif']"
Ukraine’s Lost Harvest Quantified,"Neural networks are helping humanitarian observers measure the extent of war damage to Ukraine’s grain crop.
What’s new:Analysts from the Yale School of Public Health and Oak Ridge National Laboratorybuilta computer vision model that detects grain-storage facilities in aerial photos. Its output helped them identify facilities damaged by the Russian invasion.How it works:The authors started with a database of grain silos last updated in 2019. They used machine learning to find facilities missing from that survey or built since then.
Results:Among 344 facilities, they found that 75 had suffered damage. They estimate that the destruction has compromised 3.07 million tons of grain storage capacity, nearly 15 percent of Ukraine’s total.
Why it matters:Before the war, Ukraine was the world’sfifth-largest wheat exporter. By disrupting this activity, the Russian invasion has contributed to aspikein global food prices, which observerswarnmay lead to famine. Understanding the scope of the damage to Ukraine’s grain supply could help leaders estimate shortfalls and plan responses.Behind the news:Machine learning has been applied to a variety of information in the war between Russia and Ukraine. It has been used toverifythe identities of prisoners of war, noncombatants fleeing conflict zones, and soldiers accused of committing war crimes. It has also been used to debunk propaganda, monitor the flow of displaced persons, and locate potentially damaged buildings obscured by smoke and clouds.We’re thinking:War is terrible. We’re glad that AI can help document the damage caused by invading forces, and we hope that such documentation will lead to payment of appropriate reparations.
",['https://dl-staging-website.ghost.io/content/images/2022/11/unnamed--7-.gif']
What Lurks in the Shadows?,"Ever look at a neural network’s output and think to yourself, “that's uncanny”? While the results can be inspiring — potential cures for dreaded diseases, streamlined industrial operations, beautiful artworks — they can also be terrifying. What if a model’s pattern-matching wizardry were applied to designing poison gas? Have corporate executives sold their souls in return for automated efficiency? Will evil spirits gain the upper hand as nations jockey for AI dominance? In this special issue ofThe Batch, asinpreviousyearsat this season, we raise a torch to the gloomy corners of AI and face gremlins that we ourselves have unleashed. Onward into the darkness!
",['https://dl-staging-website.ghost.io/content/images/2022/10/unnamed--4-.jpg']
The Black Box Awakens,"AI researchers are starting to see ghosts in their machines. Are they hallucinations, or does a dawning consciousness haunt the trained weights?
The fear:The latest AI models are self-aware. This development — at best —poses ethical dilemmas over human control of sentient digital beings. More worrisome, it raises unsettling questions about what sort of mind a diet of data scraped from the internet might produce.
Horror stories:Sightings of supposed machine sentience have come from across the AI community.
It’s just an illusion, right?:While media reports generally took the claim that LaMDA’s was self-aware seriously — albeit skeptically — the broader AI community roundly dismissed it. Observers attributed impressions that LaMDA is sentient to human bias and DALL-E 2’s linguistic innovation to random chance. Models learn by mimicking their training data, and while some are very good at it, there’s no evidence to suggest that they do it with understanding, consciousness, or self-reflection. Nonetheless, Loab gives us the willies.
Facing the fear:Confronted with unexplained phenomena, the human mind excels at leaping to fanciful conclusions. Science currently lacks a falsifiable way to verify self-awareness in a computer. Until it does, we’ll take claims of machine sentience or consciousness with a shaker full of salt.
",['https://dl-staging-website.ghost.io/content/images/2022/10/CHIPS--1-.jpg']
No More GPUs,"Advanced AI requires advanced hardware. What if the global supply of high-end AI chips dries up?
The fear:Most of the world’s advanced AI processors are manufactured in Taiwan, where tension with mainland China is rising. Nearly all such chips are designed in the U.S., which hasblockedChina from obtaining them. That could prompt China to cut off U.S. access to Taiwan’s manufacturing capacity. Military action would be a human tragedy. It would also imperil progress in AI.
Horror stories:China and the U.S. are on a collision course that threatens the global supply of advanced chips.
Securing the supply:Both the U.S. and China are trying to produce their own supplies of advanced chips. But fabricating circuitry measured in single-digit nanometers is enormously difficult and expensive, and there’s no guarantee that any particular party will accomplish it.
Facing the fear:If a chipocalypse does occur, the AI community will need to become adept at workarounds that take advantage of older semiconductor technology, such as small data, data-centric AI development, and high-efficiency model architectures. It will also need to push for international cooperation amid intensifying polarization. Still, a chip shortage would be the least scary thing about a great-power conflict.
Do you want to develop and deploy machine learning applications? Join our hands-on workshop “Branching out of the Notebook: ML Application Development with GitHub” on November 9, 2022, to learn industry-standard practices you can use today!RSVP
","['https://dl-staging-website.ghost.io/content/images/2022/10/Ff2woLQXwAAJ9qr.jpg', 'https://dl-staging-website.ghost.io/content/images/2022/10/HIRING.jpg']"
Inhuman Resources,"Companies are using AI to screen and even interview job applicants. What happens when out-of-control algorithmsarethe human resources department?
The fear:Automated systems manage every stage of the hiring process, and they don’t play fair. Trained on data rife with social biases, they blatantly discriminate when choosing which candidates to promote and which to reject. The door to your dream job is locked, and an unaccountable machine holds the key. Minority candidate? Speak with an accent? Unconventional background? You’re out of distribution!
Horror stories:Many companies and institutions use automated hiring systems, but independent researchers have found them prone to bias and outright error.
Bad performance review:Automated hiring systems are facing scrutiny from lawmakers and even the companies that use them.
Facing the fear:While many companies use hiring algorithms, most still keep humans in the loop. They have good incentive to do so: While machines can process mountains of resumes, human managers may recognize candidates who have valuable traits that an algorithm would miss. Humans and machines have complementary strengths, and a careful combination may be both efficient and fair.
",['https://dl-staging-website.ghost.io/content/images/2022/10/unnamed--5-.jpg']
Foundations of Evil,"A growing number of AI models can be put to purposes their designers didn’t envision. Does that include heinous deeds?
The fear:Foundation models have proven to be adept at deciphering human language. They’ve also proven their worth in deciphering the structural languages of biology and chemistry. It’s only a matter of time before someone uses them to produce weapons of mass destruction.
Horror stories:Researchers demonstrated how an existing AI system can be used to make chemical weapons.
Gas masks:In an interview, one of the researcherssuggestedthat developers of general-purpose models, such as the one they used to generate toxic chemicals, should restrict access. He added that the machine learning community should institute standards for instruction in chemistry that inform budding scientists about the dangers of misusing research.
Facing the fear:It’s hard to avoid the conclusion that the safest course is to rigorously evaluate the potential for harm of all new models and restrict those that are deemed dangerous. Such a program is likely to meet with resistance from scientists who value free inquiry and businesspeople who value free enterprise, and it might have limited impact on new threats that weren’t identified when the model was created. Europe is taking a first step with its regulation of so-calledgeneral-purpose AI. However, without a broad international agreement on definitions of dangerous technology and how it should be controlled, people in other parts of the world will be free to ignore them. Considering the challenges, perhaps the best we can do is to work proactively and continually to identify potential misuses and ways to thwart them.
",['https://dl-staging-website.ghost.io/content/images/2022/10/unnamed--3-.jpg']
AI Chips Spark International Tension,"New U.S. restrictions on chip sales aim to hamper China’s AI efforts.
What’s new:The U.S. governmentpublishedsweeping limits on sales of processors that involve U.S. designs and technology to Chinese businesses. U.S. officialsstatedthat the restrictions are meant to prevent China from militarizing AI.
New rules:The rules block sales of certain processors as well as U.S.-made equipment used to design and manufacture them. This includes high-end graphics processing units (GPUs) and other processors optimized for machine learning.
China’s response:A spokesperson for China’s foreign ministryaccusedthe U.S. of abusing export-control measures to target Chinese firms, stating that it would hinder global cooperation and supply chains.Behind the news:The restrictions initially came to light in September, when Nvidia and AMD independentlyalertedshareholders that the U.S. had imposed controls on their most advanced products. However, their details became publicly available only last week. They represent a significant escalation of earlier U.S. efforts to thwart China’s ambitions in advanced technology.
Why it matters:China has announced itsambitionto become the global leader in AI by 2030, and this requires access to cutting-edge processing power. The most advanced chips are manufactured in Taiwan and South Korea using chip-fabrication equipment made by U.S. companies, and the leading chip designers and makers of chip-design software reside in the U.S. This gives U.S. authorities a tight grip on other countries’ ability to buy and make chips. China’s effort to build domestic capacity to produce advanced semiconductors — which are hampered by the sheer difficulty and expense of etching features on silicon measured in nanometers  — now faces additional hardware, software, business, and talent hurdles.
We’re thinking:International cooperation has been essential to recent progress in AI. As barriers rise between the U.S. and China, the AI community must navigate a world where geography will have a much bigger impact on access to ideas and resources.
",['https://dl-staging-website.ghost.io/content/images/2022/10/unnamed--4-.gif']
Smarts for Farms,"The next green revolution may be happening in the server room.
What’s new:Microsoftopen-sourceda set of AI tools designed to help farmers cut costs and improve yields.
How it works:FarmVibes-AIincludes systems that analyze overhead imagery and sensor data to guide farm operations.
Behind the news:Nonprofits and academic institutions provide other open-source AI systems to increase food production in collaboration with large agribusiness firms, independent farmers, and rural communities.
Why it matters:The emerging practice ofprecision agriculture, which seeks to take into account not only entire fields but also local conditions down to the level of individual plants, could help farmers sow seeds, grow crops, fight pests, and harvest produce more efficiently. Off-the-shelf systems may not serve farmers who work in different parts of the world or grow niche crops. Open-source projects can expand their options effectively and inexpensively.
We’re thinking:Farmers tend to welcome innovations that improve yields and cut costs. They’re also famously self-sufficient, performing repairs and installing upgrades to their equipment. As self-driving tractors and precision-ag systems take root, they’re great candidates to become early adopters of industry-focusedplatformsthat make it easy for anyone to build useful AI applications.
Looking for a career that inspires you? Break into AI!TheMachine Learning Specializationteaches foundational AI concepts through an intuitive visual approach. This beginner-friendly program, created by Andrew Ng and Stanford Online, makes it easier than ever to start your AI career.Learn more
","['https://dl-staging-website.ghost.io/content/images/2022/10/MLS-TheBatch-ad.png', 'https://dl-staging-website.ghost.io/content/images/2022/10/ROGAN.jpg']"
"All Synthetic, All the Time","Joe Rogan meets Steve Jobs in an AI-generated podcast.
What’s new:For the debut episode of a new podcast series, Play.ht synthesized a 19-minute interview between the rock-star podcaster and late Apple CEO. You can hear ithereand propose computer-generated participants in future episodeshere.
How it works:The Dubai-based startup created the episode using text generation and voice cloning.
Behind the news:Rogan was also the subject of an early experiment in voice cloning. In 2019, Toronto-based Dessareleasedersatz Rogan audio clips — the first of a parade of fake celebrity voices.
Why it matters:The declamation is occasionally stilted and the script meandering (with occasional lapses into incoherence), but the rapid progress of generative audio combined with the entertainment world’s appetite for novelty suggests that satisfying synthetic productions may not be far off.We’re thinking:How long before we can produceHeroes of Deep Learningwithout actually talking with any of the heroes of deep learning?
",['https://dl-staging-website.ghost.io/content/images/2022/10/unnamed--5-.gif']
Wreckage Recognition,"A machine learning model identified areas likely to have been damaged by Hurricane Leo as it swept through the southern United States.
What's new:University of Connecticut researchers Zhe Zhu and Su Yeuseda learning algorithm to examine satellite images of the storm’s path and spot changes that might indicate wreckage.
How it works:Thesystemwas originally designed to identify damage to forests caused by fires, disease, drought, and the like. Given a satellite image, it evaluated changes in real time.
Results:The authors displayed the system’s output as anoverlayof yellow squares on a satellite image. Those areas track Ian’s course up the peninsula. They didn’t confirm the damage, however.
Behind the news:Similar approaches to detecting changes in satellite images have been used to assist relief efforts following a number of recent disasters. Researchers have used AI tomap surviving roadsthat relief groups could use to reach victims,direct firefighterstowards the most active areas of a woodland blaze, andscan satellite imagesfor signs of impending volcanic eruption.
Why it matters:Satellite imagery can be a boon to responders after a disaster, but the data is often too immense for manual evaluation. AI can enable relief workers to arrive faster and work more effectively. And it’s likely that humanity will need the extra help: Natural disasters such as hurricanes, wildfires, and floods are growingmore destructiveas global temperatures rise.
We're thinking:We enthusiastically support the use of AI to guide relief efforts after disasters. We urge agencies that are charged with responding to integrate the technology with their plans.
Want to launch an AI company? Looking for guidance on your existing startup? Join us for “Founding an AI Startup,” a panel discussion, on October 18, 2022. Speakers will share practical tips on how to get started, how to avoid common pitfalls, and more!RSVP
","['https://dl-staging-website.ghost.io/content/images/2022/10/HURRICANE.gif', 'https://dl-staging-website.ghost.io/content/images/2022/10/18.jpg', 'https://dl-staging-website.ghost.io/content/images/2022/10/PRECITASTE.gif']"
Food Forecaster,"The ability to predict customer demand could make fast food even faster.
What's new:The Mexican-themed Chipotle restaurant chain is testing AI tools that forecast demand, monitor ingredients, and ensure that workers fill orders correctly, according toQSR Magazine, a restaurant trade publication.
How it works:Eight Chipotle locations in California will employ tools from New York-based startupPreciTaste, which offers systems designed to boost efficiency in restaurants, bakeries, and food manufacturers. On the AI menu:
Behind the news:The fast-food industry’s focus on efficiency has made it a proving ground for a variety of AI applications.
Why it matters:Fast-food outlets in the U.S. are facing historicshortagesof labor — a ripe market for startups that aim to automate food prep. The captains of fast-food have taken notice: PreciTastecountsthe CEOs of McDonald’s, Burger King, and Shake Shack among its investors.
We're thinking:It’s good to see industrial AI used to help employees do their work better rather than to do it for them. Perhaps increasingly automated eateries will spur competition to emphasize the human touch.
",['https://dl-staging-website.ghost.io/content/images/2022/10/FDA.png']
Text to Video Without Text-Video Data,"Text-to-image generators like DALL·E 2, Midjourney, and Stable Diffusion arewinning art contestsandworrying artists. A new approach brings the magic of text-to-image generation to video.
What's new:Make-A-Video, a system built by Uriel Singer and colleagues at Meta, turns text prompts into high-resolution videos without training on text-video pairs. You can see its outputhere.
Key insight:While billions of text-image pairs are available to train atext-to-image generator, text-video pairs are too scarce to train a video equivalent. A model can learn relationships between words and pictures via pretraining on text-image pairs. Then it can be adapted for video by adding further layers that process image patches across frames and — while keeping the pretrained layers fixed — fine-tuning the new layers on videos, which are plentiful. In this way, a system can generate videos using knowledge it learned from text-image pairs.
How it works:The authors pretrained a series of models (one transformer and fourU-Netdiffusion models) to generate images from text, generate in-between video frames, and boost image resolution. To pretrain the text-to-image models, they used2.3 billion text-image pairs. After pretraining, they modified some of the models to process sequences of video frames: On top of each pretrained convolutional layer, the authors stacked a 1D convolutional layer that processed a grid of pixels in each frame; and on top of each pretrained attention layer, they stacked a 1D attention layer that, likewise, processed a grid of pixels in each frame. To fine-tune or train the modified models on video, they used 20 millioninternetvideos.
Results:The authors compared their system’s output to that of the previous state of the art,CogVideo, which takes a similar approach but requires training on text-video pairs. Crowdworkers supplied 300 prompts and judged the output of the author’s system to be of higher quality 77.15 percent of the time and to better fit the text 71.19 percent of the time.
Why it matters:Text-to-image generators already transform text into high-quality images, so there’s no need to train a video generator to do the same thing. The authors’ approach enabled their system to learn about things in the world from text-image pairs, and then to learn how those things move from unlabeled videos.
We're thinking:The Ng family’s penchant fordrawing pandasis about to undergo another revolution!
",['https://dl-staging-website.ghost.io/content/images/2022/10/ezgif.com-gif-maker--5--3.gif']
Tough Economy Hits AI Startups,"Venture investors are tapping the brakes on AI amid rising economic uncertainty.
What’s new:In their latestArtificial Intelligence & Machine Learning Report, market research firm PitchBookdocumentsa sharp reduction in investment in AI startups in the first half of 2022, a time of rising inflation and interest rates.What it says:The report delivers bad news and highlights categories that have continued to hold venture investors’ interest — and those that haven’t.
Future forecasts:Despite the grim numbers, the authors reject characterizing the current period as anAI winter. They expect investments to rebound from around $175 billion in 2022 to over $350 billion in 2025, driven primarily by advances in multimodal AI, general-purpose models, and synthetic data.
Behind the news:In a separate analysis, CB Insightsdeterminedthat AI funding would fall by 21 percent each quarter in 2022. Similarly, it found that the losses were not uniform: AI startups in healthcare, financial technology, and retail — areas that have a solid track record — have maintained their funding levels better than other, more speculative fields.
Why it matters:When credit is harder to obtain, investors tend toback away​​ from riskier investments. Given rising interest rates, inflation, and the threat of recession, that explains the falloff in funding for startups without proven market value. Companies that focus on proven applications and markets should continue to prosper, although competition is bound to stiffen as vendors are pressed to demonstrate that their offering is superior.We’re thinking:As we noted inpreviousissuesofThe Batch, rising interest rates and falling stock indices signal that AI developers should be ready for increased pressure to develop projects that demonstrate near-term, tangible value. We continue to believe this is a good time to invest in long-term bets on AI, as the real interest rate (adjusted for inflation) remains very low and the transformative value of AI is more financially powerful than interest rates.
Join FourthBrain’s Machine Learning Engineer program for access to live, instructor-led classes and dedicated career services. Our graduates have seen an average salary increase of $27,000! Applications are due by October 10, 2022. The next cohort starts on October 18.Learn more
","['https://dl-staging-website.ghost.io/content/images/2022/10/FourthBrain-banner-ad--1-.png', 'https://dl-staging-website.ghost.io/content/images/2022/10/MOON_600px.webp']"
The Dark Side of the Moon — Lit Up!,"Neural networks are making it possible to view parts of the Moon that are perpetually shrouded by darkness.
What’s new:Valentin Bickel at ETH Zürich and colleagues devised a method calledHyper-effective Noise Removal U-net Software(HORUS) to remove noise from images of the Moon’s south pole, where direct sunlight never falls. The National Aeronautics and Space Administration (NASA) is using the denoised images to plan lunar missions that will put humans on the Moon for the first time in decades.
The challenge:The only light that strikes the lunar south pole’s craters, boulders, mounds, and crevasses comes from scant photons that reflect off Earth or nearby lunar landforms or arrive from faraway stars. An imaging system aboard NASA’s Lunar Reconnaissance Orbiter can capture features that are lit this way, but it has a tendency todetect photons where none exist. Transmitting and processing the images introduces more noise, further blurring details in the already-dim images. Removing noise optimizes the available light, making it possible to see the landscape.
How it works:The authorstrainedtwo neural networks to remove the noise from lunar images.
Results:HORUS removed noise from 200,000 images of the lunar surface. The authors identified possible landing sites, hazards to avoid, and evidence that some areas may contain water ice beneath the surface.
Behind the news:The Moon’s south pole is the target for NASA’s upcoming Artemis program. Artemis 1, scheduled to launch in late September, will be fully automated. Artemis 2, scheduled for 2024, aims to land humans on the Moon for the first time since NASA’s final Apollo mission in 1972.
Why it matters:NASA chose the Moon’s south pole as the target for future missions because water may be frozen at the bottoms of craters there. Water on the Moon could provide clues about the heavenly body’s origin as well as hydration, radiation shielding, and propellant for missions further out in the solar system.
We’re thinking:This AI project is out of this world!
",['https://dl-staging-website.ghost.io/content/images/2022/10/DIALOG.gif']
Chipmaker Boosts AI as a Service,"Nvidia, known for chips designed to process AI systems, is providing access to large language models.
What’s new:Nvidiaannouncedearly access to NeMo LLM and BioNeMo, cloud-computing services that enable developers to generate text and biological sequences respectively, including methods that tune inputs — rather than the models themselves — to enable models trained on web data to work well with a particular user’s data and task without fine-tuning. Users can deploy a variety of models in the cloud, on-premises, or via an API.How it works:The new services are based on Nvidia’s pre-existing NeMo toolkit for speech recognition, text-to-speech, and natural language processing.
Behind the news:Nvidia’s focus on prompt learning and biological applications differentiate it from other companies that provide large language models as a service.
Why it matters:Until recently, large language models were the province of organizations with the vast computational resources required to train and deploy them. Cloud services make these models available to a wide range of startups and researchers, dramatically increasing their potential to drive new developments and discoveries.We’re thinking:These services will take advantage of Nvidia’sH100GPUs, developed specifically to process transformer models. Nvidia CEO Jensen Huang recently said the public no longer should expect chip prices to fall over time. If that’s true, AI as a service could become the only option for many individuals and organizations that aim to use cutting-edge AI.
",['https://dl-staging-website.ghost.io/content/images/2022/09/PALMROBOT-2_600px.gif']
Parsing Commands Into Actions,"A new method enables robots to respond helpfully to verbal commands by pairing a natural language model with a repertoire of existing skills.
What’s new:SayCan, a systemdevelopedby researchers at Google and its spinoff Everyday Robots, enabled a robot equipped with an arm, camera, and gripper to take a high-level command such as “I spilled my drink, can you help?” and choose low-level actions appropriate to a given environment such as “find a sponge” and “go to table.”
Key insight:A pretrained large language model can grasp verbal instructions well enough to propose a general response. But it can’t adapt that response to local conditions; for instance, an environment that includes a sponge but not a mop. Combining a large language model with a model that determines which actions are possible in the current environment makes for a system that can interpret instructions and respond according to the local context.
How it works:SayCan drew from over 550 kitchen-related actions that the authors had trained it to perform using a combination ofimage-based behavioral cloningand reinforcement learning. Actions included picking up, putting down, and rearranging objects; opening and closing drawers; and navigating to various locations.
Results:The authors tested the system by giving the robot 101 commands in a mock kitchen that contained 15 objects such as fruits, drinks, snacks, and a sponge. Human judges determined that the robot planned valid actions 84 percent of the time and carried them out 74 percent of the time. In a real-life kitchen, the robot achieved 81 percent success in planning and 61 percent success in execution.
Why it matters:The dream of a domestic robot has held the public imagination since the dawn of the industrial revolution. But robots favor controlled environments, while households are highly varied and variable. The team took on the challenge by devising a way to choose among 551 skills and 17 objects. These are large numbers, but they may not encompass mundane requests like “find granny’s glasses” and “discard the expired food in the fridge.”
We’re thinking:This system requires a well-staged environment with a small number of items. We imagine that it could execute the command, “get the chips from the drawer” if the drawer contained only a single bag of chips. But we wonder whether it would do well if the drawer were full and messy. Its success rate in completing tasks suggests that, as interesting as this approach is, we’re still a long way from building a viable robot household assistant.
Robert Wydler was always drawn to AI. After 35 years in IT, he finally decided to pursue his passion by taking Andrew Ng’s Machine Learning course. Ready for a change?Enroll in the Machine Learning Specialization!
","['https://dl-staging-website.ghost.io/content/images/2022/09/MLS_Learner_1200x628_A-1_Artboard-1-copy-9--1-.png', 'https://dl-staging-website.ghost.io/content/images/2022/09/PRISON--1-.png']"
Panopticon Down Under,"A state in Australia plans to outfit prisons with face recognition.
What’s new:Corrective Services NSW, the government agency that operates nearly every prison in New South Wales, contracted the U.S.-based IT firm Unisys to replace a previous system, which required a fingerprint scan to identify people, with one that requires only that subjects pass before a camera,InnovationAus.comreported.How it works:The new system will use face recognition to identify inmates and visitors as they enter or exit correctional facilities.
Yes, but:Samantha Floreani of Digital Rights Watchraised concernsthat face recognition may exacerbate biases in the Australian corrective system, which incarcerates indigenous people disproportionately. Additionally, Floreani said that contracting to Unisys, a U.S.-based firm, raises questions about whether personal data on Australians will be transferred to another country and whether the data will be secure and handled properly. The Australian public, too, is wary. A 2021pollfound that 55 percent of Australians supported a moratorium on face recognition until stronger safeguards are in place.Behind the news:England and Walestestedface recognition for screening prison visitors in 2019, mostly in an effort to crack down on smuggling of drugs into prisons. In the United States, the federal Justice Department has funded severalinitiativesto apply face recognition. The U.S. Marshals Service, which handles fugitive investigations, isdevelopinga face recognition system to aid in transporting prisoners.
Why it matters:The flow of visitors, contractors, and prisoners into and out of correctional facilities creates opportunities for security breaches. Face recognition promises to help manage this traffic more safely. However, the technology, which is relatively new, largely unregulated, and developing rapidly, brings with it potential for abuse, mission creep, and other adverse consequences, especially in a high-stakes field like criminal justice.We’re thinking:Surveillance has always been an inextricable part of incarceration, but it shouldn’t encroach on the rights of prisoners or the people who guard, visit, and provide services to them. More optimistically, if technology can generate indelible, auditable records of the activities of both guards and prisoners, it can help protect against abuses and address them when they occur.
",['https://dl-staging-website.ghost.io/content/images/2022/09/DEITv2-compressed--1-.gif']
Data Scientists on Data Science,"A survey of data scientists reveals a field of great opportunities but also room for improvement.
What’s new:The 2022“State of Data Science”report from Anaconda, maker of a popular Python distribution, surveyed 3,493 students, teachers, and employees in data science, machine learning, and AI about their work and opinions of the field.Who they surveyed:The poll reached data scientists in 133 countries (40 percent in the U.S. or Canada). 76 percent were men, 23 percent women, and 2 percent nonbinary. 80 percent had at least an undergraduate-level degree. The majority — 55 percent — worked for firms with 1,000 or fewer employees, while 15 percent worked for companies with over 10,000 employees.
State of the field:Participants were asked to rate various aspects of their day-to-day work and share their hopes for the future. They expressed widespread satisfaction but expressed worries about the field’s potential for harm.
Challenges:Respondents also answered questions about challenges they face, and those faced by data science at large:
Behind the news:The U.S. Bureau of Labor Statisticsforecaststhat the number of computer and information research scientists will grow by 21 percent between 2021 and 2031 — far higher than the 5 percent average across all industries. Anecdotal evidence suggests that demand for skilled AI professionals alreadyoutstripssupply.Why it matters:It’s great to hear that data science rates highly in both job satisfaction and market demand. The areas in which respondents expressed a desire for improvement — bias, privacy, the dearth of skilled engineers — suggest possible avenues for career development.We’re thinking:Given that preparing, cleansing, and visualizing data takes up 51 percent of time spent on data science, and selecting and training models occupies only 18 percent, it appears that most practitioners already dodata-centric AI development. They just need better principles and tools to help them do this work more efficiently!
",['https://dl-staging-website.ghost.io/content/images/2022/09/Screen-Shot-2022-09-21-at-10.56.05-AM.png']
Regulating AI in Undefined Terms,"A proposed European Union law that seeks to control AI is raising questions about what kinds of systems it would regulate.
What's new:Experts at a roundtable staged by the Center for Data Innovationdebatedthe implications of limitations in the EU’s forthcomingArtificial Intelligence Act.
The controversy:The legislation is in the final stages of revision and moving toward a vote next year. As EU parliamentarians worked to finalize the proposed language, the French delegation introduced the term “general-purpose AI,” which is described as any system that can “perform generally applicable functions such as image/speech recognition, audio/video generation, pattern-detection, question-answering, translation, etc., and is able to have multiple intended and unintended purposes.” Providers of general-purpose AI would be required to assess foreseeable misuse, perform regular audits, and register their systems in an EU-wide database. The proposal has promptedworriesthat the term’s vagueness could hinder AI development.
The discussion: The roundtable’s participants were drawn from a variety of companies, nongovernmental organizations, and government agencies. They generally agreed that the proposed definition of general-purpose AI was too broad and vague. The consequences, they warned, could include criminalizing AI development and weakening protection against potential abuses.
Behind the news:Initiallyproposedin 2021, the AI Act would sort AI systems into three risk levels. Applications with unacceptable risk, such as social-credit systems and real-time face recognition, would be banned outright. High-risk applications, such as applications that interact with biometric data, would face heightened scrutiny including a mandated risk-management system. The law would allow unfettered use of AI in applications in the lowest risk level, such as spam filters or video games.
Why it matters:The AI Act, like the EU’s General Data Protection Regulation of 2018, likely will have consequences far beyond the union’s member states. Regulators must thread the needle between overly broad wording, which risks stifling innovation and raising development costs, and narrow language that leaves openings for serious abuse.
We're thinking:The definition of AI has evolved over the years, and it has never been easy to pin down. Once, an algorithm for finding the shortest path between two nodes in a graph (the A* algorithm) was cutting-edge AI. Today many practitioners view it as a standard part of any navigation system. Given the challenge of defining general-purpose AI — never mind AI itself! — it would be more fruitful to regulate specific outcomes (such as what AI should and shouldn't do in specific applications) rather than try to control the technology itself.
AI has undisputed business value. So why do many companies fail to realize its potential? Join Andrew Ng, Israel Niezen (co-founder of Factored), and Susie Harrison (AI editor atInforma Tech) on September 29, 2022, at 1 p.m. Eastern Time for lessons on how to make AI a profitable part of your business. Registernow
","['https://dl-staging-website.ghost.io/content/images/2022/09/Imagen2.png', 'https://dl-staging-website.ghost.io/content/images/2022/09/LaughingRobot-2a_1200px.jpg']"
Toward Machines That LOL,"Even if we manage to stop robots from taking over the world, they may still have the last laugh.
What’s new: Researchers at Kyoto Universitydevelopeda series of neural networks that enable a robot engaged in spoken conversation to chortle along with its human interlocutor.How it works:The authors built a system of three models that, depending on a user’s spoken input, emitted either a hearty hoot, a conversational chuckle, or no laugh at all. They trained all three models on recordings of speed-dating dialogs between humans andErica, an androidteleoperatedby an actress, which they deemed to be rich in social laughter.
Results:The authors’ system and two baselines responded to brief monologues that included laughter, while more than 30 crowdsourced workers judged naturalness and human-likeness on a scale of 1 to 7. The authors’ system achieved an average 4.01 for naturalness and 4.36 for human-likeness. One baseline, which never laughed, scored an average 3.89 for naturalness and 3.99 for human-likeness. The other, which always reacted to laughter in the monologue with a social laugh, scored an average of 3.83 for naturalness and 4.16 for human-likeness.Behind the news:About the training corpus: The authors recorded speed-dating dialogs with Erica as part of a largereffortto elicit human-machine conversations that delved more deeply into human issues than typical text dialogs with chatbots. Built by researchers at Kyoto and Osaka Universities and Kyoto’sAdvanced Telecommunications Research Institute, the feminine-styled automaton hasrapped,anchoredTV news, and beencastto play the lead role in a science-fiction film scheduled for release in 2025.Why it matters:Automating laughter is no joke! Mastering when and how to laugh would be valuable in many systems that aim to integrate seamlessly with human conversation. Titters, snickers, and howls play akey rolein bonding, agreement, affection, and other crucial human interactions. Laughter’s role varies in different communities, yet it cancross culturesand bring people together.We’re thinking:We’re glad the robots are laughingwithus, notatus!
",['https://dl-staging-website.ghost.io/content/images/2022/09/MATTING.gif']
Spotting Tax Cheats From Overhead,"Tax dodgers can’t hide from AI — especially those who like to swim.
What’s new:French tax authorities, which tax swimming pools according to their size because they increase a home’s property value, netted nearly €10 million using an automated system to identify unregistered pools,Le Parisienreported.
Diving in:Developed by Google and Paris-based consultancyCapgemini, the system spots pools in a public database of aerial images. It then cross-checks them with land-registry data to determine whether they’re registered. France plans to roll it out nationwide this month.
Beneath the surface:At least 17 other European Union tax-collection agenciesuseAI for tasks that include identifying who should be audited, scraping taxpayer data from ecommerce sites, and powering chatbots that help taxpayers file. Last year, U.S. tax authoritiesimplementedtechnology from Palantir that identifies fraud by analyzing tax returns, bank statements, property records, and social media activity.
Why it matters:As AI analyzes every nook and cranny of an individual’s data trail, reluctant taxpayers will find it harder to avoid paying up.We’re thinking:There’s irony in a tech behemoth that’s known for aggressivetax-avoidance strategieshelping a government collect tax revenue.
",['https://dl-staging-website.ghost.io/content/images/2022/09/GPU.gif']
The Geopolitics of GPUs,"The U.S. government blocked U.S. makers of AI chips from selling to China, adding to existing sanctions that target Russia.
What’s new:The Department of Commerce restricted sales of Nvidia’s and AMD’s most-advanced chips for training and running large AI models,Reutersreported.
How it works:U.S. officials didn’t detail the specifics of the ban. Nvidia said it would stop selling itsA100andH100graphics processing units (GPUs) to China. AMD said the action affects itsMI250GPU.
China’s reaction:“This violates the rules of the market economy, undermines the international economic and trade order, and disrupts the stability of global industrial and supply chains,” a foreign ministry spokespersonsaid. China hasn’t announced countermeasures, but some analystsanticipatethat it will further increase funding to its domestic semiconductor sector.Behind the news:Russia hasfacedchip embargoes by South Korea, Taiwan, and the U.S. in response to its February invasion of Ukraine. In 2020, the U.S. governmentrequiredforeign chip makers that use U.S. equipment to receive special permission before doing business with the Chinese tech company Huawei.
Why it matters:AI is increasingly intertwined with geopolitics. China has repeatedly stated its intention to achieve “AI supremacy” and outpace the U.S. China, however, is still largely reliant on imported semiconductors, so the U.S. ban could hobble its ambitions.We’re thinking:An AI chip may be designed in the U.S. and manufactured in Taiwan using equipment from the Netherlands. This globalized supply chain works well when international tensions are low, but rising tensions pose risks to both progress in AI and the security of several countries.
",['https://dl-staging-website.ghost.io/content/images/2022/09/TIMES.jpeg']
Reading Readers,"A smart news paywall is optimizing subscriptions without driving away casual readers by showing them come-ons subscribe.
What’s new:The New York TimesdescribedDynamic Meter, a machine learning system that decides how many free articles to provide to a given user before prompting them to register or subscribe.
How it works:The newspaper’s data science team ran a randomized, controlled trial and found that delivering more pop-ups that ask readers to subscribe resulted in more subscriptions but fewer page views, while delivering fewer popups resulted in fewer subscriptions but greater page views.
How it works:The New York Times’ data science team collected a dataset by running a randomized, controlled trial that tracked the behavior of registered — but not yet subscribed — users with various characteristics. Generally, delivering more pop-ups that asked them to subscribe resulted in more subscriptions but fewer page views (prior to subscribing), while delivering fewer popups resulted in fewer subscriptions but greater page views.
Behind the news:The Wall Street Journal, Switzerland’sNeue Zürcher Zeitung, and Germany’sFrankfurter Allgemeine Zeitungalso use machine learning to maximize subscriptions.Why it matters:The shift in news consumption from print to online devastated publishers, in part because they’re forced to compete with the panoply of attention-grabbing content on the web. Smart paywalls can help them thrive by tantalizing readers with free content, then forcing them to decide whether they value it relative to everything else the web has to offer.We’re thinking:News is critical to a free society, and it’s important to distribute it fairly. Does allowing some people to read more articles than others give those people an advantage over people who are allowed to read fewer articles? Is it okay to offer a wealthy person five articles and a less-wealthy person 10 before demanding that they subscribe — or vice versa? While AI can help companies capture greater financial value, many questions of social value remain to be answered.
",['https://dl-staging-website.ghost.io/content/images/2022/09/PALE.gif']
Misinformation Recognition,"Google updated a key model behind the algorithm that ranks its search results to respond to the flood of misinformation on the web.
What’s new:The search giant aims to minimize the prominence of falsehoods in the information it presents near the top of search results, which it callssnippets.How it works:Google revised itsMultitask Unified Modelto verify the accuracy of snippets.
Behind the news:Google isn’t the only major website to task AI with filtering the torrent of disinformation.
Why it matters:Human fact-checkers can’t keep up with the rising tide of misinformation. AI has an imperfect record of moderating online content. For instance, Facebook faces allegations that its algorithmssuppressads aimed at people with disabilities andoverlookincitements to violence. But even incremental improvements are worthwhile in the face of anti-vaccine panics, denial of climate change,and calls for genocide.We’re thinking:AI is an important tool in keeping web searches honest, but it’s not yet ready to do the job alone. Just as autonomous taxi companies often employ human safety drivers to oversee their vehicles, automated content moderation systems benefit from humans in the loop.
",['https://dl-staging-website.ghost.io/content/images/2022/08/CHINA.gif']
China’s AI ROI,"China’s investment in AI business, infrastructure, and research could pay off big over the next decade.
What’s new:AI is expected to add $600 billion to the Chinese economy by 2030, according to ananalysisby consulting firm McKinsey.Market-ready technologies:The researchers interviewed more than 50 experts and analyzed market data collected between October 2021 and November 2021. They looked for market-ready AI technologies (as opposed to those that are either early-stage or mature) with the highest potential for impact in the coming decade. Conclusion: Transportation, manufacturing, enterprise software development, and healthcare stand to add huge value to the nation’s economy.
The big picture:In 2017, Chinaannouncedits aim to become “the world's primary AI innovation center” by 2030. Since then, the country has pushed to accelerate AI research, development, and product development. The latestAI Indexestimated that China accounted for one-fifth of all private investment in AI.
Yes, but:Not everything is rosy in China’s tech sector. Venture capital investment in the first four months of 2022 was 43.5 percentlowerthan the same period in 2021 — a decline nearly four times more severe than the global average.Why it matters:China’s contribution to AI has been impressive, but the outcomes outlined in the report are not foregone. To cash in on AI’s potential, China must put more effort into acquiring and using high-quality data, collaborating with other nations, protecting intellectual property, and training skilled workers, the report says.We’re thinking:Many commentators view AI development as a competition among global superpowers. Rising tensions between the U.S. and China tend to reinforce this view. Yet every nation’s AI effort relies on numerous individual developers. This global community can play an important role in steering AI in directions that benefit all people as well as their home countries.
",['https://dl-staging-website.ghost.io/content/images/2022/08/FISH.gif']
Neural Nets Catch Fresher Fish,"A robot deckhand aims to help fishing boats keep their haul fresh all the way to your table.
What’s new:Shinkei Systems developed a machine that uses computer vision to slaughter fish in a way that maximizes their shelf life and flavor,TechCrunchreported.How it works:The refrigerator-sized system, which is designed to withstand heavy seas, attaches to a boat’s deck. Fishermen empty their nets into a hopper that passes individual fish through the machine one by one. Inside, computer vision guides tools to pierce the animal’s brain, sever its spine, drain its blood, and deposit it into an ice bath. The process takes between 10 to 15 seconds per fish.
Behind the news:The process is modeled on a manual technique calledike jime, which typically requires a skilled practitioner, making it difficult to industrialize. Ike jime is increasingly popular among upscale seafood restaurants both within and outside Japan, where it was developed.
Why it matters:The fast pace aboard fishing boats leaves little time for processing the catch, so most fish are left to suffocate to death, which can take minutes to hours. This isn’t just inhumane, it results in meat that’s bruised by flopping and tainted by stress-induced hormones, leading to shorter shelf life and less appetizing flavor. This system could give fishing operations an efficient way to sell their catches more profitably while dispatching fish more humanely.
We’re thinking:Giving such a delicate task to a robot may seem fishy, but this application seems sure to scale.
",['https://dl-staging-website.ghost.io/content/images/2022/08/SIMPLE.gif']
AI Regulations Proceed Locally,"While the United States doesn’t explicitly regulate AI at the national level, many parts of the country have moved to limit the technology.What’s new:The Electronic Privacy Information Center publishedThe State of State AI Policy, a summary of AI-related laws that states and cities considered between January 2021 and August 2022.Passed:Seven laws were enacted that regulate a variety of AI applications and activities.
Pending:Thirteen more laws are currently in progress in nine states and Washington DC. Bills would establish advisory bodies to study the impacts of AI in California, Georgia, Maryland, Massachusetts, New Jersey, New York, and Rhode Island. California lawmakers propose mandating processes to minimize algorithmic bias. Hawaii lawmakers propose a tax credit for AI businesses.Why it matters: AI increasingly affects U.S. society, sometimes inalarmingways (and at the expense of publictrust). Yet it remains largely unregulated at the national level. State and local legislation are filling the gap. However, a patchwork legal landscape could be a headache for companies that aim to do business in multiple states.We’re thinking:A yawning gap separates leaders in technology and government. Many tech executives hold the stereotype that politicians don't understand technology. Meanwhile, politicians widely regard tech executives as being hostile to the government and primarily out to make a buck. It will take effort on both sides to overcome these stereotypes and forge a shared understanding that leads to better regulations as well as better AI.
",['https://dl-staging-website.ghost.io/content/images/2022/08/SPURIOUS--1-.gif']
Taming Spurious Correlations,"When a neural network learns image labels, it may confuse a background item for the labeled object. For example, it may learn to associate the label “camel” with desert sand and then classify a cow on a beach as a camel. New research has trained networks to avoid such mistakes.What’s new:A team at Stanford and Northeastern University led by Michael Zhang proposedCorrect-N-Contrast(CNC), a training method that makes neural networks more robust to spurious correlations, in which features and labels are associated but not causally related.Key insight:A neural network likely has learned a spurious correlation when it produces dissimilar representations of two images with the same label. When learning representations of two images of a cow, for example, the error may manifest as a representation of a grassy field in one image and a representation of a beach in the other. A contrastive loss function can help a neural network avoid such errors by encouraging it to learn similar representations for similar objects against different backgrounds.How it works:The authors trained models to classify examples and identified examples the models got wrong, possibly owing to spurious correlations. Then they trained a second neural network to classify them correctly using a contrastive loss function.
Results:The authors evaluated their models’ accuracies on groups of examples known to be difficult to classify. Their approach outperformedEIIL, which first trains a model to infer related groups of examples and then trains a second model to classify examples using the group IDs, both on average and on individual tasks. For instance, the ResNet-50 trained on CelebA with CNC achieved 88.8 percent accuracy, while training with EIIL achieved 81.7 percent accuracy. Across all tasks, the authors’ approach achieved 80.9 percent average accuracy while EIIL achieved 74.7 percent average accuracy.Yes, but:Group DRO, which provides additional information during training such as a description of the background of an image or the gender of a depicted person, achieved 81.8 percent average accuracy.Why it matters:Previous approaches to managing spurious correlations tend to expand training datasets to capture more variability in data. This work actively guides models away from representing features that reduce classification accuracy.We’re thinking:A self-driving car must detect a cow (or a person or another vehicle) whether it stands on a meadow, a beach, or pavement.
Nektarios Kalogridis was a software developer in finance. He saw the growing impact of AI on the industry, so he took Andrew Ng’sMachine Learningcourse. Today, he’s a senior algorithmic trading developer at one of the world’s largest banks.Enroll in theMachine Learning Specialization!
","['https://dl-staging-website.ghost.io/content/images/2022/08/MLS_Learner_1200x628_A-1_Artboard-1-copy-11-1.webp', 'https://dl-staging-website.ghost.io/content/images/2022/08/TXSCARA_600px.gif']"
One Cool Robot,"Autonomous robots are restocking the refrigerated sections in corner stores.What’s new:FamilyMart, a chain of Japanese convenience stores, plans toemploy robotsto fill shelves with beverage bottles at 300 locations.How it works:The TX SCAR from Tokyo-based firm Telexistence includes an arm and camera. It shuttles along a rail in between stock shelves and the rear of a customer-facing refrigerator, moving up to 1,000 containers a day.
Behind the news:FamilyMartalso operates grab-and-go stores in which AI models recognize items as shoppers put them into carts and ring up sales automatically as they exit.Amazonhas similar stores in the United Kingdom and United States.Why it matters:Japan faces anaging workforcewith no end in sight. People over 65 years old make up around a quarter of the population, which is expected to have the world’s highest average age for decades. Embracing robot labor is one solution, along with matching older workers with appropriate jobs and extending the retirement age.We’re thinking:Frommaking french friesto restocking shelves, the jobs that once were rites of passage for young adults are increasingly automated. Will the next wave of after-school gigs involve debugging code and greasing servos?
",['https://dl-staging-website.ghost.io/content/images/2022/08/GEM--1-.gif']
Bad Machine Learning Makes Bad Science,"Misuse of machine learning by scientific researchers is causing a spate of irreproducible results.
What’s new:A recentworkshophighlighted the impact of poorly designed models in medicine, security, software engineering, and other disciplines,Wiredreported.Flawed machine learning:Speakers at the Princeton University event highlighted common pitfalls that undermine reproducibility:
Behind the news:The workshop followed a recentmeta-analysisby Princeton researchers that identified 329 scientific papers in which poorly implemented machine learning yielded questionable results.
Why it matters:Experienced machine learning practitioners are well aware of the pitfalls detailed by the workshop, but researchers from other disciplines may not be. When they apply machine learning in a naive way, they can generate invalid results that inherit an aura of credibility owing to machine learning’s track record of success. Such results degrade science and impinge on the willingness of more skeptical scientists to trust the efficacy of learning algorithms. Enquiries like this one will be necessary at least until machine learning becomes far more widely practiced and understood.
We’re thinking:Many AI practitioners are eager to contribute to meaningful projects. Partnering with scientists in other fields is a great way to gain experience developing effective models and educate experts in other domains about the uses and limitations of machine learning.
",['https://dl-staging-website.ghost.io/content/images/2022/08/WALK.gif']
AI Hits Its Stride,"A smart leg covering is helping people with mobility issues to walk.
What’s new:Neural Sleeveis a cloth-covered device that analyzes and corrects wearers’ errant leg movements. Developed by startupCionicand product studioFuseproject, the device is intended to help people with conditions that affect coordination of the legs, such as multiple sclerosis, cerebral palsy, spinal cord injury, and stroke.How it works:The sleeve is fitted with electrodes that contact the wearer’s skin in the region of particular leg muscles. A machine learning model analyzes electrical impulses generated by muscles as they move and instructs the electrodes to stimulate the muscles in a way that corrects the wearer’s gait.
Behind the news:AI-enabled wearable devices have a wide variety of applications in making the world more accessible to people who are injured or otherwise disabled.
Why it matters:Some 13.7 percent of American adults who have a disability have serious trouble walking up and down stairs,according tothe United States Centers for Disease Control and Prevention. Devices like Neural Sleeve may enable many of these people to move more freely and effectively.We’re thinking:Neural Sleeve partnered with a design studio to enhance the system’s appeal to users. This sort of collaboration can be very helpful when deploying systems — especially those involved in highly personal activities like therapy — in the real world.
Are you ready to use AI for social good? Advances in AI offer opportunities to tackle important environmental, public health and socioeconomic issues. Learn how! Join us on August 31, 2022, at 9:00 a.m. Pacific Time for “The Impact of AI for Social Good.”RSVP
","['https://dl-staging-website.ghost.io/content/images/2022/08/AI-for-Social-Good-Banner.webp', 'https://dl-staging-website.ghost.io/content/images/2022/08/FALL.gif']"
Deepfakes Against Profanity,"Deepfake technology enabled a feature film to reach a broader audience.
What’s new:Fall, a thriller about two friends who climb a 2,000-foot tower only to find themselves trapped at the top, originally included over 30 instances of a certain offensive word. The filmmakers deepfaked the picture to clean up the language, enabling the film to earn a rating that welcomes younger viewers,Varietyreported.How it works:Director and co-writer Scott Mann re-recorded the film’s actors reciting more family-friendly versions of the troublesome word. Then he used a generative adversarial network to regenerate the actors’ lip motions to match the revised dialog.
Behind the news:Neural networks are increasingly common in the edit suite.
Why it matters:Fall’s distributor Lionsgate determined that the movie would make more money if it was aimed at a younger audience. However, reshooting the offending scenes might have taken months and cost millions of dollars. AI offered a relatively affordable solution.We’re thinking:The global popularity of shows likeSquid Game, in which the original dialog is Korean, andLa Casa de Papel, in which the actors speak Spanish, suggest that dialog replacement could be a blockbuster AI application.
",['https://dl-staging-website.ghost.io/content/images/2022/08/ENSEMBLE.gif']
AI Jobs Grow in Pharma,"New data suggests the drug industry is hooked on AI.
What’s new:Pharmaceutical companies in several countries are hiring machine learning engineers at increasing rates, industry news publicationPharmaceutical Technologyreported. Most job openings are posted in the United States, though some countries in Europe and Asia are gaining ground.How it works:The publication analyzed data from GlobalData’s paywalleddatabase, which tracks job listings in a variety of industries and analyzes the text to group them into categories.
Behind the news:In a recent report, GlobalDataestimatedthat the pharmaceutical industry will spend over $3 billion on AI by 2025, driven largely by applications in drug discovery. The trend has also prompted major pharma companies including Astra-Zeneca, Pfizer, and Sanofi to acquire, invest in, or partner with startups. GlobalData counted 67 such partnerships in 2021, up from 23 in 2018.Why it matters:Bringing a new drug to market can take decades and costbillions of dollars. AI can cut time and costs in myriad ways, for instance by recognizing viable molecules without lab experimentation, identifying patients who might benefit from a drug, and predicting how patients might respond to them.
We’re thinking:Given the economic value of online advertising and product recommendations, many machine learning engineers — and an entire genre of machine learning approaches — are devoted to optimizing their results. Given the value of pharmaceuticals, we have no doubt that machine learning has immense potential in that domain as well. Similarly, a large body of specialized machine learning techniques is waiting to be developed for many industries.
",['https://dl-staging-website.ghost.io/content/images/2022/08/SAFETY.gif']
Self-Driving Safety Check,"Data from vehicle makers sheds light — though not much — on the safety of current autonomous and semi-autonomous vehicles.
What’s new:The United States National Highway Traffic Safety Administration (NHTSA)detailedcollisions over a 12-month period that involved cars that drive themselves or automate some driving tasks. This is the first edition of what promises to be an annual report.
Going driverless:Fully automated driving systems (often called ADS) that operate without a driver behind the wheel aren’t yet widely available. For the most part, they're being tested in a small number of designated areas. Manufacturers must report incidents that occurred within 30 seconds of engaging an ADS or resulted in property damage or personal injury.
Extra hands on the wheel:Semi-autonomous vehicles equipped with automated driving assistance systems (known as ADAS) require a flesh-and-blood driver but can steer, accelerate, and decelerate on their own. Manufacturers must report crashes that caused an airbag to inflate, required a car to be towed, or sent someone to a hospital.
Yes, but:The report doesn’t tally miles driven by fully autonomous, semi-autonomous, and conventional vehicles, nor at what speeds they traveled. Without that information, there's no way to derive a collision rate per mile or evaluate the severity of injuries at various speeds. Moreover, the report includes only crashes known to manufacturers. It may have missed those that weren’t reported to law enforcement or through consumer complaints. (This may explain the high numbers for Tesla, which harvests data directly from its vehicles.)
Why it matters:Vehicle safety is a life-and-death matter. Fully autonomous cars may not reach the market for years, but a degree of automated driving is commonplace: Vehicles that can steer, accelerate, and decelerate temporarily with a human presentaccountedfor 30 percent of new car sales in the U.S. during the fourth quarter of 2020.We’re thinking:Initial efforts to collect data, however incomplete, often lead to better data in the future. We hope that NHTSA improves these reports in the coming years by adding the total miles, as well as subdivisions according to in-town and on-highway speed ranges, driven by each of the two automation classes as well as unassisted humans.
",['https://dl-staging-website.ghost.io/content/images/2022/08/HOLOCAUST.gif']
Identifying Faces of History,"A face recognition system is helping identify victims of the Holocaust.
What’s new:From Numbers to Namesmatches individuals to faces in publicly available images related to the genocide of European Jews between 1941 and 1945.How it works:Built by Google software engineer Daniel Patt and financier Jed Limmer, the site matches images uploaded by users with faces from the United States Holocaust Memorial Museum’sphoto collection.
Behind the news:Deep learning plays a growing role in understanding history.
Why it matters:Roughly 11 million people were systematically murdered by the government of Nazi Germany for their ethnicity, religion, political beliefs, or sexual orientation. Identifying the victims doesn’t erase the crime of their deaths, but it can help bring closure to their relatives and strengthen our resolve to make sure nothing similar ever happens again.We’re thinking:While lives lost to war havedecreasedsignificantly over the decades, humanity has yet to progress beyond senseless killing. Learning about the atrocities of the past helps us view current events — such as the Russia-Ukraine war — with a critical eye and stand firm for human rights.
",['https://dl-staging-website.ghost.io/content/images/2022/08/PROTEIN.gif']
Where Drones Fly Free,"Autonomous aircraft in the United Kingdom are getting their own superhighway.What’s new:The UK governmentapprovedProject Skyway, a 165-mile system of interconnected drone-only flight routes. The airspace is scheduled to open by 2024.How it works:The routes, each just over six miles wide, will connect six medium-sized English cities including Cambridge, Coventry, Oxford, and Rugby. They avoid forested or ecologically sensitive areas, as well as major cities like London and Birmingham.
Behind the news:Project Skyway is the largest proposed designated drone flight zone, but it’s not the only one.
Yes, but:Although Skyway includes a collision-avoidance system, it’s not designed to prevent accidents during takeoff and landing, when they’re most common. Moreover, it's not yet clear whether the plan includes designated takeoff and landing sites. “The problem is what happens when you're 10 feet away from people,” one aerospace engineertoldthe BBC.Why it matters:Drones are restricted from flying in most places due to worries that they could interfere — or collide — with other aircraft. By giving them their own airspace, the UK is allowing drones to deliver on their potential without putting other aircraft at risk.We’re thinking:Figuring out how to operate drones safely has proven one of the most difficult aspects of deploying them in commercial applications. This project is a big step toward ironing out the regulatory bugs and also provides a relatively safe space to address technical issues.
",['https://dl-staging-website.ghost.io/content/images/2022/08/OPENSOURCE.gif']
Large Language Models Unbound,"A worldwide collaboration produced the biggest open source language model to date.What’s new:BLOOMis a family of language models built by the BigScience Research Workshop, a collective of over1,000 researchersfrom 250 institutions around the globe.How it works:BLOOM is a transformer model that emulates OpenAI’sGPT-3. It was trained on a custom 1.6 billion terabyte dataset to generate output in any of 46 human languages and 13 programming languages.
Behind the news:BigScience began in May 2021 as a year-long series of workshops aimed at developing open source AI models that are more transparent, auditable, and representative of people from diverse backgrounds than their commercial counterparts. Prior to BLOOM, the collaboration released theT0family of language models, which were English-only and topped out at 11 billion parameters.Why it matters:Developing large language models tends to be the province of large companies because they can afford to amass gargantuan datasets and expend immense amounts of processing power. This makes it difficult for independent researchers to evaluate the models’ performance, including biased or harmful outputs. Groups like BigScience and EleutherAI, whichreleasedits own open source large language model earlier this year, show that researchers can band together as a counterweight to Big AI.We’re thinking:Just over two years since GPT-3’s debut, we have open access to large language models from Google, Meta, OpenAI, and now BigScience. The rapid progress toward access is bound to stimulate valuable research and commercial projects.
Course 3 of theMachine Learning Specialization, “Unsupervised Learning, Recommender Systems, and Reinforcement Learning,” is available! Learn unsupervised techniques for anomaly detection, clustering, and dimensionality reduction. Build a recommender system, too!Enroll now
","['https://dl-staging-website.ghost.io/content/images/2022/08/DeepLearningAI_Banner_Stanford_Launch_1200x628-V2_Artboard-2-copy.png', 'https://dl-staging-website.ghost.io/content/images/2022/08/BEES.png']"
Protection for Pollinators,"A machine learning method could help chemists formulate pesticides that target harmful insects but leave bees alone.What’s new:Researchers at Oregon State Universitydevelopedmodels that classify whether or not a chemical is fatally toxic to bees. The authors believe their approach could be used to screen pesticide formulations for potential harm to these crucial pollinators.How it works:The authors trained two support vector machines to classify molecules as lethal or nonlethal. The dataset was 382 graphs ofpesticide molecules, in which each atom is a node and each bond between atoms is an edge, labeled for toxicity. The researchers used a different method to train each model.
Results:The two models performed similarly. They accurately classified 81 to 82 percent of molecules as lethal or nonlethal to bees. Of the molecules classified as lethal, 67 to 68 percent were truly lethal.Behind the news:Bees play a crucial role inpollinatingmany agricultural products. Without them, yields of important crops like cotton, avocados, and most fruit would drop precipitously.Numerous studieshave shown that pesticides are harmful to bees. Pesticides have contributed to increased mortality amongdomesticated honey beesas well as a decline in the number ofwild bee species.Why it matters:Pesticides, herbicides, and fungicides have their dangers, but they help enable farms to produce sufficient food to feed a growing global population. Machine learning may help chemists engineer pesticides that are benign to all creatures except their intended targets.We’re thinking:It’s good to see machine learning take some of the sting out of using pesticides.
",['https://dl-staging-website.ghost.io/content/images/2022/08/JURY.gif']
AI War Chest Grows,"Western nations are making a substantial investment in AI.
What’s new:The North Atlantic Treaty Organization (NATO), which includes the United States, Canada, and much of Europe,announceda €1 billion venture capital fund that will focus on technologies including AI. The move adds to the growing momentum behind AI for warfare.How it works:The alliance’s Innovation Fund is bankrolled primarily by 22 of the alliance’s 30 members with additional pledges from other members. It will disburse its money over 15 years.
Behind the news:NATO members recently boosted their individual AI budgets as well.
Why it matters:Besides autonomous weaponry, AI has numerous military applications that confer strategic and tactical advantages. In the Russian invasion of Ukraine alone, AI has been used toidentifyenemy soldiers,combatpropaganda, andinterceptcommunications.We’re thinking:The rising tide of military AI adds urgency to calls for international agreements on how the technology can be used in warfare. We support the United Nations’ proposedbanon autonomous weapons.
",['https://dl-staging-website.ghost.io/content/images/2022/07/UVEYE.gif']
Auto Diagnosis,"A drive-through system automatically inspects vehicles for dents, leaks, and low tire pressure.
What’s new:General Motors is giving its dealerships an option toinstalla visual inspection system from UVeye. Volvostrucka similar deal with the Tel Aviv startup in March.How it works:UVeye’s technology is designed to cut the time it takes to inspect a vehicle from minutes, possibly hours, to seconds. The company offers three systems to be installed on a service center’s premises for an undisclosed subscription fee.
Behind the news:General Motors and Volvo separately invested undisclosed sums in UVeye, as have Honda, Toyota, and Škoda, a Volkswagen subsidiary. Several General Motors dealers around the U.S. already use its technology for vehicle checkups; the new deal will make it available to all 4,000. Volvo uses UVeye scanners on its assembly lines and offers incentives to dealerships to use them as well.Why it matters:A computer vision system that completes inspections in seconds can free mechanics to focus on more critical tasks, help dealers evaluate trade-ins, and give customers confidence that service stations are addressing real issues.We’re thinking:Autonomous driving is the first automotive application for AI that many people think of, but other important tasks are easier to automate. Streamlining routine maintenance is one. Others includeassessing insurance claimsandoptimizing traffic patterns.
Launching today: Course 3 of theMachine Learning Specialization, “Unsupervised Learning, Recommender Systems, and Reinforcement Learning.” Learn to train models using unsupervised clustering, generate recommendations via collaborative filtering, and build deep reinforcement learning models!Enrollto #BreakIntoAI
","['https://dl-staging-website.ghost.io/content/images/2022/07/DeepLearningAI_Banner_Stanford_Launch_1200x628-V2_Artboard-2-copy-2--1-.webp', 'https://dl-staging-website.ghost.io/content/images/2022/07/SCOUT_Recrop_600px.gif']"
On the Ball,"Neural networks are spotting up-and-coming players for some of the best teams in football (known as soccer in the United States).
What’s new:AiSCOUTuses computer vision to grade amateur footballers and recommends those who score highest to representatives of professional teams,Forbesreported.
How it works:Amateurs upload videos of themselves performing eight drills such as passing, shooting, and dribbling around cones. AiSCOUT scores the performance on a scale of 0 to 2 relative to others it has evaluated (a score of 1.7 might prompt an in-person trial with a top team).
Behind the news:Machine learning is being used to improve performance in a wide range of sports.
Why it matters:Talent scouts have been obsessed with data since the days of pencil and paper. Machine learning can help clubs to cast a wider net and give far-flung aspirants a shot at going pro.
We’re thinking:We get a kick out of this app!
",['https://dl-staging-website.ghost.io/content/images/2022/07/MOTOR.gif']
What AI Employers Want,"A website that aggregates AI jobs revealed the roles that are most in-demand.What’s new:Ai-jobs.netpublishedits second annual list of the job titles that appeared most frequently in its listings. The site, which pulls from various hiring platforms andsellsads to employers, is maintained by Foorilla, a Zurich-based consultancy.What they found:The list covers over 100 job titles in more than 2,500 listings posted between June 2021 and June 2022. The rankings are approximate because the listings in the site’s database change by the hour, an ai-jobs.net representative toldThe Batch. The snapshot used to compose the rankings is availablehere.
Why it matters:AI jobs continue to proliferate! Machine learning engineer was thefourth-fastest growing U.S. job titleon the professional social network Linkedin between January 2017 and July 2021, but demand is growing for many other titles.We’re thinking:Look at all the times the word “data” appears in the top titles! This speaks to the growing importance of systematically engineering the data used in AI systems.
",['https://dl-staging-website.ghost.io/content/images/2022/07/EU.webp']
Keep Your AIs on the Road,"The European Union passed a law that requires new vehicles to come equipped with automated safety features.What’s new:The new Vehicle General Safety Regulationcompelsmanufacturers of new vehicles to include as standard features automatic speed control, collision avoidance, and lane-keeping. The systems cannot collect biometric data, and drivers must be able to switch them off. The law, which does not apply to two- or three wheeled vehicles, will take effect in July 2024.How it works:Some requirements apply to all vehicles. Others govern light and heavy commercial vehicles:
Behind the news:Automated safety features are increasingly common. In the U.S.,30 percentof new vehicles sold in the fourth quarter of 2020 were able to accelerate, decelerate, and steer on their own.
Why it matters:The European Commissionestimatesthat 19,800 people died in road accidents in 2021. AI-powered safety features may help the governing body reach its goal of halving road fatalities by 2030 and eliminating them altogether by 2050.We’re thinking:Although these regulations were designed to address important safety concerns, some of them, such as automatic speed monitoring and feedback, can also reduce vehicle emissions, which would be good for the planet.
Why did Mahsa Zamanifard, a sales executive with an interest in data analysis, enroll in Andrew Ng’s Machine Learning course? Let hertell you herself! #BreakIntoAI too with the newMachine Learning Specialization
",['https://dl-staging-website.ghost.io/content/images/2022/07/CARBON--1-.gif']
When Self-Driving Cars Won’t Drive,"Dormant robotaxis are snarling traffic on the streets of San Francisco.What’s new:Cruise self-driving cabs lately have stalled en masse,Wiredreported.What's happened:Vehicles from Cruise, a subsidiary of automotive giant General Motors, lost contact with the company’s servers at least four times since May. The outages leave the cars, which don’t carry human safety drivers, unable to move for substantial periods of time.
Behind the news:On June 2, Cruise acquired the first-everpermitto collect robotaxi fares in San Francisco. The permit allows 30 vehicles to operate between 10 p.m. and 6 a.m. They’re authorized to drive up to 30 miles per hour in clear weather.Why it matters:Rolling out self-driving cars has proven to be more difficult than many technologists realized. Cruise has made great progress with its taxi program, reducing the hazard of autonomous vehicles in motion sufficiently to gain a permit to operate on public roads. But centralized control brings its own hazards — and a fat target for hackers and saboteurs.We’re thinking:Why do self-driving cars need internet access to drive? Many autonomous systems actually rely on remote humans to monitor and help them operate safely. A failsafe for loss of contact with remote servers is in order, but this is very challenging with today’s technology.
",['https://dl-staging-website.ghost.io/content/images/2022/07/RESPONSIBLE--1-.gif']
Ethical AI 2.0,"Microsoft tightened the reins on both AI developers and customers.What’s new:The tech titan revised itsResponsible AI Standardand restricted access to some AI capabilities accordingly.
Taking responsibility:The update is intended to support six core values.
Face off:To comply with its new guidelines, the company limited AI services offered via its Azure Cloud platform.
Behind the news:Microsoft published itsfirstResponsible AI Standard in 2019 but concluded that the initial draft was vague. The new version is intended to give developers clearer directions for compliance. To that end, the company alsoprovidesnearly 20 tools intended to aid developers in building responsible AI systems. For instance, HAX Workbook helps make AI systems easier to use, InterpretML helps explain model behavior, and Counterfit stress-tests security.Why it matters:Regulation in the United States and elsewhere lags rising concern that AI is growing more capable of causing harm even as it becomes enmeshed in everyday life. Microsoft’s latest moves represent a proactive effort to address the issue.We’re thinking:Hundreds of guidelines have been drafted to govern AI development. The efforts are laudable, but the results are seldom actionable. We applaud Microsoft for working to make its guidelines more concrete, and we’re eager to see how its new standards play out in practice.
Join us on August 3, 2022, forAccelerating Your AI Career! Andrew Ng and other industry experts will discuss core skills for a career in machine learning and how learners can develop them. A demo of theMachine Learning Specializationwill follow.
","['https://dl-staging-website.ghost.io/content/images/2022/07/Accelerating-Your-AI-Career.MLS.3-Aug_The-Batch-Image.png', 'https://dl-staging-website.ghost.io/content/images/2022/07/DALLEMINI--1-.gif']"
Text-to-Image Goes Viral,"A homebrew re-creation of OpenAI’s DALL·E model is the latest internet sensation.What’s new:Craiyon has been generating around 50,000 user-prompted images daily, thanks to its ability to produce visual mashups likeDarth Vader ice fishingandphotorealistic Pokemon characters,Wiredreported. You can try ithere.
How it works:U.S. machine learning consultantBoris Daymabuilt Craiyon, formerly known as DALL·E Mini, from scratch last summer. It went viral in early June following upgrades that improved its output quality.
Behind the news:Fans of the word-guessing game Wordle may enjoyWordalle, which shows players six images generated by Craiyon and asks them to guess the prompt.
Why it matters:Advances in machine learning are unlocking new ways for people to amuse themselves, from generating images ofimaginary pizzasto makingsuperheroes lip-synch popular songs. Enabling the internet audience to remix popular culture in unprecedented ways unleashes imagination and good humor worldwide.We’re thinking:OpenAI says it controls access to DALL·E out of concern that people might use it to indulge their worst impulses. Craiyon’s deluge of delightful output is an encouraging counterpoint.
",['https://dl-staging-website.ghost.io/content/images/2022/07/REGULARIZATION--1-.gif']
Tracking Changes on Earth’s Surface,"Computer vision systems are scanning satellite photos to track construction on the Earth’s surface — an exercise in behavior recognition on a global scale.What’s new:Space-based Machine Automated Recognition Technique (Smart) is a multi-phase competition organized by the United States government. So far, it has spurred teams to develop systems that track large-scale construction in sequential satellite images,Wiredreported.The challenge:Barren earth, dump trucks, and large cranes are common markers of construction sites. But they aren’t always present at the same time, and they may be found in other contexts — for instance, dump trucks travel on highways and large cranes sit idle between jobs. Moreover, different satellites have different imaging systems, orbits, schedules, and so on — a stumbling block for automated classification. In the first phase of the contest, from January 2021 through April 2022, competitors built models that correlate features that were present in the same location but not at the same time, regardless of the image source.How it works:The Intelligence Advanced Research Projects Activity (IARPA), a U.S. intelligence agency, organized the challenge.
Results:Judges evaluated contestants based on how they approached the problem and how well their models performed. The jury came from institutions including NASA’s Goddard Space Flight Center, U.S. Geological Survey, and academic labs.
Behind the news:Satellite imagery is a major target of development in computer vision. Various teams are tracking the impact ofclimate change, predicting volcaniceruptions, and watching China’s post-Covid economyrebound.Why it matters:Photos taken from orbit are akeyresource for intelligence agencies. Yet the ability to see changes on Earth’s surface is a potential game changer in fields as diverse as agriculture, logistics, and disaster relief. It’s impractical for human analysts to comb the flood of images from more than 150 satellites thatobserveEarth from orbit. By automating the process, machine learning opens huge opportunities beyond Smart’s focus on national security.We’re thinking:Large-scale events on Earth are of interest to all of the planet’s inhabitants. We’re glad to see that the contestants will be able to use the models they build, and we call on them to use their work to help people worldwide.
",['https://dl-staging-website.ghost.io/content/images/2022/07/MAYFLOWER--1-.gif']
"AI AI, Captain!","An autonomous research ship crossed the Atlantic Ocean — with a few pit stops to address challenges along the way.What’s new:Built by IBM and marine research nonprofit ProMare, the Mayflower Autonomous Ship 400 (MAS400) last weekcompleteda voyage from Plymouth, England, to Plymouth, Massachusetts.How it works:The vessel navigates autonomously using a system based on IBM’sOperational Decision Manager, a rules-based system that integrates data from machine learning and other sources to adhere to conventions for maritime navigation. It carries no human crew, but ProMare can control it remotely if necessary.
Choppy waters:The passage from Plymouth to Plymouth, which originally was scheduled to commemorate the 400-year anniversary of Pilgrims who traveled from England to America to escape religious persecution, ran a year late. It was supposed to take place over three weeks in June 2021, but less than a week into the first attempt, a power issue forced ProMare to guide the vessel back to England for repairs. The second attempt lasted over two months, with two unplanned port calls in the Azores and Nova Scotia to address generator issues and battery defects.Behind the news:Autonomous vessels are increasingly plying the seven seas.
Why it matters:Removing the crews from ships can save space, fuel, and money. The industry has taken notice, and the International Maritime Organization isdraftingrules to adapt maritime regulation for autonomous craft.We’re thinking:Let this ship’s voyage be a lesson: You may encounter setbacks, but persist and you will arrive at your destination — schooner or later.
“The Machine Learning course explained mathematical concepts, and I found the programming approach intuitive for a non computer-science major. It helped me get into a master’s degree program in data science.”— Jose Eduardo Santo.Enroll in the Machine Learning Specialization
","['https://dl-staging-website.ghost.io/content/images/2022/07/MLS_Learner_1200x628_A-1_Artboard-1-copy-5.png', 'https://dl-staging-website.ghost.io/content/images/2022/07/EVERLAW--1-.gif']"
Order in the Court,"Machine learning is helping lawyers sift through mountains of documents to find evidence.What’s new:The legal technology company Everlawlauncheda clustering feature that automatically organizes up to 25 million documents for lawyers gathering evidence to be used during a trial.How it works:The new feature analyzes text documents via unsuperviseddensity-based clusteringto build a visual map of word clouds.
Making headlines:ProsecutorsusedEverlaw’s software during the high-profile trial of Theranos co-founder Elizabeth Holmes. Among 1 million documents, they found 40 that implicated her criminal intent to defraud investors.Behind the news:AI increasingly contributes to legal proceedings.
Why it matters:Tools that streamline the mundane, high-stakes chore of sifting through documents could help lawyers and their aides  discover evidence they might otherwise overlook. This may be a boon especially for less-privileged plaintiffs and defendants, as some legal scholars have longheldthat the resource-intensive discovery process favors the wealthy.We’re thinking:There’s a strong case for AI in legal practice.
",['https://dl-staging-website.ghost.io/content/images/2022/07/MASKED.gif']
More Autonomy for Martian Drone,"The United States space agency is upgrading the system that pilots its helicopter on the Red Planet.What’s new:The National Aeronautics and Space Administration (NASA) announced that Ingenuity, a drone sent to Mars as part of its 2020 mission to Mars, will receive a new collision-avoidance algorithm,Wiredreported. Ingenuity acts as a scout for the Perseverance rover as it travels from relatively flat, featureless areas to more hazardous terrain.How it works:NASA engineers on Earth plot waypoints in a simulation. They transmit the waypoints to the rover, which relays them to the drone, where algorithmsdetermineits path based on input from an onboard camera, altimeter, and other devices.
Behind the news:Ingenuity was designed for only five flights, but has flown 29 times since its debut in April 2021. NASA hopes to extend its lifespan even further by letting it hibernate through the Martian winter. Solar energy is scarce for four months starting in July, and hibernation will enable the craft to devote its battery to keeping its electronics warm. The team plans to install the upgrade during that period.Why it matters:Ingenuity’s evolving combination of Earthbound direction and local autonomy lays the groundwork for missions deeper into the solar system, where the delay in communications — up to 24 minutes between Earth and Mars — will be even longer. For example, theDragonflyoctocopter is scheduled to take off for Titan’s soupy atmosphere in 2027.We’re thinking:Over-the-air software updates aren’t only for terrestrial devices!
",['https://dl-staging-website.ghost.io/content/images/2022/06/ADS-1.gif']
U.S. Acts Against Algorithmic Bias,"Regulators are forcing Meta (formerly Facebook) to display certain advertisements more evenly across its membership.What’s new:The United States government compelled Meta toreviseits ad-placement system to deliver ads for housing to members regardless of their age, gender, or ethnicity. The company is voluntarily rebalancing its distribution of ads for credit and employment as well.How it’s changed:The new algorithm will control ads that appear to U.S. users of Facebook, Instagram, and Messenger. Meta will roll it out by December.
Behind the news:The update is part of asettlementbetween Meta and the U.S. Justice Department, whichfoundthat the company had violated laws against discrimination in housing. Meta also agreed to terminate a differentsystemthat was intended to enforce a more even distribution of ads but was found to have the opposite effect. It will pay a fine of $115,054, the maximum penalty under the law.Why it matters:AI technology is largely unregulated in the U.S. But that doesn’t mean the federal government has no jurisdiction over it, especially when it migrates into highly regulated sectors. Facebook once hosted ads forcredit cardsthat excluded younger people,job postingsthat excluded women, andhousing adsthat excluded people by race. Regulators who oversee civil rights didn’t settle for mere changes in Meta’s advertising guidelines and ultimately forced it to alter the algorithm itself.We’re thinking:Meta’s periodic reports will provide some evidence whether or not regulation can mitigate algorithmic bias. Still, we wonder whether regulators can craft effective rules. Data can be sliced in a variety of ways, and it can be very difficult to detect bias against a particular group within a slice. For example, a system that appears not to discriminate by gender on average may do so, say, within a particular type of town or when handling a certain sort of housing. Given the slow progress of legislation and the rapid development of technology, we worry that regulators will always trail the companies they regulate.
How important is querying databases to your role? Do you use Python or R? Take the Workera 2022Roles in Data and AIsurvey and help us upskill the data and AI workforce.Tell us about your roleand get a chance to win a $100 gift card!
","['https://dl-staging-website.ghost.io/content/images/2022/06/test--6-.png', 'https://dl-staging-website.ghost.io/content/images/2022/06/PAPERCUP--1-.gif']"
Speaking Your Language,"A startup that automatically translates video voice overs into different languages is ready for its big break.What’s new:London-basedPapercupoffers a voice translation service that combines algorithmic translation and voice synthesis with human-in-the-loop quality control. A recentfunding roundsuggests that investors have a measure of confidence in the company’s approach.How it works:Video producers can upload clips and specify an output language such as English, Mandarin, Italian, Latin American Spanish, or Brazilian Portuguese. They can choose among synthesized voices that represent a range of gender and age, and tweak the voice’s pitch and character and alter its emotional expression as “happy,” “sad,” “angry,” and the like.
Yes, but:Keeping in a human in the loop to oversee an operation as sensitive as language translation makes good sense. However, current technology can take this automation a good deal further. For instance, Papercup offers a selection of voices rather than generating afacsimile of the original voicein a new language. It doesn’tconform video of the speaker’s mouthto new languages — the mouth continues to form words in one language while the synthesized voice intones another. Nor does itdemixand remix vocal tracks that are accompanied by background music or other sounds.Why it matters:Automated voice over translation is yet another task in which machines are vying to edge out human workers. On one hand, automation can make translation available to producers on a tight budget, dramatically extending their reach to new markets and use cases. On the other hand, we worry that performing artists will lose work to such systems and support efforts toprotecttheir livelihoods.We’re thinking:Earlier this week, Nando de Freitas — DeepMind research director, Oxford professor, and former officemate of Andrew Ng’s —urgedus on Twitter to translate the newly updatedMachine Learning Specializationinto every language. We're working withCoursera’s global translator communityto create subtitles, but we're always eager to have options.
",['https://dl-staging-website.ghost.io/content/images/2022/06/GRAPHTRANSFORMER--1-.gif']
Toward Next-Gen Language Models,"A new benchmark aims to raise the bar for large language models.What’s new:Researchers at 132 institutions worldwide introduced theBeyond the Imitation Game benchmark(BIG-bench), which includes tasks that humans perform well but current state-of-the-art models don’t.How it works:The authors selected over 200tasksbased on 10criteriasuch as being sensible to humans, not solved by current language models, and “not solvable by memorizing the internet.” Many involve atypical problems such as identifying a single move that will win a game of chess, guessing a movie title from a series of emojis, and playing a role in a mock courtroom trial.
Results:No model, regardless of size, outperformed the best-performing human on any task. However, for some tasks, the best-performing model beat the average human. For example, answering multiple-choice questions about Hindu mythology, the best model scored around 76 percent, the average human scored roughly 61 percent, and the best human scored 100 percent (random chance was 25 percent). Generally, larger models performed better than smaller ones. For example, BIG-G’s average accuracy on three-shot, multiple-choice tasks was nearly 33 percent with a few million parameters but around 42 percent with over a hundred billion parameters.
Why it matters:BIG-bench’s creators argue that benchmarks likeSuperGLUE,SQuAD2.0, andGSM8Kfocus on narrow skills. Yet the latest language models, after pretraining on huge datasets scraped from the internet, showunexpected abilitiessuch as solving simple arithmetic problems. BIG-bench’s diverse, few-shot tasks give researchers new ways to track such emergent capabilities as models, data, and training methods evolve.We’re thinking:Devising tasks that can’t be solved by memorizing the internet may push researchers to develop algorithms — including ones that enable complex forms of reasoning — that generalize well even with limited amounts of training data.
",['https://dl-staging-website.ghost.io/content/images/2022/06/WIND.webp']
Wind in the Forecast,"Machine learning is making wind power more predictable.What’s new:Engie SA, a multinational energy utility based in France, is the first customer for an AI-powered tool from Google that predicts the energy output of wind farms,Bloombergreported. The company plans to deploy the system on 13 wind farms in Germany.How it works:Google’s DeepMind subsidiarytraineda neural network to predict energy output from wind farms up to 36 hours ahead of time. The training data included historical weather forecasts and unspecified data from wind turbines.
Behind the news:Google isn’t the only firm employing machine learning to squeeze more electricity out of renewable resources.
Why it matters:Wind and solar power are notoriously uncertain, leading utilities to default to fossil fuels, which are available on-demand. Predicting wind-energy yields can reduce some of that uncertainty, helping utilities benefit from advantages such as renewables’lower overheadand easing dependence on fossil-fuel and nuclear sources.
We’re thinking:Stopping climate change isn’t the only motivation to cut dependence on fossil fuels. The conflict in Ukraine has contributed to a global shortage of oil and gas, causing energy prices to spike. Alternative sources can help make the global economy less reliant on oil producers and more resilient to disruptions in supply.
The DeepLearning.AI community continues to grow, thanks to Pie & AI ambassadors like Emilio Soria-Olivas of Valencia, Spain. We’re thrilled to share his accomplishments.Sign upto become a Pie & AI ambassador and learn how you could be featured as well!
","['https://dl-staging-website.ghost.io/content/images/2022/06/pie---Ai-amabssador-soptlight_The-Batch-Emilio.png', 'https://dl-staging-website.ghost.io/content/images/2022/06/ezgif.com-gif-maker--31--1.gif']"
Deep Doo-Doo,"People who suffer from gastrointestinal conditions such as irritable bowel syndrome are number two when it comes to describing the characteristics of their own poop.What’s new:The smartphone appDietahelps patients to keep gastrointestinal illnesses in check by tracking their own behaviors and symptoms. It includes a computer vision model that recognizes medically salient characteristics of excrement as accurately as doctors and better than most patients, a recentstudyfound.How it works:The app enables patients to log symptoms such as nausea, constipation, and abdominal pain; behaviors like exercise, sleep, and meals; treatments including medications, supplements, and diet; and feelings of illness or wellbeing. It also helps patients experiment on themselves, recommending lifestyle changes and treatments and enabling patients to forward the results to caregivers. A computer vision model classifies feces according to characteristics that are useful in diagnosis.
Behind the news:Machine learning engineers have trained other models to peer into the toilet.
Why it matters:Roughly 40 percent of adults worldwide may suffer from gastrointestinal conditions, according to a 2021study. Tracking bowel movements helps to diagnose these conditions earlier and more accurately.We’re thinking:We’re grateful that someone — other than us — builds models that classify the Bristol Stool Scale.
",['https://dl-staging-website.ghost.io/content/images/2022/06/DEEPNET.gif']
Ethics Team Scuttles Taser Drone,"A weaponized AI system intended to protect students has been grounded.What’s new:Axon, which makes law-enforcement equipment such as tasers and body cameras,canceleda plan to sell remote-controlled drones capable of firing electroshock darts to incapacitate attackers at schools, businesses, and other public places. The company, which hadannouncedthe taser drone in early June,shelvedit days later after the majority of its independent ethics boardresignedin protest.How it works:The canceled flier, which was based on the company’s existingAxon Airsurveillance drone, was to include a camera as well as a taser. A human operator would decide when to fire its electroshock projectile.
Behind the news:Axon’s announcement came about a week after a gunmankilled19 students and two teachers at an elementary school in Texas. It was the27th school shootingwith casualties in the U.S. in 2022.Why it matters:The U.S. public is divided on how to address an ongoing epidemic of gun violence, with amajoritycalling for greater safety regulations that would limit who can own a firearm. The opposition, which believes that gun-control measures violate rights guaranteed by the nation’s constitution, favors solutions like armed guards and surveillance — proposals that align with Axon’s canceled drone.We’re thinking:Technological countermeasures are appealing in the face of repeated attacks on schools, workplaces, hospitals, and other public spaces. However,researchargues against increased security in favor of better safety regulations. Axon should have consulted its ethics committee before announcing the product, but it did the right thing by canceling it afterward.
Today is the day! Our brand-newMachine Learning Specialization, created in collaboration with Stanford Online, is live on Coursera! If you want to #BreakIntoAI, now is the time to take the first step.Enroll now
","['https://dl-staging-website.ghost.io/content/images/2022/06/specialization-name-banner--2-.png', 'https://dl-staging-website.ghost.io/content/images/2022/06/ezgif.com-gif-maker--34-.gif']"
Meta Decentralizes AI Effort,"The future of Big AI may lie with product-development teams.What’s new:Metareorganizedits AI division. Henceforth, AI teams will report to departments that develop key products.How it works:Prior to the reshuffle, the company’s Responsible AI, AI for Products, AI4AR (that is, for augmented reality), and Facebook AI Research teams were managed by a single division called Meta AI. This structure made it difficult to translate machine learning into marketable applications, according to chief technology officer Andrew Bosworth.
Shaky platform:AI teams who work for Meta’s flagship Facebook social platform have had a rocky few years.
Trend in the making?Meta isn’t the first large company to move AI teams closer to its product groups.
Why it matters:In 2019, 37 percent of large AI companies maintained a central AI group,The Wall Street Journalreported. Reorgs by Meta and others suggest that centralization hindered their ability to capitalize on AI innovations.We’re thinking:In a corporate setting, when a technology is new, a centralized team can make it easier to share learnings, set standards, and build company-wide platforms. As it matures, individual business units often gain the ability to manage the technology themselves and absorb experienced developers. Apparently this pattern — which we describe inAI For Everyone— is playing out in some leading AI companies.
",['https://dl-staging-website.ghost.io/content/images/2022/06/PHYSICS.gif']
DALL·E 2’s Emergent Vocabulary,"OpenAI’s text-to-image generatorDALL·E 2produces pictures with uncanny creativity on demand. Has it invented its own language as well?What’s new:Ask DALL·E 2 to generate an image that includes text, and often its output will include seemingly random characters. Giannis Daras and Alexandros G. Dimakis at University of Texasdiscoveredthat if you feed the gibberish back into the model, sometimes it will generate images that accord with the text you requested earlier.How it works:The authors devised a simple process to determine whether DALL·E 2’s gibberish has meaning to the model.
Results:The authors provide only a handful of quantitative results, but they are intriguing. They report that “a lot of experimentation” was required to find gibberish that produced consistent images.
Inside the mind of DALL·E 2:Inputs to DALL·E 2 are tokenized as subwords (for instance, apoploe may divide into apo, plo, e). Subwords can make up any possible input text including gibberish. Since DALL·E 2 was trained to generate coherent images in response to any input text, it’s no surprise that gibberish produces good images. But why does the author’s method for deriving this gibberish produce consistent images in some cases, random images in others, and a 50/50 combination of consistent and random images in still others? The authors and denizens of social media came up with a few hypotheses:
Why it matters:The discovery that DALL·E 2’s vocabulary may extend beyond its training data highlights the black-box nature of deep learning and the value of interpretable models. Can users benefit from understanding the model’s idiosyncratic style of communication? Does its apparent ability to respond to gibberish open a back door that would allow hackers to get results the model is designed to block? Do builders of natural language models need to start accounting for gibberish inputs? These questions may seem fanciful, but they may be critical to making such models dependable and secure.We’re thinking:AI puzzles always spur an appetite, and right now a plate of fresh wa ch zod rea would hit the spot!
",['https://dl-staging-website.ghost.io/content/images/2022/06/AI100.gif']
Standout Startups,"AI startups are creating high value across a wide variety of industries.What’s new:CB Insights, which tracks tech startups,publishedthe latest edition of the AI 100, its annual list of 100 notable AI startups. The list includes companies in healthcare, retail, transportation, finance, construction, media, and manufacturing. (Disclosure: The list includesLanding AI, where Andrew Ng is CEO, andBearing.ai, a portfolio company of AI Fund, the venture studio that he leads.)Early-stage stars:The authors considered 7,000 private companies headquartered around the world. They selected outstanding entries based on the factors that include number and types of investors, research and development activities, market potential, sentiment analysis of news reports, plus a proprietary score related to the startup’s target market, level of funding, and momentum.
Blasts from the past:Many of last year’s  AI 100 continue to gain momentum. They’ve raised $6 billion in aggregate since April 2021. Six are valued at over $1 billion, and nine were acquired or offered shares to the public. (SeeThe Batch’s coverage of the AI 100 in2021and2020.)Behind the news:CB Insights’ recentState of AIreport highlighted trends among AI startups during the first quarter of 2022.
Why it matters:The AI 100 confirms that AI is finding valuable applications beyond the technology’s stronghold among consumer-internet companies. It also highlights hot sectors for both entrepreneurs and funders. Healthcare and finance are perennial favorites among investors, while automation for warehouses and logistics receive steadily growing attention.We’re thinking:Investors, take note: U.S. companies received the lion’s share of AI startup funding, but the rest of the world is a rich source of talent for both existing startups and those yet to be formed.
Congratulations to Brigita Bizjak of Amsterdam! She’s making a positive impact on her local community as one of DeepLearning.AI’s global Pie & AI Ambassadors.Sign upto be a Pie & AI Ambassador and learn how you, too, could be featured!
Officials in charge of protecting children stopped using a machine learning model designed to help them make decisions in difficult cases.What’s new:The U.S. state of Oregon halted its use of an algorithm intended to identify children who may benefit from intervention,The Associated Pressreported. The state did not disclose the reason for the move. It came roughly one month after a similar algorithm used by the state of Pennsylvania, which inspired Oregon’s effort, came under criticism for bias.How it works:Oregon’s Department of Human Services developed theSafety at Screening Toolto help social workers screen reports of at-risk children. Social workers were empowered to decide whether to take action with respect to any given report.
Pennsylvania’s problem:Researchers at Carnegie Mellon Universityfoundsigns of bias in a similar tool used in Pennsylvania. That algorithm, which assesses the probability that a child will enter foster care within two years, is still in use.
Why it matters:Oregon’s decision to drop its learning algorithm sounds a note of caution for public agencies that hope to take advantage of machine learning. Many states have applied machine learning to ease the burden on social workers as both the number of child welfare cases has risen steadily over the past decade. However, the effort to automate risk assessments may come at the expense of minority communities whose members may bear the brunt of biases in the trained models.We’re thinking:We’re heartened to learn that independent researchers identified the flaws in such systems and public officials may have acted on those findings. Our sympathy goes out to children and families who face social and economic hardships, and to officials who are trying to do their best under difficult circumstances. We continue to believe that AI, with robust auditing for bias, can help.
The computational systems known as cellular automata reproduce patterns of pixels by iteratively applying simple rules based loosely on the behavior of biological cells. New work extends their utility from reproducing images to generating new ones.What’s new:Rasmus Berg Palm and colleagues at IT University of Copenhagen developed an image generator calledVariational Neural Cellular Automata(VNCA). It combines a variational autoencoder with aneural cellular automaton, which updates pixels based on the output of a neural network and the states of neighboring pixels.Key insight:A variational autoencoder (VAE) learns to generate data by using an encoder to map input examples to a distribution and a decoder to map samples of that distribution to input examples. Any architecture can serve as the decoder, as long as it can reconstruct data similar to the inputs. Given a distribution, a neural cellular automaton can use samples from it to generate new, rather than predetermined, data.How it works:VNCA generates pixels by updating a grid of vectors, where each vector is considered a cell and each cell corresponds to a pixel. The encoder is a convolutional neural network, and the decoder is a neural cellular automaton (in practical terms, a convolutional neural network that updates vectors depending on the states of neighboring vectors). The authors trained the system to reconstruct images in theMNISTdataset of handwritten digits.
Results:The authors showed that a cellular automaton can generate images, though not very well at this point. They evaluated VNCA using log likelihoods innatural units of information (nats), which gauge similarity between the system’s output and the training data (higher is better). VNCA achieved -84.23 nats, worse than the -77 nats achieved on MNIST by state-of-the-art models such asNVAEandBIVA.Why it matters:This work demonstrates that a neural cellular automaton can generate new images. While it shows no clear advantage of using a neural cellular automaton in a VAE, the combination might lend itself to useful applications. For instance, neural cellular automata have an inherent regenerative ability: Deface an image, and they can regrow the damaged pixels. Thus a VNCA-type approach might be useful for image inpainting. Given an image, the encoder could map it to a Gaussian distribution. Then you could damage the image where you wanted to change it, sample from the distribution, and use the decoder to generate novel pixels in that area.Yes, but:This approach may be challenging to scale. VNCA’s decoder used only 1.2 million parameters rather than the hundreds of millions used in other high-performing decoders. Adding parameters would increase its computational cost significantly, since it updates cells repeatedly based on the states of neighboring cells.We’re thinking:Deep learning offers a wideningarrayof neural image generators: GANs, VAEs, diffusion models, normalizing flows, and more. While each has its advantages and disadvantages, together they amount to an enticing playground for synthesizing data and producing visual art.
","['https://dl-staging-website.ghost.io/content/images/2022/06/pie---Ai-amabssador-soptlight_The-Batch-Brigita.png', 'https://dl-staging-website.ghost.io/content/images/2022/06/OREGON.gif', 'https://dl-staging-website.ghost.io/content/images/2022/06/AUTOMATA--1-.gif']"
Actors Act Against AI,"Performing artists are taking action to protect their earning power against scene-stealing avatars.What’s new:Equity, a union of UK performing artists,launcheda campaign to pressure the government to prohibit unauthorized use of a performer’s AI-generated likeness. The unionpublishedtips to help artists who work on AI projects exercise control over their performances and likenesses.Protections for performers:Equity demands that the UK revise existing copyright laws and adopt guidelines enacted by other jurisdictions.
What performers think of AI:Equity conducted a survey of its members between November 2021 and January 2022. Among the 430 people who responded:
Why it matters:While synthetic images, video, and audio contribute to countless exciting works, they’re an obvious source of concern for artists who wish to preserve — never mind increase — their earning power. These developments also affect members of the audience, who may find that their favorite performers have less and less to do with the productions they nominally appear in.We’re thinking:Using autotune to fix a wayward vocal performance doesn’t require the performer’s permission (though perhaps it should). The emerging generation of media production tools can generate performances entirely without the artist’s participation, further concentrating power in the hands of studios that own the technology. Defining the legal and ethical boundaries of generated media should help tip the balance toward performers, and it might lead to more fruitful creative collaborations between artists and machines.
",['https://dl-staging-website.ghost.io/content/images/2022/06/CONTENT--1-.gif']
Winning The Google Game,"AI startups are helping writers tailor articles that appear near the top of Google’s search results.What’s new:At least 14 companies sell access to software that uses GPT-3, the language model from OpenAI, to generate headlines, product descriptions, blog posts, and video scripts,Wiredreported.How it works:The services enable people who have little experience or skill in writing to make content that’s optimized for web search engines.
Machine privilege:Google’s guidelinesstatethat it may take action against automatically generated content. However, a Google spokesperson toldWiredthat the company may take a more lenient approach toward generated text that has been designed to serve readers rather than manipulate search results.Behind the news:Neural networks are reaching into video production, too. Given a script, Synthesiaproducescustomized videos, rendered by a generative adversarial network, aimed at corporate customers. Given a finished video, Mumbai-based Videoverse tags key highlights andrenders them into clipsoptimized for sharing on social media.Why it matters:Producing text for online marketers is an early commercial use case for text-generation models. The tech gives people who don’t specialize in marketing a leg up and raises the bar for professional writers — assuming it produces consistently high-quality output. In any case, AI has found a lucrative place in advertising and marketing, helping to drive$370 billionin ad sales this year, according to the marketing agency GroupM.We’re thinking:AI may write compelling marketing copy, but it’s still a long way from producing a great newsletter. Right?!
In FourthBrain’s newIntroduction to MLOpscourse, you’ll walk through the AI product life cycle by building a minimum viable product using the latest tools. This live course meets on Tuesdays from July 5 to July 26, 2022, 5 p.m. to 8 p.m. Central European Summer Time. Join us!Learn more
","['https://dl-staging-website.ghost.io/content/images/2022/06/Intro-to-MLOps-Cover--1-.webp', 'https://dl-staging-website.ghost.io/content/images/2022/06/UPSIDE.webp']"
Deep Learning for Deep Discounts,"With prices on the rise, an app analyzes user data to deliver cash back on retail purchases.What’s new:Upside, a startup based in Washington, D.C., works with gas stations, grocery stores, and restaurants to offer personalized discounts to consumers,The Markupreported.How it works:The app displays a map studded with offers, customized for each user, from 30,000 partners, most of them U.S. retail chains. A user who patronizes a partner pays full price, then uploads an image of the receipt. The app applies a discount to the user’s in-app balance, which can be transferred to a bank account — for a fee — or traded for digital gift cards.
Behind the news:Founded in 2015, Upside says its services reach 30 million U.S. users. Lyft and Uber integrate it with their driving app to offset inflation-driven spikes in gas prices. Fuel-saving apps GasBuddy and Checkout51 offer Upside-powered promotions, and DoorDash and Instacart have offered Upside to their drivers.Yes, but:Upside’s algorithmic approach to calculating discounts may leave some customers feeling left out.
Why it matters:Many families, individuals, and employees are on the lookout for ways to cut their expenses, and they may consider surrendering personal information a fair trade. However, the terms of the deal should be transparent and easy to understand. It’s deceptive to offer discounts that don’t pan out or diminish without warning as a casual shopper becomes a steady customer.We’re thinking:Offering discounts to attract users is an old tactic; think of Groupon and its countless competitors. But AI can tailor a deal to each individual user — a new approach that could make this strategy more effective, scalable, and sticky.
",['https://dl-staging-website.ghost.io/content/images/2022/06/ezgif.com-gif-maker--12--1.gif']
Essential Algorithms,"Machine learning offers a deep toolbox for solving all kinds of problems, but which tool is best for which task? When is the open-ended wrench better than the adjustable kind? Who invented these things anyway? In this special issue ofThe Batch, we survey six of the most useful algorithms in the kit: where they came from, what they do, and how they’re evolving as AI advances into every facet of society. If you want to learn more, theMachine Learning Specializationprovides a simple, practical introduction to these algorithms and more.Join the waitlistto be notified when it’s available.
",['https://dl-staging-website.ghost.io/content/images/2022/05/LinearRegression_CarWeight-Milege_1200px.webp']
Linear Regression: Straight & Narrow,"Linear regression may be the key statistical method in machine learning, but it didn’t get to be that way without a fight. Two eminent mathematicians claimed credit for it, and 200 years later the matter remains unresolved. The longstanding dispute attests not only to the algorithm’s extraordinary utility but also to its essential simplicity.Whose algorithm is it anyway?In 1805, French mathematician Adrien-Marie Legendre published the method of fitting a line to a set of points while trying to predict the location of a comet (celestial navigation being the science most valuable in global commerce at the time, much like AI is today — the new electricity, if you will, two decades before the electric motor). Four years later, the 24-year-old German wunderkind Carl Friedrich Gauss insisted that he had been using it since 1795 but had deemed it too trivial to write about. Gauss’ claim prompted Legendre to publish an addendum anonymously observing that “a very celebrated geometer has not hesitated to appropriate this method.”Slopes and biases:Linear regression is useful any time the relationship between an outcome and a variable that influences it follows a straight line. For instance, a car’s fuel consumption bears a linear relationship to its weight.
Two steps to ubiquity:The algorithm immediately helped navigators to follow the stars, and later biologists (notably Charles Darwin’s cousin Francis Galton) to identify heritable traits in plants and animals. Two further developments unlocked its broad potential. In 1922, English statisticians Ronald Fisher and Karl Pearson showed how linear regression fit into the general statistical framework of correlation and distribution, making it useful throughout all sciences. And, nearly a century later, the advent of computers provided the data and processing power to take far greater advantage of it.Coping with ambiguity:Of course, data is never perfectly measured, and some variables are more important than others. These facts of life have spurred more sophisticated variants. For instance, linear regression with regularization (also calledridge regression) encourages a linear regression model to not depend too much on any one variable, or rather to rely evenly on the most important variables. If you’re going for simplicity, a different form of regularization (L1 instead of L2) results inlasso, which encourages as many coefficients as possible to be zero. In other words, it learns to select variables with high prediction power and ignores the rest.Elastic netcombines both types of regularization. It’s useful when data is sparse or features appear to be correlated.In every neuron:Still, the simple version is enormously useful. The most common sort of neuron in a neural network is a linear regression model followed by a nonlinear activation function, making linear regression a fundamental building block of deep learning.
",['https://dl-staging-website.ghost.io/content/images/2022/05/LogisticRegression_tumbler_1200px.webp']
Logistic Regression: Follow the Curve,"There was a moment when logistic regression was used to classify just one thing: If you drink a vial of poison, are you likely to be labeled “living” or “deceased”? Times have changed, and today not only does calling emergency services provide a better answer to that question, but logistic regression is at the very heart of deep learning.Poison control:The logistic function dates to the 1830s, when the Belgian statistician P.F. Verhulst invented it to describe population dynamics: Over time, an initial explosion of exponential growth flattens as it consumes available resources, resulting in the characteristic logistic curve. More than a century passed before American statistician E. B. Wilson and his student Jane Worcester devised logistic regression to figure out how much of a given hazardous substance would be fatal. How they gathered their training data is a subject for another essay.Fitting the function:Logistic regression fits the logistic function to a dataset in order to predict the probability, given an event (say, ingesting strychnine), that a particular outcome will occur (say, an untimely demise).
More outcomes:Verhulst’s work found the probabilities of binary outcomes, ignoring further possibilities like which side of the afterlife a poison victim might land in. His successors extended the algorithm:
Versatile curve:The logistic function describes a wide range of phenomena with fair accuracy, so logistic regression provides serviceable baseline predictions in many situations. In medicine, it estimates mortality and risk of disease. In political science, it predicts winners and losers of elections. In economics, it forecasts business prospects. More important, it drives a portion of the neurons, in which the nonlinearity is a sigmoid, in a wide variety of neural networks.
",['https://dl-staging-website.ghost.io/content/images/2022/05/Heroes-MountainPaths-Gullies_1200px-1.webp']
Gradient Descent: It’s All Downhill,"Imagine hiking in the mountains past dusk and finding that you can’t see much beyond your feet. And your phone’s battery died so you can’t use a GPS app to find your way home. You might find the quickest path down via gradient descent. Just be careful not to walk off a cliff.Suns and rugs:Gradient descent is good for more than descending through precipitous terrain. In 1847, French mathematician Augustin-Louis Cauchyinventedthe algorithm to approximate the orbits of stars. Sixty years later, his compatriot Jacques Hadamard independentlydevelopedit to describe deformations of thin, flexible objects like throw rugs that might make a downward hike easier on the knees. In machine learning, though, its most common use is to find the lowest point in the landscape of a learning algorithm’s loss function.Downward climb:A trained neural network provides a function that, given an input, computes a desired output. One way to train the network is to minimize the loss, or error in its output, by iteratively computing the difference between the actual output and the desired output and then changing the network’s parameter values to narrow that difference. Gradient descent narrows the difference, minimizing the function that computes the loss. The network’s parameter values are tantamount to a position on the landscape, and the loss is the current altitude. As you descend, you improve the network’s ability to compute outputs close to the desired one. Visibility is limited because, in a typical supervised learning situation, the algorithm relies solely on the network’s parameter values and the gradient, or slope of the loss function — that is, your position on the hill and the slope immediately beneath your feet.
Stuck in the valley:Too bad your phone is out of juice, because the algorithm may not have propelled you to the bottom of a convex mountain. You may be stuck in a nonconvex landscape of multiple valleys (local minima), peaks (local maxima), saddles (saddle points), and plateaus. In fact, tasks like image recognition, text generation, and speech recognition are nonconvex, and many variations on gradient descent have emerged to handle such situations.  For example, the algorithm may havemomentumthat helps it zoom over small rises and dips, giving it a better chance at arriving at the bottom. Researchers have devised so many variants that it may seem as though there are as many optimizers as there are local minima. Luckily, local and global minima tend to beroughly equivalent.Optimal optimizer:Gradient descent is the clear choice for finding the minimum of any function. In cases where an exact solution can be computed directly — say, a linear regression task with lots of variables — it can approximate one, often faster and more cheaply. But it really comes into its own in complex, nonlinear tasks. Armed with gradient descent and an adventurous spirit, you might just make it out of the mountains in time for dinner.
No advanced math required! The newMachine Learning Specializationbalances intuition, practice, and theory to create a powerful learning experience for beginners.Enrollnow and achieve your career goals.
","['https://dl-staging-website.ghost.io/content/images/2022/05/DeepLearningAI_Banner_Stanford_Teaser_1200x628_Artboard-2--1-.webp', 'https://dl-staging-website.ghost.io/content/images/2022/05/AdmiralPerceptron_1200px.webp']"
Neural Networks: Find the Function,"Let’s get this out of the way: A brain is not a cluster of graphics processing units, and if it were, it would run software far more complex than the typical artificial neural network. Yet neural networks were inspired by the brain’s architecture: layers of interconnected neurons, each of which computes its own output depending on the states of its neighbors. The resulting cascade of activity forms an idea — or recognizes a picture of a cat.From biological to artificial:The insight that the brain learns through interactions among neurons dates back to 1873, but it wasn’t until 1943 that American neuroscientists Warren McCulloch and Walter Pittsmodeledbiological neural networks using simple mathematical rules. In 1958, American psychologist Frank Rosenblatt developed theperceptron, a single-layer vision network implemented on punch cards with the intention of building a hardware version for the United States Navy.Bigger is better:Rosenblatt’s invention recognized only classes that could be separated by a line. Ukrainian mathematicians Alexey Ivakhnenko and Valentin Lapa overcame this limitation bystackingnetworks of neurons in any number of layers. In 1985, working independently, French computer scientist Yann LeCun, David Parker, and American psychologist David Rumelhart and his colleaguesdescribedusingbackpropagationto train such networks efficiently. In the first decade of the new millennium, researchers including Kumar Chellapilla, Dave Steinkraus, and Rajat Raina (with Andrew Ng)acceleratedneuralnetworksusing graphical processing units, which has enabled ever-larger neural networks to learn from the immense amounts of data generated by the internet.Fit for every task:The idea behind a neural network is simple: For any task, there’s a function that can perform it. A neural network constitutes a trainable function by combining many simple functions, each executed by a single neuron. A neuron’s function is determined by adjustable parameters called weights. Given random values for those weights and examples of inputs and their desired outputs, it’s possible to alter the weights iteratively until the trainable function performs the task at hand.
Black box:While a trained network may perform its task, good luck determining how. You can read the final function, but often it’s so complex — with thousands of variables and nested activation functions — that it’s exceedingly difficult to explain how the network succeeded at its task. Moreover, a trained network is only as good as the data it learned from. For instance, if the dataset was skewed, the network’s output will be skewed. If it included only high-resolution pictures of cats, there’s no telling how it would respond to lower-resolution images.Toward common sense:Reporting on Rosenblatt’s Perceptron in 1958,The New York Timesblazed the trail for AI hype bycallingit “the embryo of an electronic computer that the United States Navy expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.” While it didn’t live up to that billing, it begot a host of impressive models: convolutional neural networks for images; recurrent neural networks for text; and transformers for images, text, speech, video, protein structures, and more. They’ve done amazing things, exceeding human-level performance in playing Go and approaching it in practical tasks like diagnosing x-ray images. Yet they still have a hard time with common sense and logical reasoning.Ask GPT-3, “When counting, what number comes before a million?” and it may respond, “Nine hundred thousand and ninety-nine comes before a million.” To which we reply: Keep learning!
",['https://dl-staging-website.ghost.io/content/images/2022/05/DecisionTree_1200px.webp']
Decision Trees: From Root to Leaves,"What kind of beast was Aristotle? The philosopher's follower Porphyry, who lived in Syria during the third century, came up with a logical way to answer the question. He organized Aristotle’s proposed “categories of being” from general to specific and assigned Aristotle himself to each category in turn: Aristotle’s substance occupied space rather than being conceptual or spiritual; his body was animate not inanimate; his mind was rational not irrational. Thus his classification was human. Medieval teachers of logic drew the sequence as a vertical flowchart: An early decision tree.The digital difference:Fast forward to 1963, when University of Michigan sociologist John Sonquist and economist James Morgan, dividing survey respondents into groups, firstimplementeddecision trees in a computer. Such work became commonplace with the advent of software that automates training the algorithm, now implemented in a variety of machine learning libraries including scikit-learn. The code took a quartet of statisticians at Stanford and UC Berkeley 10 years to develop. Today, coding a decision tree from scratch is a homework assignment in Machine Learning 101.Roots in the sky:A decision tree can perform classification or regression. It grows downward, from root to canopy, in a hierarchy of decisions that sort input examples into two (or more) groups. Consider the task of Johann Blumenbach, the German physician and anthropologist who first distinguished monkeys from apes (setting aside humans) circa 1776, before which they had been categorized together. The classification depends on various criteria such as presence or absence of a tail, narrow or broad chest, upright versus crouched posture, and lesser or greater intelligence. A decision tree trained to label such animals would consider each criterion one by one, ultimately separating the two groups.
Top 10 hit:Given Blumenbach’s conclusion (later overturned by Charles Darwin) that humans are distinguished from apes by a broad pelvis, hands, and close-set teeth, what if we wanted to extend the decision tree to classify not just apes and monkeys but humans as well? Australian computer scientist John Ross Quinlan made this possible in 1986 withID3, which extended decision trees to support nonbinary outcomes. In 2008, a further refinement calledC4.5capped a list of Top 10 Algorithms in Data Mining curated by the IEEE International Conference on Data Mining. In a world of rampant innovation, that’s staying power.Raking leaves:Decision trees do have some drawbacks. They can easily overfit the data by growing so many levels that leaf nodes include as few as one example. Worse, they’re prone to the butterfly effect: Change one example, and the tree that grows could look dramatically different.Into the woods:Turning this trait into an advantage, American statistician Leo Breiman and New Zealander statistician Adele Cutler in 2001 developed therandom forest, an ensemble of decision trees, each of which processes a different, overlapping selection of examples that vote on a final decision. Random forest and its cousin XGBoost are less prone to overfitting, which helps make them among the most popular machine learning algorithms. It’s like having Aristotle, Porphyry, Blumenbach, Darwin, Jane Goodall, Dian Fossey, and 1,000 other zoologists in the room together, all making sure your classifications are the best they can be.
",['https://dl-staging-website.ghost.io/content/images/2022/05/K-Means_3Clusters_1200px_Crop-2.webp']
"One Model, Hundreds of Tasks","Researchers took a step toward achieving a longstanding goal: One model that performs a whole lot of very different tasks.What's new:Scott Reed, Konrad Żołna, Emilio Parisotto and a team at DeepMind announcedGato, a model that performs over 600 diverse tasks including generating image captions, manipulating a physical robot arm, and playing Atari.How it works:The authors trained the 1.2  billion-parameter transformer on seven vision-language tasks likeMS-COCO Captions, an image and joint-angle dataset ofstacking blocks with a real robot, recorded state-of-the-art simulations of 595 tasks likeALE Atari, plus the language datasetMassiveText.
Results:In the simulated tasks, Gato achieved at least 50 percent of the score achieved in the recorded simulations of over 450 tasks. In ALE Atari, Gato matched or exceeded an average human score in 23 of 51 games, and it did at least twice as well in 11 of those 23. Gato successfully piloted a robot arm to stack a red block on top of a blue block (while ignoring a green block), in roughly 50 percent of the trials with previously unseen block shapes, comparable to a specialized baseline model, which achieved 49 percent.What they’re saying:DeepMind’s research director, Nando de Frietas, used Gato’s achievements toarguethat “it’s all about scale”: That larger models and better data are the keys to artificial general intelligence. New York University professor Gary Marcusrebuttedthis claim, pointing out that, alongside their increasingly brilliant results, large neural networks often generate baffling sentences, images, and behaviors.Why it matters:This work is the latest, and most expansive, in a line of improvements in multimodal AI recently lately showcased by the impressiveUNiTfrom Facebook. Transformers are well suited to a variety of tasks partly because they find patterns in long input sequences and because a variety of data types lend themselves to being divided into sequences to feed them.We're thinking:Gato is an impressive engineering feat. We don’t find it so interesting that a giant neural network can do what 600 distinct, smaller networks could do. But evidence that Gato might generalize across different tasks is fascinating. Specifically, the authors pretrained Gato, fine-tuned it on four new tasks, and showed that, in three cases, the fine-tuned model outperformed models trained specifically for those tasks. We look forward to more research that evaluates the extent to which such networks, beyond memorizing various unrelated tasks, generalize across tasks and to new tasks. In other words, further progress in the direction indicated by the paper’s title:  A Generalist Agent.
",['https://dl-staging-website.ghost.io/content/images/2022/05/ezgif.com-gif-maker--25--1.gif']
Recognizing Workplace Hazards,"A wearable device may help warehouse workers avoid injuries.What’s new:Modjoul, maker of a system that evaluates risks to people engaged in physical labor,receivedan undisclosed sum from Amazon as part of a $1 billion investment in technologies that might enhance the retailer giant’s operations.How it works:Mounted on a belt, the device monitors the wearer's behavior and surrounding conditions. External software analyzes the data and directs the device to deliver feedback. Supervisors can view the results on a software dashboard.
Behind the news:Modjoul is one of five companies thatreceivedinvestments last week from Amazon’s Industrial Innovation Fund. Several are developing AI products. For instance, California-based Vimaan is building a computer vision system to track inventories in real time by scanning barcodes, expiration dates, and serial numbers. BionicHIVE, an Israeli startup, is working on robots that use cameras and other sensors to track the locations of products on warehouse shelves.Why it matters:AI holds potential to make traditional industries more efficient and hopefully more humane. This system’s ability to recognize hazards related to physical posture means that everyone on the factory floor can benefit from moment-to-moment ergonomic evaluation. Protecting workers from injury is a win-win for employers and employees.We’re thinking:Monitoring workers raises obvious concerns about privacy and fairness. While we hope that employers will use such technology to improve the lives of workers, we also see potential for abuse by managers who, say, aim to maximize productivity at the cost of driving people to exhaustion. Automated monitoring of worker performance demands clear policies that govern its use, periodic auditing that documents its benefits and exposes its harms, and transparent mechanisms that make employers accountable for its impacts. Amazon is in an ideal position to take the lead in developing such policies and procedures.
Looking to get a certification to launch your machine learning career? #BreakIntoAI with the updated Machine Learning Specialization, launching in June on Coursera!Sign up to be notified
","['https://dl-staging-website.ghost.io/content/images/2022/05/DeepLearningAI_Banner_Stanford_Teaser_1200x628_Artboard-2--1--1.png', 'https://dl-staging-website.ghost.io/content/images/2022/05/LocationEtcData.webp']"
When Data = Danger,"Amid rising social tension in the United States over reproductive freedom, a company that analyzes location data on abortion clinics stopped distributing its findings after a critical press report.What’s new:SafeGraph, a company that analyzes consumer behavior based on location data, provided anonymized data on locations of Planned Parenthood, a chain of clinics that offer family-planning services including abortion,Vicereported. After the article was published, the companyremoveddata related to Planned Parenthood citing the risk that it might be misused.How it works:Based in Denver, SafeGraph purchases anonymized location data gathered by third-party apps installed on consumers’ phones. It uses machine learning algorithms to estimatewhere consumers live,which buildings they visit, how long they remain at each location. Customers can purchase reports that show foot traffic with respect to individual companies, type of business, or commercial category (such as “family planning centers”).
Behind the news:Activists, politicians, and others are using data collected by mobile apps to model and track individual behavior in increasingly invasive ways.
Why it matters:As the political winds in the U.S. shift against abortion, women who consider or undergo the procedure, doctors who perform it, and clinic workers who support reproductive services are increasingly at risk of harassment and violence. Tracking their movements, analyzing the details, and distributing the analysis far and wide only increases the risks.We’re thinking:Personal data is revealing. Coupled with machine learning, it can be revealing on a grand scale. We commend SafeGraph for withholding data about Planned Parenthood, but the business of analytics calls for a much more proactive stance. Companies that profit from personal data have a special responsibility to protect privacy and provide information only to customers with a legitimate interest.
",['https://dl-staging-website.ghost.io/content/images/2022/05/CXVv2.gif']
GPT-Free,"Itching to get your hands on a fully trained large language model? The wait is over.What’s new:Metaintroducedthe OPT family of transformer-based language models with nearly unfettered access to source code and trained weights. The family’s eight models range in size from 125 million to 175 billion parameters.How it works:The OPT architecture is similar to that of OpenAI’s GPT-3. The models were trained on publicly available datasets that include novels, news articles, Reddit posts, and a subset ofThe Pile.
Behind the news:OPT-175B is the largest and most ambitious open-source language model to date, but it’s not the first.
Yes, but:A parameter count of 175 billion parameters is mouthwatering, but it takes a lot of horsepower to drive a model that large. As Maarten Sap of the Allen Institute for Artificial IntelligencetoldIEEE Spectrum, “[I’d] love to use OPT-175B,” but “few research labs actually have the infrastructure to run this model.”Why it matters:For researchers — well, for anyone interested in language modeling, really — the opportunity is obvious. OPT comes pretrained, ready to be used, fine-tuned, dissected, or adapted for any purposes the AI community dreams up. No more APIs! No more paywalls! It’s your party, so indulge yourself. For Meta, open-sourcing these models may have several benefits. Giving away OPT is a community-minded gesture at a time when the company has been under fire for proliferating hatred, misinformation, and disinformation on a grand scale. It’s a bid to attract talent that could help break in young engineers to the company’s coding practices. And it’s a shot at OpenAI, the former nonprofit, open-source shop, which was criticized for keeping GPT-3’s code under wraps.We’re thinking:The OPT-175B traininglogoffers a rare look at a large-scale machine learning project. While the mass media may imagine bespectacled programmers in airy, well-lit rooms debating the nature of intelligence, technology development is often messy as researchers struggle to visualize what an algorithm is doing or trace the source of a GPU crash. Worth a look!
",['https://dl-staging-website.ghost.io/content/images/2022/05/MOXI--1-.gif']
Nurse’s Mechanical Helper,"Hospitals are using robots to lighten the load on clinical staff.What’s new:A number of U.S. hospitals are using Moxi, a robot from Diligent Robotics, to ferry supplies, lab specimens, soiled laundry, and other items,Wiredreported.How it works:Moxiis four feet tall with blinking L.E.D. eyes and an articulated arm tipped with a rubber grip. It navigates using a front-facing camera, rear-facing lidar, and auditory cues. A secure compartment that can be unlocked by a radio-frequency badge holds sensitive items such as lab samples. Fifteen Moxi robots are operating in U.S. hospitals, and another 60 are scheduled for deployment this year.
Behind the news:In 2020, the American Nurses AssociationassessedMoxi’s performance in three Texas hospitals. The study found that the robots improved nurse productivity and reduced feelings of burnout. However, the robots struggled to navigate crowded hospital halls, and their inability to read expiration dates raised the worry that they might contribute to adverse consequences.Why it matters:Robots may not have the best bedside manner (yet), but they can create much-needed breathing room for human caregivers. In a 2021surveyof U.S. nurses, 83 percent of respondents said their shifts were understaffed in a way that affected patients’ safety half of the time, and 68 percent had considered leaving the profession. Meanwhile, the U.S. is one of many countries with a rapidly growing population of elderly people, putting further strain on the healthcare system. These conditions create a clear opening for robots capable of performing many low-risk, repetitive chores.We’re thinking:Come to think of it, Hippocrates’ dictum “first, do no harm” bears a striking similarity toAsimov’s First Law of Robotics, “a robot may not harm a human being.”
What’s new about the revisedMachine Learning Specializationthat’s set to launch in June? It takes the core curriculum — vetted by millions of learners — and makes it more approachable by balancing intuition, code, and math for beginners.Pre-enroll now
","['https://dl-staging-website.ghost.io/content/images/2022/05/DeepLearningAI_Banner_Stanford_Teaser_1200x628_Artboard-2--1-.png', 'https://dl-staging-website.ghost.io/content/images/2022/05/MUSIIO--1-.gif']"
Hit Picker,"A neural network may help an online music service to spot songs with the potential to go big.What’s new:Musiio uses AI to identify specific attributes and qualities in recorded music. Online audio distributor SoundCloudpurchasedthe Singapore-based startup, which was valued at $10 million last year, for an undisclosed sum.How it works:Musiio trained its model on a proprietary database of songs, each tagged with dozens of labels including genre, vocalist’s gender, instruments featured, and emotions expressed.
Behind the news:A number of companies offer AI-powered tools designed to enable recording companies, artists, and fans to squeeze more value out of music.
Why it matters:Millions of new songs are released every year. Amid the deluge, AI can help distributors recognize potential hits, recording companies identify talent, fans find music they like, and musicians create sounds that stand out. Of course, the makings of a hit include social dynamics among listeners — presumably that’s where acquirer SoundCloud comes in.We’re thinking:According to models, this edition ofThe Batchhas moderate energy with high variance and a 72 percent chance of being powerful.
",['https://dl-staging-website.ghost.io/content/images/2022/05/EMBEDDEDv2.gif']
Predicting the Next Pandemic,"Odds are that the next mass contagion will jump to humans from animals. But which species?What’s new:Virus hunters are using learning algorithms to learn which animals are likely to carry microbes that pose a danger to humans,The New York Timesreported.How it works:Several systems trained on biological, ecological, and genetic data have shown promise in identifying sources of interspecies infection.
Behind the news:The AI community isn’t just working to predict future pandemics, it’s also fighting the current one.
Why it matters:Ebola, HIV, swine flu — many dire human diseases evolved in animals. Using AI to identify viruses likely to cross the species barrier could give scientists a jump on whatever comes next. Medical researchers could develop vaccines and treatments ahead of time, and officials could mitigate the spread of potentially dangerous disease by managing animal populations and limiting trade in disease-carrying species.We’re thinking:Whether an animal virus can infect a human is one question. Whether it can cause a pandemic is another. Machine learning engineers have an opportunity to help answer that one as well.
",['https://dl-staging-website.ghost.io/content/images/2022/05/BEIJING--1-.gif']
Autonomy Becomes Autonomous,"In Beijing, self-driving cars are rolling without a driver behind the wheel.What’s new:China’s capital city authorized Pony.ai and Apollo Go, a Baidu subsidiary, to deploy self-driving taxis without a human in the driver’s seat,Reutersreported. An authorizationissuedlast November had allowed the companies to operate in the same area with safety drivers.How it works:The robotaxis are restricted to a 23 square-mile zone in Beijing’s southeastern suburbs, home to roughly 300,000 residents. Rides are free, but both companies plan to charge in the near future.
Behind the news:Autonomous taxis — with safety drivers — are operating in a number of Chinese cities includingShanghaiand Shenzhen. All except Pony.ai have provisional licenses that require them to provide free rides. In April, Pony.aireceivedthe country’s first authorization to charge robotaxi passengers.Why it matters:Permission to operate autonomous vehicles in Beijing, which is home to over 20 million people, is a huge market opportunity. Permission to do it without safety drivers likely will represent a huge cost saving if the government relaxes the requirement to carry a supervisor. But the symbolism is an even bigger deal: If robotaxis can handle Beijingtraffic, they may be ready for wider adoption. (Then again,gridlockisn’t the most challenging condition for an autonomous vehicle.)We’re thinking:Safe adoption of self-driving cars still requires limitations on their range and cooperation with governments. Countries that aim to accelerate progress in this area should help entrepreneurs deploy vehicles in a relatively self-contained region and expand from there.
Considering a career in data science or machine learning?Join uson May 18, 2022, to hear from industry leaders about their experiences. Every story is different, and everyone’s journey is unique. Take the next step now!
","['https://dl-staging-website.ghost.io/content/images/2022/05/AI-X-5.18_The-Batch-Image.png', 'https://dl-staging-website.ghost.io/content/images/2022/05/HOSPITALS-1.webp']"
Managing Medical Uncertainty,"Hospitals across the United States are relying on AI to keep patients safe.What’s new:Doctors are using a variety of machine learning systems to assess the risk that a given patient will suffer complications,The Wall Street Journalreported.How it works:Several facilities are using AI to identify patients who need special attention.
Behind the news:Government regulators are beginning to accept machine learning’s potential to transform healthcare.
Why it matters:The Covid-19 pandemic has highlighted tragically underfunded and overworked healthcare workers around the globe. Automated tools could help providers make better use of limited time and resources and help them to focus their attention on the most important cases.We’re thinking:Many countries face a demographic cliff: The population of younger people is falling precipitously, while the number of elders is growing. It seems likely that AI will be instrumental in helping doctors care for an aging population with a rising life expectancy.
",['https://dl-staging-website.ghost.io/content/images/2022/05/SMALL.gif']
The View Through the Windshield,"Overhead cameras equipped with computer vision are spotting distracted drivers on the road.What’s new:A system from Melbourne-based Acusensus alerts police when drivers are engaged in risky activities such as using a cell phone, not wearing a seatbelt, or speeding,The New York Timesreported.How it works:TheHeads-Upsystem uses sensors mounted over the road on overpasses, signs, or movable structures. Aninfrared flash cameracaptures images through windshield glare, heavy weather, and nighttime darkness. Radar gauges a vehicle’s speed.
Results:New South Wales, Australia, deployed the system in 2019. In its first two years, itcontributedto a 22 percent decline in road fatalities and an 80 percent decline in use of mobile phones behind the wheel. An 18-hour assessment along a stretch of road in Missouri that saw an average three and a half crashes daily found that 6.5 percent of drivers used mobile phones and around 5 percent engaged in more than one risky behavior.Behind the news:AI is being applied to traffic safety worldwide — and not always by surveilling drivers.
Why it matters:About1.3 millionpeople worldwide die in road accidents every year, according to the World Health Organization. Many fatalities are associated with speeding, distracted driving, and not wearing seatbelts. AI systems that identify these behaviors can help save lives.We’re thinking:People tend to buckle up when they see a police car and slow down when they see their current speed flashing on a sign ahead. If cameras looming over the road can save lives — given adequate controls on who has access to the data and how they can use it — it’s worth a try.
",['https://dl-staging-website.ghost.io/content/images/2022/04/BRIDGE.webp']
Bridge to Explainable AI,"DeepMind’s AlphaGo famously dominated Go, a game in which players can see the state of play at all times. A new AI system demonstrated similar mastery of bridge, in which crucial information remains hidden.What’s new:NooK, built by Jean-Baptiste Fantun, Véronique Ventos, and colleagues at the French startup NukkAI, recently beat eight world champions at bridge — rather, a core aspect of the game.Rules of the game:Bridge is played by four players grouped into teams of two. Each player is dealt a hand of cards, after which the game is played in two phases:
This study focused on the play phase, pitting NooK and human champions against previous automated bridge-playing systems, none of which has proven superior to an excellent human player. Each deal had a preassigned bid and trump suit, and competitors played the same 800 deals, divided into sets of 10. The player with the highest average score in the most sets won.How it works:The developers didn’t reveal the mechanisms behind NooK, but we can offer a guess based on press reports and the company’sresearchpapers.
Results:Pitted against the previous systems, NooK scored higher than the human champions in 67 out of 80 sets, or 83 percent of the time.Why it matters:Neural networks would be more useful in many situations if they were more interpretable; that is, if they could tell us why they classified a cat as a cat, or misclassified a cat as an iguana. This work’s approach offers one way to build more interpretable systems: a neurosymbolic hybrid that combines rules (symbolic AI, also known as good old-fashioned AI) describing various situations with neural networks trained to handle specific cases of each situation.We’re thinking:In bridge, bidding is a way to hint to your partner (and deceive your opponent) about what you have in your hand, and thus a vital strategic element. NooK is impressive as far as it goes, but mastering bids and teamwork lie ahead.
More than 4.7 million learners took the original Machine Learning course by Andrew Ng. A decade later, a new and updated Machine Learning Specialization is set to launch in June! #BreakIntoAI with this foundational three-course program. Sign uphere
","['https://dl-staging-website.ghost.io/content/images/2022/04/DeepLearningAI_Banner_Stanford_Teaser_1200x628_Artboard-2--1-.png', 'https://dl-staging-website.ghost.io/content/images/2022/04/GLAM.gif']"
Efficiency Experts,"The emerging generation of trillion-parameter language models take significant computation to train. Activating only a portion of the network at a time can cut the requirement dramatically and still achieve exceptional results.What’s new:Researchers at Google led by Nan Du, Yanping Huang, and Andrew M. Dai developedGeneralized Models (GLaM), a trillion-parameter model for language tasks. Like the company’s earlierSwitch, this work usesmixture-of-experts(MoE) layers to select which subset(s) of a network to use depending on the input. It provides a clearer picture of how MoE can save time and electricity in practical language tasks.Key insight:A neural network’s parameter count entails a compromise between performance (bigger is better) and energy cost (smaller is better). MoE architectures use different subsets of their parameters to learn from different examples. Each MoE layer contains a group of vanilla neural networks, or experts, preceded by a gating module that learns to choose which ones to use based on the input, enabling different experts to specialize in particular types of examples. In this way, the network uses less energy and learns more than the size of any given subset might suggest.How it works:The authors trained a transformer model equipped with MoE layers (similar toGShard) to generate the next word or part of a word in a text sequence using a proprietary 1.6-trillion-word corpus of webpages, books, social media conversations, forums, and news articles. They fine-tuned the model to perform 29 natural language tasks in seven categories such as question answering and logical reasoning.
Results:Training the 1.2 trillion-parameter GLaM required 456 megawatt hours, while the 175 billion-parameterGPT-3required 1,287 megawatt hours. Moreover, GLaM outperformed GPT-3 in six categories of zero-shot tasks and in five categories for one-shot tasks. For example, answering trivia questions in the one-shotTriviaQA, it achieved 75 percent accuracy — a state-of-the-art result —  compared to GPT-3’s 68 percent.Why it matters:Increased computational efficiency means lower energy costs, presumably making it easier for everyday engineers to train state-of-the-art models. It also means reduced CO2emissions, sparing the planet some of the environmental impact incurred by AI.We’re thinking:MoE models are attracting a lot of attention amid the public-relations race to claim ever higher parameter counts. Yes, building a mixture of 64 experts boosts the parameter count by 64 times, but it also means building 64 models instead of one. While this can work better than building a single model, it also diverts attention from other architectures that may yield insights deeper thanbigger is better.

",['https://dl-staging-website.ghost.io/content/images/2022/04/JET--1-.gif']
How AI Ventures Spend Their Capital,"AI startups are putting their cash into . . . AI startups.What’s new:Young AI companies flush with venture capital are purchasing startups to expand the range of services they can offer,The Wall Street Journalreported.Feeding frenzy:Venture-funded companies spent $8 billion on AI startups in 2021, up from $942 million in 2020 and $82 million in 2019, according to market analyst 451 Research. The number of acquisitions jumped from 48 to 72 in that period. TheJournalfocused on two chatbot deals: Gupshup’s purchase of Active.ai and Observe.AI’s acquisition of Scope.AI.
Behind the news:All told, investors are spending more than ever on AI. Private investments in AI more than doubled to $93 billion in 2021 from $42 billion in 2019, according to theStanford AI Index. However, they’re also becoming choosier about where they put their money. The number of newly funded AI companies worldwide fell from 1,200 to 746 between 2018 and 2021.Why it matters:AI continues to be hot in the startup world — so hot that startups themselves want more of it. The current wave of purchases suggests that startups not only want to expand their AI holdings, they consider purchasing AI companies a strategic way to broaden their markets.We’re thinking:Ultimately, young companies have to make money by creating long-term value, but the route may not be direct. For instance, we’ve seen self-driving car startups that have little in the way of products or revenue thrive by serving other self-driving car startups. This is part of the value of venture capital: It gives companies the time and resources they need to (hopefully) create massive value.
",['https://dl-staging-website.ghost.io/content/images/2022/04/MRKL.gif']
Neural Nets + Rules = Truer Text,"A new approach aims to cure text generators of their tendency to produce nonsense.What’s new:AI21 Labs launched Jurassic-X, a natural language processing system that combines neural networks and rule-based programs. Jurassic-X weds a large language model with modules that supply up-to-date facts, solve math problems, and process special kinds of input.How it works:Jurassic-X is built on a software infrastructure called Modular Reasoning, Knowledge and Language (MRKL) that incorporates a variety of programs. AI21’sJurassic-1, a large pretrained transformer model, performs general language tasks. Specialized modules include a calculator and programs that query networked databases such as Wikipedia, as well as a router that mediates among them.
Why it matters:Current neural networks perform at nearly human levels in a variety of narrow tasks, but they have little ability to reason (especially over words ornumbers), are prone toinventing facts, and can’t absorb new information without further training. On the other hand, rules-based models can manipulate meanings and facts, but they fall down when they encounter situations that aren’t covered by the rules. Combining a general language model with specialized routines to handle particular tasks could yield output that’s better aligned with the real world.We’re thinking:Humans frequently resort to a calculator or Wikipedia. It makes sense to make these things available to AI as well.
Our updated and expanded Machine Learning Specialization is set to launch in June! The new specialization will cover the latest techniques and foundational AI concepts that made the original the world’s most popular machine learning course.Sign up to be notified when it’s available
",['https://dl-staging-website.ghost.io/content/images/2022/04/GLIDEv2.gif']
More Realistic Pictures From Text,"OpenAI’sDALL·Egot an upgrade that takes in text descriptions and produces images in styles from hand-drawn to photorealistic. Thenew versionis a rewrite from the ground up. It uses the earlierCLIPzero-shot image classifier to represent text descriptions. To generate images, it uses a method first described in a recent paper.Imagination engine:Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, and colleagues at OpenAI publishedGLIDE, adiffusion modelthat produces and edits images in response to text input.Diffusion model basics:During training, this generative approach takes noisy images and learns to remove the noise. At inference, it starts with pure noise and generates an image.Key insight:Previousworkshowed that, given a class label in addition to an image, a diffusion model can generate new images of that class. Likewise, given a representation of text as an additional input, it should produce output that reflects the representation.How it works:GLIDE used a transformer andADM, a convolutional neural network outfitted with attention. Like DALL·E, the system was trained on 250 million image-text pairs collected from the internet. Unlike DALL·E, the authors added noise to each image incrementally to produce 150 increasingly noisy examples per original.
Results:Human evaluators rated GLIDE’s output more photorealistic than DALL·E’s in 91 percent of 1,000 comparisons. They ranked GLIDE’s images more similar to the input text than DALL·E’s 83 percent of the time. The authors reported only qualitative results for the model’s ability to edit existing images, finding that it introduced objects in an appropriate style with good approximations of illumination, shadows, and reflections.Yes, but:GLIDE’s photorealistic output comes at a cost of inference time. It took 15 seconds — far longer than GAN-based text-to-image generators, which generally take a fraction of a second.Why it matters:Generative models typically are hard to control in an intuitive way. Enabling users to direct photorealistic image generation via natural language opens the door to broader and more widespread uses.We’re thinking:Diffusion models are emerging as an exciting alternative among generative architectures. GLIDE’s 3.5 billion-parameter implementation (which, while very large, is roughly a quarter the size of DALL·E) is further evidence.
",['https://dl-staging-website.ghost.io/content/images/2022/04/ezgif.com-gif-maker--19--2.gif']
AI Enters the Radiology Department,"The European Union approved for clinical use an AI system that recognizes normal chest X-rays.What’s new:ChestLink is the first autonomous computer vision system toearnthe European Economic Area’s CE mark for medical devices, which certifies that products meet government requirements for health and safety. The mark enables Oxipit, the Lithuanian startup that makes the system, to deploy it in the 27 E.U. countries plus Iceland, Liechtenstein, Norway, Switzerland, and Turkey.How it works:ChestLink uses a previous Oxipit product, ChestEye, to scan for 75 abnormalities such as edema and tuberculosis. If it finds none, it generates a medical report. Otherwise it forwards the image to a radiologist for review.
Why it matters:Reading X-ray images is highly subjective. Moreover, a radiologist’s judgment canvaryas fatigue sets in over the course of a working day. By identifying and reporting on normal images, this system could help radiologists focus on the cases that need the most attention.We’re thinking:Even the best AI systems for diagnosing chest X-raysfall shortof a board-certified radiologist’s accuracy. Training AI to recognize problem-free images, which are less ambiguous, is a clever approach.
",['https://dl-staging-website.ghost.io/content/images/2022/04/FACEWAR.gif']
Seeing Through the Fog of War,"Face recognition is identifying people who have been killed, displaced, or recorded perpetrating alleged war crimes in Russia’s invasion of Ukraine.What’s new:Clearview AI, a startup that has beencriticizedfor harvesting online images without subjects’ permission, made its face recognition system freely available to the Ukrainian government,The New York Timesreported. Researchers unaffiliated with the Ukrainian government are using similar tools to analyze images of the conflict.How it works:Clearview has created 200 accounts at five Ukrainian agencies. Officials have used its app to conduct over 5,000 searches.
Yes, but:Face recognition can produce erroneous output. Amid military conflict, such errors — combined with wartime pressures — may cause people to be misidentified as war criminals, spies, or deceased.Behind the news:AI is being used to analyze a variety of data types flowing out of Ukraine.
Why it matters:The invasion of Ukraine — captured in an avalanche of photos, videos, aerial imagery, and radio transmissions shared on social media — is one of the most data-rich conflicts in history. Given this grim corpus, face recognition and other AI techniques can help to sketch a more complete picture of the battlefield.We’re thinking:The ability to unmask war criminals and thereby help bring them to justice offers solace amid unspeakable misery. We hope it also will deter other offenders. To recover from this tragedy will require still greater ingenuity and fortitude. We join the international community incallingon Vladimir Putin to withdraw Russian forces immediately.
Many organizations embark on machine learning projects only to encounter roadblocks and eventually fail.Join usfor a live event on how to maximize your potential for success.
","['https://dl-staging-website.ghost.io/content/images/2022/04/4.28_The-Batch-Image.png', 'https://dl-staging-website.ghost.io/content/images/2022/04/DEEPFAKE--1-.gif']"
Your Salesbot Connection,"Marketers are using fake social media personas — enhanced by AI-generated portraits — to expand their reach without busting their budgets.What’s new:Renee DiResta and Josh Goldstein at Stanford Internet Observatory combed LinkedIn and discovered over 1,000 fake profiles with false faces they believe to have been produced using generative adversarial networks, the radio network NPRreported.How it works:Companies hire independent marketers to expand their markets by messaging potential customers on social media. These marketers use fake profiles to send sales pitches. Responses are routed to a salesperson at the original company.
Spot the fake:DiResta and Goldstein shared tips for recognizing forged LinkedIn profiles.
Why it matters:In the era of social media, companies have access to far more potential customers than their sales teams could possibly reach. This gives them ample incentive to look to AI for assistance. However, the risk of blowback for deceiving the public may outweigh the prospective gains.We’re thinking:Need we say it? Deceptive sales tactics are unacceptable no matter how cool your technology may be.
",['https://dl-staging-website.ghost.io/content/images/2022/04/SYMBOLIC.gif']
The Hammer Drops,"The U.S. government punished an app vendor for building an algorithm based on ill-gotten data.What’s new:The Federal Trade Commission (FTC), the U.S. agency in charge of consumer protection, ruled that an app developed by WW International (formerly Weight Watchers) violated data-collection laws. In a settlement, the company agreed to pay a fine, destroy data, and deactivate the app, the tech-news websiteProtocolreported.How it works:The FTC isempoweredto take action against companies that engage in deceptive business practices. Combined with other laws that protect specific classes of people — in this case, children — the agency exercised its authority to combat misuse of data.
Behind the news:The FTC has punished companies for using improperly collected data twice before. In 2021, itforcedthe developer of photo-sharing app Everalbum to destroy models it developed using images uploaded by users who hadn’t consented to face recognition. Two years earlier, itdemandedthat Cambridge Analytica, a UK political consultancy, destroy data it had collected illegally from Facebook users.Why it matters:The U.S. lacks comprehensive national privacy laws that protect consumer data, but that doesn’t mean it won’t act against companies that abuse personal data. The FTC can prosecute algorithmic abuse based on several interrelated laws, and lately it has done so with increasing frequency.We’re thinking:If the public is to trust the AI community, it’s necessary to respect privacy and obtain permission for any data that goes into building a model. If the FTC’s willingness to prosecute developers of unruly algorithms provides further incentive, so be it.
",['https://dl-staging-website.ghost.io/content/images/2022/04/ZOOBUILDER.gif']
Animal Animations From Video,"A video game studio is looking to machine learning to cut the effort and expense of populating three-dimensional scenes with animated animals.What’s new:Ubisoft showed offZooBuilder, a pipeline of machine learning tools that converts videos of animals into animations. The system is a prototype and hasn’t been used in any finished games.How it works:In the absence of an expensive dataset that depicts animals in motion, researchers at Ubisoft China and elsewhere generated synthetic training data from the company’s existing keyframe animations of animals. They described the system in an earlierpaper.
Yes, but:ZooBuilder initially was limited to cougars and had trouble tracking them when parts of their bodies were occluded or out of the frame, and when more than one creature was in the frame. Whether Ubisoft has overcome these limitations is not clear.Behind the news:Machine learning is playing an increasing role in 3D graphics.
Why it matters:It can take months of person-hours to animate a 3D creature using the typical keyframe approach. Automated systems like this promise to make animators more productive and could liberate them to focus on portraying in motion the fine points of an animal’s personality.We’re thinking:There’s face recognition forcows, speech recognition forbirds, sentiment analysis forpigs, and now OpenPose for cougars. What will the animals steal from us next?!
Learn how to apply machine learning to concrete problems in medicine — including diagnosis, prognosis, and treatment — with theAI for Medicine Specialization!Enroll today
","['https://dl-staging-website.ghost.io/content/images/2022/04/The-Batch-Image--4-.png', 'https://dl-staging-website.ghost.io/content/images/2022/04/ezgif.com-gif-maker--18--1.gif']"
Learning After Overfitting,"When a model trains too much, it can overfit, or memorize, the training data, which reduces its ability to analyze similar-but-different inputs. But what if training continues? New work found that overfitting isn’t the end of the line.What's new:Training relatively small architectures on an algorithmically generated dataset, Alethea Power and colleagues at OpenAI observed that ongoing training leads to an effect they callgrokking, in which a transformer’s ability to generalize to novel data emerges well after overfitting.Key insight:It takes a lot of computation to study how learning progresses over time in models with billions of parameters that train on datasets of millions of examples. It’s equally revealing — and more practical — to study models with hundreds of thousands of parameters that train on thousands of examples. Models on that scale can train through many more steps in far less time.How it works:The authors trained a set of transformers to classify the solutions to each of 12 two-variable equations, mostly polynomials.
Results:As the models trained, validation accuracy rose, fell, and —  after the number of training steps continued to rise by a factor of 1,000 — rose a second time. (In the case of modular division, validation accuracy improved from nearly 5 percent to nearly 100 percent). In experiments using reduced datasets, the authors found that the smaller the training set, the more training was needed to achieve the second increase. For instance, when training on 30 percent as many examples, roughly 45 percent more training steps were required.Why it matters:Grokking may be the way thatdouble descent, in which a model’s performance improves, worsens, and improves again as the number of parameters or training examples increases, plays out with small models and datasets. That said, this work provides evidence that we've been mistaken about the meaning of overfitting. Models can continue to learn after they overfit and can go on to become quite capable.We're thinking:The authors discovered this phenomenon in a petri dish. Now we need to find out whether it holds with life-size models and datasets.
",['https://dl-staging-website.ghost.io/content/images/2022/04/ALGAE--1-.gif']
AI Designs Chemical Weapons,"It’s surprisingly easy to turn a well-intended machine learning model to the dark side.
What’s new:In an experiment, Fabio Urbina and colleagues at Collaborations Pharmaceuticals, who had built a drug-discovery model to design useful compounds and avoid toxic ones, retrained it togenerate poisons. In six hours, the model generated 40,000 toxins, some of them actual chemical warfare agents that weren’t in the initial dataset.How it works:The authors didn’t detail the architecture, dataset, and method to avoid encouraging bad actors. The following description is drawn from the few particulars they did reveal along with accounts of the company’s existing generative model,MegaSyn.
Why it matters:The authors took an industrial model and turned it into what they call “a computational proof of concept for making biochemical weapons.” They emphasize that it wouldn’t be difficult to copy using publicly available datasets and models. It may be similarly easy to subvert models built for tasks other than drug discovery, turning helpful models into harmful ones.We’re thinking:Despite machine learning’s enormous potential to do good, it can be harnessed for evil. Designing effective safeguards for machine learning research and implementation is a very difficult problem. What is clear is that we in the AI community need to recognize the destructive potential of our work and move with haste and deliberation toward a framework that can minimize it. NeurIPS’effortsto promote introspection on the part of AI researchers are a notable start — despiteargumentsthat they politicize basic research — and much work remains to be done.
",['https://dl-staging-website.ghost.io/content/images/2022/03/NVIDIA.gif']
Transformer Accelerator,"Is your colossal text generator bogged down in training? Nvidia announced a chip designed to accelerate the transformer architecture, the basis of large language models such as GPT-3.What’s new:TheH100graphics processing unit (GPU) can train transformer models many times faster than Nvidia’s previous flagship A100 (or, presumably, any other chip on the market).How it works:Transformer networks have ballooned in size from GPT-3’s 175 billion parameters to Wu Dao’s 1.75 trillion, requiring more computation for training and inference. The H100’s underlying chip design, known as Hopper, includes a so-called Transformer Engine designed to make such models run more efficiently.
Time savings:In tests, a 395 billion-parametermixture-of-expertsmodel took 20 hours to train running on 8,000 H100s, while it took seven days running on the same number of A100s. A chatbot based on Nvidia’sMegatrongenerated output up to 30 times faster running on H100s than A100s. Nvidia plans to link 4,608 H100 chips into a trainingsupercomputerthat the company touts as the world’s fastest system for training AI.Behind the news:While Nvidia is the undisputed leader in specialized AI chips, several competitors are vying for the same market.
Why it matters:The transformer has driven a tidal wave of progress in AI for language as well as an expandingarrayof domains including vision, image generation, and biomedicine. The ability to train such models faster greases the wheels for this versatile architecture.We’re thinking:Conventional chips lately havestruggledto keep pace with Moore’s Law, which predicts a doubling of processing power every 18 months. AI chips areoutpacingit by a wide margin. Yet another reason to dig into AI!
Want to design applications that can chat, answer questions, evaluate sentiments, translate languages, and summarize text? Learn how with theNatural Language Processing Specialization!Enroll today
","['https://dl-staging-website.ghost.io/content/images/2022/03/The-Batch-Image--3-.png', 'https://dl-staging-website.ghost.io/content/images/2022/03/IVOW.png']"
Who Needs Programming?,"The next killer AI application may be developed by someone who has never heard of gradient descent.What’s new:A rising generation of software development platforms serves users who aren’t familiar with AI — and even programming.The New York Timessurveyedthe scene.Robocoders:Using no-code AI platform — an automated programming tool that either generates new code or customizes pre-existing code according to user input — generally requires access to a web browser and training data. From there, a user-friendly interface lets users train a prebuilt architecture.
Behind the news:Similar tools for building non-AI applications like websites (Wordpress), ecommerce stores (Shopify), and video games (RPG Maker) undergird a significant portion of the online economy.OpenAIandDeepMindoffer natural language tools that write code using plain-English prompts.Source AI, available in a beta-test version, extends such auto-coding functionality to French, German, and Spanish to generate programs in at least 40 languages.Why it matters:Platforms that automate coding, data collection, and training are an important part of AI’s future. Although no-code AI tools are still maturing — for example, they’re limited to particular tasks and some aren’t yet suitable for commercial-grade applications — they’re on track to open the field to a far broader range of users, enabling them to apply tried-and-true approaches to certain classes of problems. And they may be useful to experienced AI developers, too. For instance, trained engineers may also use them to build wireframe versions of more intensive projects.We’re thinking:No-code tools have a long way to go, and even when they get there, education in AI technology will be necessary to handle difficult problems, high-stakes situations, and cutting-edge developments. Skilled engineers will exceed the capabilities available at the press of a button for the foreseeable future.
",['https://dl-staging-website.ghost.io/content/images/2022/03/ezgif.com-gif-maker--24--1.gif']
The Many Faces of Genetic Illness,"People with certain genetic disorders share common facial features. Doctors are using computer vision to identify such syndromes in children so they can get early treatment.What’s new:Face2Geneis an app from Boston-basedFDNAthat recognizes genetic disorders from images of patients’ faces. Introduced in 2014, it was upgraded recently to identify over 1,000 syndromes (more than three times as many as the previous version) based on fewer examples. In addition, the upgrade can recognize additional conditions as photos of them are added to the company’s database — no retraining required.How it works:Newworkby Aviram Bar-Haim at FDNA, Tzung-Chien Hsieh at Rheinische Friedrich-Wilhelms-Universität Bonn, and colleagues describes the revised model.
Results:In tests, the new version proved somewhat less accurate than its predecessor at recognizing the 91 syndromes pictured in theLondon Medical Database. It ranked the correct syndrome in the top 30 possibilities 86.59 percent of the time versus the earlier version’s 88.34 percent. However, it was able to identify 816 conditions that its predecessor couldn’t, ranking the correct one in the top 30 possibilities 24.41 percent of the time and in the top position 7.07 percent of the time. (The chance of choosing the correct syndrome randomly was 0.09 percent.)Why it matters:Some350 million peopleworldwide live with a rare genetic disorder. Such conditions are especially difficult to diagnose because they’re so numerous, and many doctors never encounter a case. Face2Gene, which reportedly is used by thousands of geneticists, has beencreditedwith making the job much easier.We’re thinking:Humanity has a sad history of judging people based on appearance. While this model is designed for healthcare professionals to evaluate children who may need medical treatment, we caution against trying to use AI to classify an individual’s traits such as intelligence, character, or sexual preference based on their looks.
Looking to build or customize powerful real-world models to solve difficult problems? Check out theTensorFlow: Advanced Techniques Specialization!Enroll today
",['https://dl-staging-website.ghost.io/content/images/2022/03/ezgif.com-gif-maker--11--2-1.gif']
Stock-Trading Test Bed,"If you buy or sell stocks, it’s handy to test your strategy before you put real money at risk. Researchers devised a fresh approach to simulating market behavior.What's new: Andrea Coletta and colleagues at Sapienza University of Rome used a Conditional Generative Adversarial Network (cGAN) tomodela market’s responses to an automated trader’s actions.Key insight:Previous approaches tested a simulated trader in a virtual market populated by other simulated traders. However, real-world markets tend to be too complex to be modeled by interactions among individual agents. Instead of simulating market participants, a cGAN can model aggregated sales and purchases in each slice of time.Conditional GAN basics:Given a random input, a typical GAN learns to produce realistic output through competition between a discriminator that judges whether output is synthetic or real and a generator that aims to fool the discriminator. AcGANworks the same way but adds an input — in this case, details about individual buy and sell orders and the overall market — that conditions both the generator’s output and the discriminator’s judgment.How it works: The authors built a simulated stock exchange based on theAgent-Based Interactive Discrete Event Simulation(ABIDES) framework to match buy and sell orders. They trained a cGAN to generate such orders based on two days ofmarket datafor Apple and Tesla stocks. Then they added orders by an independent trader.
Results: The authors checked statistical similarity between historical and cGAN orders in terms of price, volume, direction (buy or sell), and frequency distributions. In particular, they looked at Tesla shares on May 2 and May 3, 2019, and plotted the distributions. The real and synthetic distributions matched fairly closely. When they ran the simulation using historical orders plus cGAN orders, the price rose slightly during the 30 minutes when the agent would have been active. Given the orders generated by the cGAN and the agent, the price rose by an order of magnitude more and returned to normal shortly after the agent stopped trading, demonstrating the simulation’s response to the agent’s activity.Why it matters: GANs are usually associated with image generation. This paper adds to agrowingbodyofresearchshowing that they can successfully generate data outside of perceptual domains.We're thinking:Supervised learning tends to apply when a specific output y can be predicted from a given input x. In applications where y is a complex data type that’s also inherently stochastic — such as a sequence of market trades or afuture weather map— we might try to model y using a stochastic process rather than attempt to learn one correct answer. cGANs appear to be emerging as a promising approach.
",['https://dl-staging-website.ghost.io/content/images/2022/03/INDEX--1-.gif']
Help Wanted: AI Developers,"A shortfall in qualified AI professionals may be a windfall for aspiring engineers.
What’s new:Hiring managers are struggling to find machine learning engineers amid an ongoing, global talent shortage,Business Insiderreported. Some employers are going the extra mile to distinguish themselves from competitors in the eyes of potential employees.
Supply and demand:The number of new graduates with machine learning backgrounds is not keeping pace with demand for their skills.
Fringe benefits:High demand for machine learning engineers is empowering qualified applicants to secure perks.
Behind the news:Recent studies confirm both the rising demand for machine learning engineers and the scarcity of qualified candidates.
Why it matters:The hiring boom in machine learning and data science isn’t new, but it shows no sign of slowing and may be intensifying as the pandemic wanes. It’s a great time for candidates to approach employers and for academic institutions to meet rising demand with strong educational programs.
We’re thinking:The labor shortage is great for employees in the short term, but it also holds back AI development from reaching its full potential. It’s high time for everyone to build AI capacity, from individuals to businesses to institutions.
",['https://dl-staging-website.ghost.io/content/images/2022/03/Investorbots-Too-Good-to-Be-True-2.gif']
Investorbots: Too Good to Be True?,"Machine learning models aren’t likely to replace human stock-market analysts any time soon, a new study concluded.
What’s new:Wojtek Buczynski at University of Cambridge and colleagues at Cambridge and Oxford Brookes Universitypinpointedkey flaws in prior research into models that predict stock-market trends. Neither the algorithms nor the regulators who oversee the market are ready for automated trading, they said.How it works:The authors surveyed 27 peer-reviewed studies published between 2000 and 2018 that used machine learning to forecast the market. They found patterns that rendered these approaches inadequate as guides to real-world investment.
Behind the news:Although investment funds that claim to use AI have garnered attention, so far they’ve generated mixed results.
Why it matters:If machine learning can make predictions, why can’t it predict market activity? A couple of reasons stand out. This paper examines the misalignment between AI research and the likely challenges of real-world deployment. Moreover, even if an algorithm predicts market dynamics accurately within the short term, it will lose accuracy as its own predictions come to influence sales and purchases.
We’re thinking:Studying algorithms that make trading decisions has always been a challenge, since traders tend to keep information about successful algorithms to themselves lest competitors replicate them and dull their edge. Hedge funds that have access to non-public data (for example, specific online chats) have used machine learning with apparent success over years. But those funds haven’t published papers that describe their models!
Ready to deploy your models in the real world? Learn how with theTensorFlow: Data and Deployment Specialization.Enroll today
",['https://dl-staging-website.ghost.io/content/images/2022/03/Coordinating-Robot-Limbs-2.gif']
Coordinating Robot Limbs,"A dog doesn’t think twice about fetching a tennis ball, but an autonomous robot typically suffers from delays between perception and action. A new machine-learning model helped a quadruped robot coordinate its sensors and actuators.
What's new:Chieko Sarah Imai and colleagues at University of California devised a reinforcement learning method,Multi-Modal Delay Randomization(MMDR), that approximates real-world latency in a simulated training environment, enabling engineers to compensate for it.
Key insight:Most robot simulations wait for the machine to take an action after a change in its surroundings. But in the real world, it takes time for a sensor to read the environment, a neural network to compute the action, and motors to execute the action — and by that time, the environment has already shifted again. Simulating the latency of sensors that track position and movement during traininghelpsa model to learn to adjust accordingly, but that doesn’t account for lags due to reading and processing visual sensors. Simulating a separate latency for vision should address this issue.
How it works:The authors trained their system to compute optimal angles for a simulated robot's joints using the reinforcement learning algorithmproximal policy optimization. The virtual robot traversed uneven virtual ground between box-like obstacles in aPyBulletsimulation.
Results:The authors tested aUnitree A1robot in the real world, comparing MMDR with alternatives they call No-Delay and Frame-Extract. No-Delay used only the four most recent frames as input. Frame-Extract was similar to MMDR but used the initial frames from each of the buffered sequences. MMDR was consistently best in terms of steps traveled through a variety of terrain. For example, in nine forest trials, the robot using MMDR moved an average of 992.5 steps versus 733.8 steps for No-Delay and 572.4 steps for Frame-Extract.
Why it matters:Robots in the wild often face different mechanical and environmental conditions than a simulation can reproduce. To build autonomous machines that work in the real world, it’s critical to account for all kinds of latency in the system.
We're thinking:Roboticists and mechanical engineers who work with physical robots have been accounting for various control latencies for decades. But much of the recent activity in reinforcement learning has involved simulated environments. We’re glad to see researchers working to bridge the sim-to-real gap and address the challenges of working with physical robots.
",['https://dl-staging-website.ghost.io/content/images/2022/03/Algorithms-for-the-Aged.gif']
How to Overcome Societal Obstacles,"By Benjamin Harvey
The top artificial intelligence companies include many people who earned degrees at elite educational institutions and started their employment with prior work experience. Yet the world is full of people from nontraditional backgrounds. They also have much to contribute to AI, but they face obstacles like minority status, low income, poor education, or social unrest.
I know this first-hand. I grew up poor and black in Jacksonville, the murder capital of the U.S. state of Florida. My neighborhood was a venue for street basketball and dope dealers. Many of my friends from that time are either dead or in jail.
If those challenges resonate with you, I offer a message of hope. I earned a doctorate in computer science, became chief of Operations Data Science at the National Security Agency, and founded an AI startup. You, too can join the community of machine learning engineers. It won’t be easy, but it can be done, and the rewards can be great in terms of both having a satisfying career and bringing good into the world.
The fact is, the AI industry needs a socially diverse workforce. Diversity among workers who curate datasets, design architectures, build models, and deploy systems can reduce bias and increase fairness.That makes for more robust systems and thriving businesses.
There are many ways that both students and companies can smooth the way from a disadvantaged background to a career in AI.
Students:You can benefit from many free services and educational materials available online, if you have access to a good internet connection. You may also be able to get help from your employer and nonprofits.
Companies:If you don’t have a process for finding and hiring minority candidates, you’re missing a huge talent pool that is largely untapped by your competitors. Once you’ve hired them, be ready to nurture their talent and fill in gaps in their knowledge.
If you work for a company that can open its doors wider to disadvantaged candidates, I urge you to proceed with all due haste. But if you’re an individual trying to find your way, don’t rush the process. You have a long journey ahead. No matter what school you come from, what stage you are in your career, and what adverse conditions you have experienced, continue to seek ways to improve. Face adversity head-on. Don’t give up on your dreams.
Benjamin Harvey is founder and CEO of AI Squared, a startup that helps organizations integrate AI into applications.
Is taking theDeep Learning Specializationthe right next step for you? Attend a live Ask Me Anything session with course mentors and community leaders  at 10:00 a.m. Pacific time on January 26, 2022, and get answers to your questions.Watch The Replay
",['https://dl-staging-website.ghost.io/content/images/2022/05/DeepLearningAI_Banner_Main_1200x628_A.png']
